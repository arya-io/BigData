# ğŸ” What is Big Data?  
Big Data refers to extremely large and complex datasets that traditional data-processing software struggles to handle. It involves storing, processing, and analyzing huge amounts of information efficiently.  

ğŸ“– *Definition:* â€œBig data is a term used to refer to the study and applications of data sets that are too complex for traditional data-processing software.â€ â€“ Wikipedia  

---

# ğŸ“Š The Three V's of Big Data  
Big Data is characterized by three main factors:  

- **ğŸ“¦ Volume:** The size of the data (huge amounts of information collected from various sources).  
- **ğŸŒ Variety:** The different formats and types of data (structured, semi-structured, and unstructured).  
- **âš¡ Velocity:** The speed at which data is generated, processed, and analyzed.  

ğŸ’¡ *Example:* Think of social mediaâ€”millions of users generate posts, comments, and messages every second. This enormous, fast-moving, and diverse data is classic Big Data!  

---

# ğŸ–¥ï¸ Big Data Concepts & Terminology  

- **ğŸ–¥ï¸ Clustered Computing:** Using multiple machines together to form a powerful system.  
- **ğŸ”„ Parallel Computing:** Performing computations simultaneously to speed up processing.  
- **ğŸŒ Distributed Computing:** Multiple networked computers work together, sharing the workload.  
- **ğŸ“¦ Batch Processing:** Dividing large jobs into smaller chunks and running them separately.  
- **âš¡ Real-Time Processing:** Handling data instantly as it arrives (e.g., stock market updates).  

ğŸ’¡ *Example:* Online transactionsâ€”when you make a purchase, the system instantly processes payment details, checks availability, and confirms your order.  

---

# âš™ï¸ Big Data Processing Systems  

### ğŸ”¹ Hadoop/MapReduce  
- âœ”ï¸ A scalable, fault-tolerant framework written in Java.  
- ğŸ”“ Open-source, widely used in Big Data.  
- â³ Primarily supports batch processing (processing data in chunks).  

### âš¡ Apache Spark  
- ğŸš€ A high-speed, general-purpose cluster computing system.  
- ğŸ”“ Open-source, like Hadoop.  
- ğŸï¸ Supports both batch and real-time data processing, making it more versatile.  

ğŸ’¡ *Comparison:* Hadoop works like a traditional assembly line, processing one batch at a time. Spark, on the other hand, processes data much faster and supports real-time tasks!  

---

# âš¡ Features of Apache Spark  

- ğŸŒ **Distributed computing framework** â€“ Runs efficiently across multiple machines.  
- ğŸ”¥ **In-memory computations** â€“ Stores intermediate results in RAM for ultra-fast performance.  
- ğŸï¸ **Lightning-fast processing** â€“ Executes tasks much quicker than traditional systems.  
- ğŸ”£ **Multi-language support** â€“ Works with Java, Scala, Python, R, and SQL.  
- âš¡ **Always faster than MapReduce** â€“ Uses optimized memory management for speed.  

ğŸ’¡ *Example:* If Hadoop is a truck carrying data from one place to another, Spark is a high-speed bullet train that gets the job done in record time! ğŸš„  

---

# ğŸ”¥ Apache Spark Components  

![image](https://github.com/user-attachments/assets/4a5d8342-4251-45ce-9efc-12f3563b7965)  

Apache Spark consists of several components that enhance its functionality:  

- **ğŸ—‚ï¸ Spark SQL** â€“ Used for structured data processing, similar to SQL databases.  
- **ğŸ¤– MLlib (Machine Learning Library)** â€“ Provides algorithms for machine learning tasks like classification, clustering, and regression.  
- **ğŸ”— GraphX** â€“ Optimized for graph-based computations and analytics (think social network connections).  
- **ğŸŒŠ Spark Streaming** â€“ Handles real-time data processing, such as analyzing live tweets or stock prices.  
- **âš¡ RDD API (Resilient Distributed Dataset)** â€“ The core data model of Spark, just like:  
  - Tables in Hive  
  - Relations in Pig  
  - DataFrames in Pandas  

ğŸ’¡ *Example:* If youâ€™ve ever worked with Pandas in Python, RDDs are somewhat similarâ€”they provide a flexible way to process data across multiple machines!  

---

# ğŸš€ Spark Modes of Deployment  

Spark can run in different modes depending on the environment:  

- **ğŸ’» Local Mode** â€“ Runs on a single machine, like your laptop.  
  - âœ… Best for testing, debugging, and small-scale experiments.  
  - âœ… Convenient for learning and demonstrations.  

- **ğŸŒ Cluster Mode** â€“ Runs on a group of predefined machines in a distributed setup.  
  - âœ… Used for production environments.  
  - âœ… Handles large-scale data efficiently.  

ğŸ”„ **Workflow:**  
Development starts on **Local Mode**, and then moves to **Cluster Mode** for productionâ€”without requiring any code changes!  

ğŸ’¡ *Example:* Think of Local Mode as using your laptop for practice, while Cluster Mode is like deploying an app on powerful servers for real-world users.  

---

# ğŸ PySpark  

## ğŸš€ PySpark: Spark with Python  

Apache Spark is originally written in Scala, but to enable Python developers to use Spark, the community introduced **PySpark**!  

### âš¡ Features of PySpark:  
- âœ… Provides the same **speed** and **power** as Scala-based Spark.  
- âœ… PySpark APIs are **similar to Pandas and Scikit-learn**, making it beginner-friendly.  

ğŸ’¡ *Example:* If you already use Pandas for data analysis, switching to PySpark for big data tasks will feel natural!  

---

# ğŸ—ï¸ What is Spark Shell?  

Spark Shell is an interactive environment where you can quickly execute Spark jobs. It helps in:  

âœ”ï¸ **Fast prototyping** â€“ Quickly testing whether something works or not.  
âœ”ï¸ **Interacting with data** â€“ Directly working with files on disk or data in memory.  

### ğŸ–¥ï¸ Types of Spark Shells:  
- ğŸŸ  **Spark-shell** for Scala  
- ğŸ”µ **PySpark-shell** for Python  
- ğŸŸ¢ **SparkR** for R  

ğŸ”¹ Spark can also run in **Java**, but there is **no dedicated shell for Java**.  

ğŸ’¡ *Example:* Think of Spark Shell as a Python REPL (interactive prompt), but for big data processingâ€”it allows you to run small code snippets and experiment with Spark's capabilities without setting up full programs!  

---

# ğŸ› ï¸ Opening Python Shell Using PySpark  

You can open the **Python shell using PySpark** instead of Jupyter Notebook by running the following command in your terminal:  

```bash
pyspark
```

This will launch the PySpark interactive shell, allowing you to run Spark commands directly.  

ğŸ’¡ *Example:* If you're familiar with Python's interactive prompt (`python` command in the terminal), PySpark works similarly but with Spark-based functions included!  

---

To run **PySpark** in the **Python shell** instead of launching Jupyter Notebook, you need to unset certain configurations. Hereâ€™s how you can do it:  

### ğŸ› ï¸ Steps to Unset the Configuration & Use Python Shell  

1ï¸âƒ£ **Unset the `PYSPARK_DRIVER_PYTHON` and `PYSPARK_DRIVER_PYTHON_OPTS` environment variables**  
Run the following command in your terminal:  

```bash
unset PYSPARK_DRIVER_PYTHON
unset PYSPARK_DRIVER_PYTHON_OPTS
```

ğŸ”¹ These environment variables are responsible for launching Jupyter Notebook by default when you run `pyspark`.  

2ï¸âƒ£ **Run PySpark in the Terminal**  
Once the configurations are unset, simply enter:  

```bash
pyspark
```

âœ”ï¸ This will now open the **PySpark interactive shell**, instead of Jupyter Notebook.  

ğŸ’¡ *Example:* Think of this like switching from a **fancy graphical interface (Jupyter)** to a **raw terminal experience**, where you can directly run Python commands with Spark.  

---

## ğŸš— **Understanding SparkContext**

### What is SparkContext?

SparkContext is the **entry point** to the world of **Apache Spark**. ğŸŒŸ

* Think of it like a **key** to a **house**. You need this key to access the Spark functionalities (like opening the door to the house). ğŸ 
* In **PySpark**, the default **SparkContext** is named **sc**. PySpark automatically creates this for you when you start working in the PySpark shell, so you donâ€™t need to create it yourself.

### Analogy to Make It Easier

* Imagine SparkContext as the **key** to your **car** ğŸš—. Without it, you can't drive or interact with the car (Spark). PySpark gives you the key (SparkContext) so you can start driving (running your Spark jobs).

---

### Example: Accessing SparkContext Information

Letâ€™s check out a simple **script.py** to learn more about **SparkContext**.

```python
# Print the version of SparkContext
print("The version of Spark Context in the PySpark shell is", sc.version)

# Print the Python version of SparkContext
print("The Python version of Spark Context in the PySpark shell is", sc.pythonVer)

# Print the master of SparkContext
print("The master of Spark Context in the PySpark shell is", sc.master)
```

* **Output**:

  ```bash
  The version of Spark Context in the PySpark shell is 2.4.5
  The Python version of Spark Context in the PySpark shell is 3.6
  The master of Spark Context in the PySpark shell is local[*]
  ```

In the example:

* **sc.version** gives the Spark version.
* **sc.pythonVer** shows the Python version used with Spark.
* **sc.master** tells you whether Spark is running locally or on a cluster.

---

## ğŸ” **Inspecting SparkContext**

If you want to inspect your **SparkContext**, you can run a few commands:

```python
# Check the ID of the SparkContext
id(sc)  # This will give you a unique ID for the SparkContext

# Check the type of SparkContext
type(sc)  # <class 'pyspark.context.SparkContext'>

# Check SparkContext version
sc.version  # Example output: '2.4.5'

# Check Python version
sc.pythonVer  # Example output: '3.6'

# Check the master
sc.master  # Example output: 'local[*]'
```

### Example Output:

```bash
id(sc)
139771463232984

type(sc)
<class 'pyspark.context.SparkContext'>

sc.version
'2.4.5'

sc.pythonVer
'3.6'

sc.master
'local[*]'
```

---

## ğŸ“¥ **Loading Data in PySpark**

You can load data into PySpark using the **SparkContext** methods.

### 1. **parallelize() Method**

* This method is used to **create an RDD** (Resilient Distributed Dataset) from a list or collection.

  Example:

  ```python
  rdd = sc.parallelize([1, 2, 3, 4, 5])
  ```

  Here, we parallelize a list of numbers, which will allow us to process them in parallel across multiple Spark workers.

### 2. **textFile() Method**

* This method is used to **load a text file** into an RDD.

  Example:

  ```python
  rdd2 = sc.textFile("test.txt")
  ```

  This will read the **test.txt** file and create an RDD from it, which can be processed by Spark.

### File Protocols

In **Jupyter Notebooks**, you're not working in a Spark shell. However, you can still execute Spark commands and interact with Spark via **PySpark** in a notebook environment. ğŸ“

---

### ğŸ§  **Quick Recap**:

* **SparkContext** is your **entry point** to using **Apache Spark**.
* **parallelize()** and **textFile()** are common methods to load data into Spark.
* You can easily inspect the version, Python version, and cluster information using `sc.version`, `sc.pythonVer`, and `sc.master`.

---

## ğŸ–¥ï¸ **Interactive Use of PySpark**

### What is the PySpark Shell?

PySpark comes with its own **interactive Python shell**. Think of this shell as a **playground** for testing and experimenting with Spark operations. ğŸ› ï¸

* The shell comes with **PySpark already installed**, making it very convenient for **basic testing and debugging**. You don't need to worry about setting things up manually!
* One of the coolest things? You don't have to create a **SparkContext object** yourself. **PySpark** automatically creates a **SparkContext**, and itâ€™s available by default as the variable `sc`. This saves you a lot of time! â±ï¸

---

### Example: Working with PySpark Shell

Letâ€™s see how we can use the **PySpark shell** to work with data:

```python
# Create a Python list of numbers from 1 to 100
numb = range(1, 101)
print(type(numb))  # <class 'range'>

# Load the list into PySpark
spark_data = sc.parallelize(numb)
print(type(spark_data))  # <class 'pyspark.rdd.RDD'>

# Collect the data and print it
print(spark_data.collect())  # Prints all the numbers in the list

# Get help on the collect method
help(spark_data.collect())
```

### Key Points:

* **parallelize()**: This method is used to convert a regular Python list (like `numb`) into a **distributed collection** in Spark (an RDD). ğŸ§‘â€ğŸ’»
* **collect()**: This gathers the data back to the driver and prints it, but be careful â€“ for very large datasets, this could overload your memory. ğŸ˜…

---

## ğŸ“‚ **Loading Data in PySpark Shell**

In PySpark, data is processed through **distributed collections** (like RDDs). These collections are automatically **parallelized** across the cluster, so you donâ€™t need to worry about manually splitting the data. ğŸ’¥

### Example: Loading a Local File into PySpark

Letâ€™s load a file into PySpark using the `textFile()` method. Here's how:

```python
# Define the file path
file_path = 'file:////home/talentum/spark/README.md'

# Load the file into PySpark
lines = sc.textFile(file_path)

# Print the first 5 lines of the file
print(lines.take(5))
```

### Output:

```bash
['# Apache Spark', '', 'Spark is a fast and general cluster computing system for Big Data. It provides', 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that', 'supports general computation graphs for data analysis. It also supports a']
```

### Understanding the Methods:

* **take(5)**: This gets the **first 5 lines** of the file (like reading the first few lines of a book). ğŸ“–

  * `lines.take(5)` returns a **list**: `['line 1', 'line 2', ...]`
* **first()**: If you only want the very first line, you can use this method:

  ```python
  lines.first()  # Returns the very first line
  ```

### Types of Output:

```python
# Checking the types
print(type(lines.take))  # <class 'method'>
print(type(lines.take(5)))  # <class 'list'>
print(type(lines.first()))  # <class 'str'>
```

In **RDDs**, every line in the file is considered an **element** of the RDD. Think of an RDD as a collection of **lines of text** or **data records** that can be processed in parallel.

---

### ğŸ§  **Quick Recap**:

* The **PySpark shell** is great for testing and debugging because it comes with PySpark pre-installed and automatically creates a `SparkContext` for you.
* You can **parallelize** your data and run operations like **collect()** or **take()** to inspect the data.
* Data is represented as **RDDs** in PySpark, and you can load files using **textFile()** and perform operations on them.

---

## ğŸ§‘â€ğŸ’» **Use of Lambda Function in Python - filter()**

### What are Anonymous Functions in Python? ğŸ¤”

In Python, **lambda functions** are **anonymous functions**. This means they don't have a name like regular functions created with `def`. Instead, they are used for **short, simple tasks** where defining a full function would be overkill. ğŸ¯

Lambda functions are:

* **Powerful**: You can use them in combination with functions like **map()** and **filter()**.
* **Efficient**: They let you write **concise** code for simple operations.

They allow you to create a function on the fly, and it gets executed later. Think of them like **mini functions** that you donâ€™t need to give a name to!

---

## ğŸ“ **Lambda Function Syntax**

A **lambda function** in Python follows this basic syntax:

```python
lambda arguments: expression
```

### Example:

```python
# Lambda function to double a number
double = lambda x: x * 2
print(double(3))  # Output: 6
```

Here:

* `lambda x: x * 2` creates an anonymous function that takes `x` and returns `x * 2`.
* It is similar to writing a `def` function but in a **one-liner**.

---

## ğŸ†š **Difference Between def and lambda Functions**

Both `def` and `lambda` functions are used to create functions in Python, but there are some key differences:

### Example: Cube of a Number

```python
# Using def to create a function
def cube(x):
    return x ** 3

# Using lambda to create a function
g = lambda x: x ** 3

print(g(10))   # Output: 1000
print(cube(10))  # Output: 1000
```

### Key Differences:

* **Return statement**: Lambda functions do not explicitly require the `return` keyword.
* **Location**: You can place lambda functions **anywhere** in your code (inline), whereas `def` functions need to be defined before use.

---

## ğŸ”„ **Use of Lambda Function in Python - map()**

The `map()` function in Python applies a **function** to all items in a list (or another iterable) and returns a new list with the results. You can use **lambda functions** inside `map()` for a concise, efficient solution. ğŸ§ 

### General Syntax of `map()`:

```python
map(function, iterable)
```

### Example of `map()`:

```python
# Add 2 to each element of the list
items = [1, 2, 3, 4]
result = list(map(lambda x: x + 2, items))
print(result)  # Output: [3, 4, 5, 6]
```

### Why Use map()?

The `map()` function eliminates the need for a **for loop**, making the code more **concise** and **easier to read**. Instead of writing a long loop, you can achieve the same result in a single line.

---

## ğŸ” **Use of Lambda Function in Python - filter()**

The `filter()` function in Python is used to **filter** elements from a list based on a condition provided by a function. You can combine **filter()** with **lambda functions** to create **inline filters** that select specific items from the list. ğŸ§¹

### General Syntax of `filter()`:

```python
filter(function, iterable)
```

### Example of `filter()`:

```python
# Filter out even numbers, keep odd numbers
items = [1, 2, 3, 4]
result = list(filter(lambda x: (x % 2 != 0), items))
print(result)  # Output: [1, 3]
```

Here:

* The lambda function checks if a number is **odd** (i.e., `x % 2 != 0`).
* `filter()` only keeps the numbers where the condition evaluates to **True**.

---

## ğŸš« **Lambda Functions Are Not a Feature of Spark**

Although **lambda functions** are **great** for data manipulation in Python, **Spark** does not directly use lambda functions for its core operations. However, you can use lambda functions in **PySpark** for some data transformations when working with RDDs or DataFrames. ğŸ§‘â€ğŸ’»

---

### ğŸ§  **Quick Recap**:

* **Lambda functions** are **anonymous** functions that can be used inline to simplify your code.
* **map()** applies a function to every item in a list, and **filter()** filters items based on a condition.
* These functions allow for **concise** and **efficient** code when handling lists or iterables in Python.

---

ğŸ“Œ **Understanding RDD (Resilient Distributed Datasets) in Apache Spark** âš¡

![image](https://github.com/user-attachments/assets/4748b963-20e5-4cdd-a64f-fdc403ed6444)


RDD is the **fundamental building block** of Apache Spark, enabling distributed data processing with fault tolerance. Letâ€™s break it down in simple terms! ğŸ˜Š  

---

### ğŸ” **What is an RDD?**  
**Resilient Distributed Dataset (RDD)** is a **distributed collection of data** that is **fault-tolerant** and allows parallel processing across multiple nodes in a cluster.  

Think of an RDD like a **large dataset split into smaller chunks** and spread across multiple computers for efficient processing! ğŸš€  

---

### ğŸ–¥ï¸ **How RDD Works**  
1ï¸âƒ£ The **Spark driver** reads data from a source like HDFS, local files, or databases.  
2ï¸âƒ£ It **splits the data into multiple partitions** and distributes them across nodes in the cluster.  
3ï¸âƒ£ Each partition is **processed in parallel**, making computations much faster! ğŸ’¡  

ğŸ“œ **Example Code to Create an RDD:**  
```python
rdd = sparkContext.textFile("hdfs://data/sample.txt")
```
âœ… This loads a text file as an RDD, which Spark processes in chunks across nodes.  

---

### ğŸ“· **Visual Representation**  
Your image illustrates this well:  
- The **Spark driver reads data** from a file on disk.  
- It **creates an RDD** and distributes the partitions across nodes in the cluster.  
- Each node processes its **RDD partition independently**, ensuring **parallel computation**.  

---

### ğŸ¯ **Key Benefits of RDDs**  
âœ… **Fault Tolerance** â†’ Data is automatically recovered if a failure occurs ğŸ”„  
âœ… **Parallel Processing** â†’ Speeds up computations across multiple machines ğŸš€  
âœ… **Lazy Evaluation** â†’ Optimizes execution by processing data only when needed ğŸï¸  
âœ… **Immutability** â†’ Ensures consistency by keeping original data unchanged ğŸ”’  

---

## ğŸ§‘â€ğŸ’» **Decomposing RDDs in PySpark**

### What are Resilient Distributed Datasets (RDDs)? ğŸ¤”

An **RDD** is a fundamental data structure in **Spark**. Here's what RDD stands for:

* **Resilient**: Spark can handle failures and continue processing without any issues. It **recovers** data if something goes wrong. ğŸ’ª
* **Distributed**: The data is **spread** across multiple machines or nodes, allowing Spark to handle large datasets efficiently. ğŸŒ
* **Datasets**: An RDD is simply a **collection** of data that can be anything like arrays, tables, or tuples. ğŸ—‚ï¸

In simple terms, an RDD is a **distributed collection** of data that Spark can process across multiple machines, while also being **fault-tolerant**.

---

## ğŸ—ï¸ **Creating RDDs in PySpark**

You can create RDDs in PySpark from different sources. The two most common ways to create RDDs are:

### 1ï¸âƒ£ **Parallelizing an Existing Collection**:

You can create RDDs by parallelizing an existing Python list or collection. This means that Spark will divide the list into smaller **partitions** and process them across multiple machines.

### 2ï¸âƒ£ **External Datasets**:

You can create RDDs from **external datasets**, such as:

* **Files in HDFS (Hadoop Distributed File System)** ğŸ—„ï¸
* **Objects in an Amazon S3 bucket** â˜ï¸
* **Lines in a text file** ğŸ“„

### 3ï¸âƒ£ **From Existing RDDs**:

You can also create new RDDs by applying transformations to **existing RDDs**.

---

## ğŸ”„ **Parallelizing Collections**

You can convert an existing Python collection (like a list) into an RDD using **parallelize()**. This function splits the collection into partitions and processes them in parallel.

### Example:

```python
# Parallelizing a list of numbers into an RDD
numRDD = sc.parallelize([1, 2, 3, 4])

# Parallelizing a string into an RDD
helloRDD = sc.parallelize("Hello world")

# Check the type of helloRDD
print(type(helloRDD))  # Output: <class 'pyspark.rdd.PipelinedRDD'>
```

The `helloRDD` is now a distributed collection (RDD) that Spark can process in parallel. âœ¨

---

## ğŸ“‚ **From External Datasets**

Another way to create RDDs is by reading data from **external sources** such as a text file. You can use **textFile()** to load data from files and convert them into RDDs.

### Example:

```python
# Loading a file into an RDD
fileRDD = sc.textFile("README.md")

# Check the type of fileRDD
print(type(fileRDD))  # Output: <class 'pyspark.rdd.PipelinedRDD'>
```

Here, the file `README.md` is converted into an RDD, allowing Spark to process the file across multiple machines. ğŸŒ

---

## ğŸ“Š **Understanding Partitioning in PySpark**

A **partition** is a logical division of a large dataset. Spark breaks down large datasets into smaller partitions so that it can process them in parallel across different machines or nodes.

### Example with **parallelize()**:

```python
# Creating an RDD with 6 partitions
numRDD = sc.parallelize(range(10), numSlices=6)
```

### Example with **textFile()**:

```python
# Loading a file with 6 partitions
fileRDD = sc.textFile("README.md", minPartitions=6)
```

You can check how many partitions an RDD has using the **getNumPartitions()** method:

```python
# Get the number of partitions in an RDD
numPartitions = numRDD.getNumPartitions()
print(numPartitions)
```

### Glomming Partitions:

You can also **glom** partitions to group them together and see the data inside each partition. This helps in debugging or understanding how data is distributed:

```python
# Glom to see data in each partition
rdd.glom()
```

---

## ğŸ” **Checking File System in HDFS**

You can use the HDFS **fsck** command to check the **files**, **blocks**, and **locations** of data in HDFS:

```bash
hdfs fsck /user/talentum/stocks.csv -files -blocks -locations
```

This will give you details about the **location** and **health** of the file stored in HDFS. ğŸ“

---

### ğŸ§  **Quick Recap**:

* **RDDs** are distributed collections of data that can be processed across machines and are **fault-tolerant**.
* You can create RDDs by **parallelizing** collections or loading **external datasets** like text files.
* **Partitioning** helps Spark process data in parallel, and you can check partition details using **getNumPartitions()**.

---

### ğŸ”¥ Overview of PySpark Operations  

![image](https://github.com/user-attachments/assets/ccff4a44-8a7c-4c61-9277-2ac240308f2a)  

PySpark, the Python API for Apache Spark, is all about handling large-scale data processing efficiently. It operates on two key concepts:  

#### ğŸ› Transformations â€“ Creating New RDDs  
Transformations are operations that take an RDD (Resilient Distributed Dataset) and create a new one without modifying the original. Think of it like a caterpillar turning into a butterflyâ€”each transformation results in a new dataset.  

For example:  
```python
rdd1 = sparkContext.parallelize([1, 2, 3, 4])
rdd2 = rdd1.map(lambda x: x * 2)  # Transformation (map) creates a new RDD
```
Here, `map()` applies a function to each element, creating a new RDD (`rdd2`). The original RDD (`rdd1`) remains unchanged.  

#### ğŸ–¨ï¸ Actions â€“ Performing Computations on RDDs  
Actions trigger computations and return values. Until an action is performed, transformations are **lazy**, meaning they donâ€™t execute immediately but wait for an action to trigger processing.  

For example:  
```python
result = rdd2.collect()  # Action (collect) triggers computation
print(result)  # Output: [2, 4, 6, 8]
```
Here, `collect()` gathers the elements of `rdd2` and prints them.  

So, remember:  
âœ… **Transformations** create new RDDs but donâ€™t execute immediately.  
âœ… **Actions** trigger computation and return results.  

This way, PySpark optimizes processing by applying transformations lazily and executing only when needed! ğŸš€  

---

### âš¡ RDD Transformations â€“ Lazy Evaluation  

![image](https://github.com/user-attachments/assets/d0be59c6-a464-4f04-a379-469a98b17b33)  

RDD transformations in PySpark are **lazy**, meaning they donâ€™t execute immediately! Instead, they wait until an **action** is triggered, ensuring efficient computation by avoiding unnecessary processing.  

#### ğŸš€ How Lazy Evaluation Works  
Imagine a factory assembling a product but only running machines when an order is placed. Similarly, RDD transformations stack up, but PySpark waits until an action demands results before processing them.  

##### ğŸ”— Data Flow in Lazy Evaluation  
1ï¸âƒ£ **RDD1** is created from storage (e.g., reading a file).  
2ï¸âƒ£ Transformation applies â†’ **RDD2** is generated but not processed yet.  
3ï¸âƒ£ Another transformation applies â†’ **RDD3** is ready but still waiting.  
4ï¸âƒ£ Finally, an **action** executes â†’ Computation occurs, and results are returned.  

#### ğŸ”¥ Basic RDD Transformations  
These fundamental operations help manipulate data efficiently:  

âœ… `map()`: Applies a function to each element and returns a new RDD.  
```python
rdd = sparkContext.parallelize([1, 2, 3])
mapped_rdd = rdd.map(lambda x: x * 2)  # [2, 4, 6]
```  

âœ… `filter()`: Extracts elements based on a condition.  
```python
filtered_rdd = rdd.filter(lambda x: x % 2 == 0)  # [2]
```  

âœ… `flatMap()`: Flattens nested structures by splitting elements into multiple outputs.  
```python
rdd = sparkContext.parallelize(["Hello World"])
flat_mapped_rdd = rdd.flatMap(lambda x: x.split())  # ["Hello", "World"]
```  

âœ… `union()`: Merges two RDDs into one.  
```python
rdd1 = sparkContext.parallelize([1, 2])
rdd2 = sparkContext.parallelize([3, 4])
union_rdd = rdd1.union(rdd2)  # [1, 2, 3, 4]
```  

### ğŸ“ Recap:  
âœ¨ **Transformations** create new RDDs but do not execute immediately.  
âœ¨ **Lazy evaluation** optimizes performance by delaying computation.  
âœ¨ Common transformations like `map()`, `filter()`, `flatMap()`, and `union()` help manipulate data efficiently.  

---

### ğŸ”„ `map()` Transformation â€“ Applying a Function to All Elements  

![image](https://github.com/user-attachments/assets/54629600-f45d-4bde-9ccb-61508c2d35c8)  

#### âœ¨ What is `map()` Transformation?  
The `map()` transformation is used in PySpark to apply a **function** to every element in an RDD, producing a **new RDD** with transformed values.  

Think of it like a **magic converter**â€”every item in the dataset passes through a function and comes out transformed!  

#### ğŸ” Example Breakdown  
```python
RDD = sc.parallelize([1, 2, 3, 4])  # Creating an RDD with numbers  
RDD_map = RDD.map(lambda x: x * x)  # Squaring each number  
```
ğŸ”¹ The `map()` function applies `lambda x: x * x` to each element in `RDD`.  
ğŸ”¹ The new RDD (`RDD_map`) contains `[1, 4, 9, 16]`.  

#### ğŸ“Š Visual Representation  
Imagine the original RDD as a conveyor belt:  
**Before `map()` Transformation**: `[1, 2, 3, 4]`  
â¡ Function applied (`x * x`)  
**After `map()` Transformation**: `[1, 4, 9, 16]`  

This transformation is **element-wise**, meaning it **processes each item independently**.  

#### ğŸš€ Key Points to Remember  
âœ… `map()` **always returns a new RDD** (original RDD remains unchanged).  
âœ… It is a **one-to-one transformation** (each input results in one output).  
âœ… Used for **modifying values** (e.g., squaring, doubling, converting formats).  

---

### ğŸš¦ `filter()` Transformation â€“ Selecting Specific Elements  

![image](https://github.com/user-attachments/assets/6aa6dfaa-a6e2-4148-b552-69e55e2486b5)  

#### ğŸ” What is `filter()` Transformation?  
The `filter()` transformation helps **extract only the elements that meet a specific condition**, creating a **new RDD** with the filtered results.  

Think of it like a **sieve**â€”it keeps what you need and removes the rest!  

#### âœ¨ Example Breakdown  
```python
RDD = sc.parallelize([1, 2, 3, 4])  # Creating an RDD with numbers  
RDD_filter = RDD.filter(lambda x: x > 2)  # Keep only numbers greater than 2  
```
ğŸ”¹ The function `lambda x: x > 2` checks each element, keeping only `3` and `4`.  
ğŸ”¹ The new RDD (`RDD_filter`) contains `[3, 4]`.  

#### ğŸ“Š Visual Representation  
Imagine the original RDD is a list of items:  
**Before `filter()` Transformation**: `[1, 2, 3, 4]`  
â¡ Condition applied (`x > 2`)  
**After `filter()` Transformation**: `[3, 4]`  

### ğŸš€ Key Takeaways  
âœ… `filter()` **creates a new RDD** with only selected elements.  
âœ… It **removes** elements that donâ€™t match the condition.  
âœ… Used for **data preprocessing** (e.g., filtering errors, selecting relevant data).  


---

### ğŸŒŠ `flatMap()` Transformation â€“ Expanding Elements  

![image](https://github.com/user-attachments/assets/4eebc285-9729-45f2-be1c-70eee10bc780)  

#### ğŸ” What is `flatMap()` Transformation?  
Unlike `map()`, which transforms each element **one-to-one**, `flatMap()` **splits** elements and returns **multiple values** for each original item, creating a **flattened** RDD.  

Think of it like breaking sentences into individual wordsâ€”each input expands into multiple outputs!  

#### âœ¨ Example Breakdown  
```python
RDD = sc.parallelize(["hello world", "how are you"])  
RDD_flatmap = RDD.flatMap(lambda x: x.split(" "))  
```
ğŸ”¹ The function `lambda x: x.split(" ")` splits each string into words.  
ğŸ”¹ The new RDD (`RDD_flatmap`) contains `["hello", "world", "how", "are", "you"]`.  

#### ğŸ“Š Visual Representation  
**Before `flatMap()` Transformation**:  
`["hello world", "how are you"]`  

â¡ Function applied (`split()` on space)  

**After `flatMap()` Transformation**:  
`["hello", "world", "how", "are", "you"]`  

#### ğŸš€ Key Takeaways  
âœ… `flatMap()` **splits elements into multiple outputs** (not one-to-one like `map()`).  
âœ… Creates a **flattened RDD**, removing nested structures.  
âœ… Useful for **text processing**, where sentences need to be broken into words.  



---

### ğŸ”— `union()` Transformation â€“ Merging RDDs  

![image](https://github.com/user-attachments/assets/4bb17119-cc1a-4892-b5ca-cd9c3286b816)  

#### ğŸ” What is `union()` Transformation?  
The `union()` transformation **combines two RDDs into a single RDD**, merging all elements while keeping duplicates.  

Think of it like **merging two lists**â€”you get everything from both, without any filtering!  

#### âœ¨ Example Breakdown  
```python
inputRDD = sc.textFile("logs.txt")  # Reading log file as RDD  

errorRDD = inputRDD.filter(lambda x: "error" in x.split())  # Filtering error messages  
warningsRDD = inputRDD.filter(lambda x: "warnings" in x.split())  # Filtering warnings  

combinedRDD = errorRDD.union(warningsRDD)  # Merging both RDDs  
```
ğŸ”¹ `errorRDD` keeps lines containing **"error"**  
ğŸ”¹ `warningsRDD` keeps lines containing **"warnings"**  
ğŸ”¹ `union()` merges both, keeping all elements  

#### ğŸ“Š Visual Representation  
Imagine two lists:  
**Before `union()` Transformation**  
`errorRDD`: `["Error: Disk Full", "Error: Timeout"]`  
`warningsRDD`: `["Warning: Low Memory", "Warning: High CPU Usage"]`  

â¡ `union()` merges them  

**After `union()` Transformation**  
`["Error: Disk Full", "Error: Timeout", "Warning: Low Memory", "Warning: High CPU Usage"]`  

#### ğŸš€ Key Takeaways  
âœ… `union()` **combines two RDDs** while keeping duplicates  
âœ… Useful for **merging filtered results** (like logs, events, data subsets)  
âœ… Doesnâ€™t remove duplicatesâ€”both datasets are preserved  

---

## âš¡ **RDD Actions in PySpark**

### What are RDD Actions? ğŸ¤”

**Actions** are operations that **trigger a computation** and return a value after running the computation on the RDD. They are the final step that brings the results of RDD transformations to the **driver** program. ğŸ

Common **Basic RDD Actions** include:

* **collect()**: Brings all elements of the dataset to the driver program as a list. ğŸ“œ
* **take(N)**: Returns the first **N** elements of the dataset. ğŸ”¢
* **first()**: Returns the **first element** of the dataset. ğŸ”‘
* **count()**: Returns the **number of elements** in the RDD. ğŸ“Š

---

## ğŸ“¦ **Understanding collect() and take() Actions**

### 1ï¸âƒ£ **collect()** Action:

The **collect()** action gathers all the elements from the RDD and brings them to the **driver** program as a Python **list**.

#### Example:

```python
RDD_map = sc.parallelize([1, 2, 3, 4])
squaredRDD = RDD_map.map(lambda x: x ** 2)

# Collect all the elements of the RDD
print(squaredRDD.collect())  
# Output: [1, 4, 9, 16]
```

### 2ï¸âƒ£ **take(N)** Action:

The **take(N)** action returns the first **N** elements of the dataset as a list.

#### Example:

```python
RDD_map = sc.parallelize([1, 2, 3, 4])

# Take the first 2 elements of the RDD
print(RDD_map.take(2))  
# Output: [1, 2]
```

---

## ğŸ§‘â€ğŸ’» **first() and count() Actions**

### 1ï¸âƒ£ **first()** Action:

The **first()** action returns the **first element** of the RDD.

#### Example:

```python
RDD_map = sc.parallelize([1, 2, 3, 4])

# Get the first element
print(RDD_map.first())  
# Output: 1
```

### 2ï¸âƒ£ **count()** Action:

The **count()** action returns the **number of elements** in the RDD.

#### Example:

```python
RDD_flatmap = sc.parallelize([1, 2, 3, 4, 5])

# Count the number of elements
print(RDD_flatmap.count())  
# Output: 5
```

---

## ğŸ§ª **Lab Example: Using Actions with RDDs**

Hereâ€™s an example demonstrating how to use the **map()** transformation and **RDD actions**.

### Step 1: Cube Numbers in an RDD

```python
# Create an RDD from a list of numbers
numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
numbRDD = sc.parallelize(numbers)

# Cube the numbers using map() transformation
cubedRDD = numbRDD.map(lambda x: x ** 3)

# Collect the results (get the entire dataset)
numbers_all = cubedRDD.collect()

# Print the cubed numbers
for numb in numbers_all:
    print(numb)
```

#### Output:

```
1
8
27
64
125
216
343
512
729
1000
```

---

### Step 2: Filter Lines Containing the Keyword "Spark" in a File

```python
file_path = 'file:////home/talentum/spark/README.md'

# Load the file into an RDD
fileRDD = sc.textFile(file_path)

# Filter the fileRDD for lines that contain the word "Spark"
fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)

# Count the number of lines containing the word "Spark"
print("The total number of lines with the keyword Spark is", fileRDD_filter.count())

# Print the first 4 lines containing the word "Spark"
for line in fileRDD_filter.take(4):
    print(line)
```

#### Output:

```
The total number of lines with the keyword Spark is 19
# Apache Spark
Spark is a fast and general cluster computing system for Big Data. It provides
rich set of higher-level tools including Spark SQL for SQL and DataFrames,
and Spark Streaming for stream processing.
```

---

## ğŸ“š **Quick Recap**:

* **RDD Actions** trigger the computation and bring results to the driver program.
* **collect()** retrieves all elements as a list, while **take(N)** returns the first **N** elements.
* **first()** returns the first element, and **count()** returns the number of elements in the RDD.

With these actions, you can efficiently retrieve, filter, and count data within RDDs! ğŸ‰

---

## ğŸ”‘ **Introduction to Pair RDDs in PySpark**

In real-life datasets, the data is often structured as **key-value pairs**, where each row is a **key** that maps to one or more **values**.

* **Pair RDD** is a special kind of RDD that allows you to work with key-value pairs.
* **Key** is the identifier (e.g., an ID or name) and **value** is the data associated with that key.

This makes it easy to perform operations like grouping, sorting, or combining data based on the key. ğŸ”„

---

## ğŸ› ï¸ **Creating Pair RDDs**

### 1ï¸âƒ£ **From a List of Key-Value Tuples**

You can create a pair RDD by directly passing a list of tuples, where each tuple contains a key and a value.

#### Example:

```python
my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)] 
pairRDD_tuple = sc.parallelize(my_tuple)
```

### 2ï¸âƒ£ **From a Regular RDD**

You can also convert a regular RDD into a pair RDD by using **map()** and splitting the data into key-value pairs.

#### Example:

```python
my_list = ['Sam 23', 'Mary 34', 'Peter 25'] 
regularRDD = sc.parallelize(my_list)

# Convert the regular RDD to a pair RDD
pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))
```

---

## ğŸ”„ **Transformations on Pair RDDs**

Just like regular RDDs, **pair RDDs** support many transformations. However, you need to pass functions that operate on **key-value pairs**, not individual elements.

Some common pair RDD transformations include:

* **reduceByKey(func)**: Combine values with the same key.
* **groupByKey()**: Group values with the same key.
* **sortByKey()**: Sort the RDD by key.
* **join()**: Join two pair RDDs based on their key.

---

## â— **reduceByKey() Transformation**

The **reduceByKey()** transformation combines the values associated with the same key. It runs in parallel across the dataset, performing the operation on each key.

#### Example:

```python
regularRDD = sc.parallelize([("Messi", 23), ("Ronaldo", 34), ("Neymar", 22), ("Messi", 24)])

# Combine values with the same key
pairRDD_reducebykey = regularRDD.reduceByKey(lambda x, y: x + y)

# Collect the results
print(pairRDD_reducebykey.collect())
# Output: [('Neymar', 22), ('Ronaldo', 34), ('Messi', 47)]
```

---

## ğŸ”¢ **sortByKey() Transformation**

The **sortByKey()** operation sorts the pair RDD by its key, either in ascending or descending order.

#### Example:

```python
pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0]))

# Sort by key in descending order
sorted_RDD = pairRDD_reducebykey_rev.sortByKey(ascending=False)

# Collect the results
print(sorted_RDD.collect())
# Output: [(47, 'Messi'), (34, 'Ronaldo'), (22, 'Neymar')]
```

---

## ğŸ”€ **groupByKey() Transformation**

The **groupByKey()** transformation groups all values with the same key together in the pair RDD.

#### Example:

```python
airports = [("US", "JFK"), ("UK", "LHR"), ("FR", "CDG"), ("US", "SFO")]
regularRDD = sc.parallelize(airports)

# Group values with the same key
pairRDD_group = regularRDD.groupByKey().collect()

# Print the results
for cont, air in pairRDD_group:
    print(cont, list(air))
# Output:
# FR ['CDG']
# US ['JFK', 'SFO']
# UK ['LHR']
```

---

## ğŸ”— **join() Transformation**

The **join()** transformation allows you to join two pair RDDs based on their keys. It returns a new RDD containing the joined data.

#### Example:

```python
RDD1 = sc.parallelize([("Messi", 34), ("Ronaldo", 32), ("Neymar", 24)])
RDD2 = sc.parallelize([("Ronaldo", 80), ("Neymar", 120), ("Messi", 100)])

# Join the two RDDs based on their key (name)
joined_RDD = RDD1.join(RDD2)

# Collect the results
print(joined_RDD.collect())
# Output: [('Neymar', (24, 120)), ('Ronaldo', (32, 80)), ('Messi', (34, 100))]
```

---

## ğŸ“š **Quick Recap**:

* **Pair RDDs** are special RDDs that allow you to work with **key-value pairs**.
* You can create pair RDDs from **lists of tuples** or **regular RDDs** by using **map()**.
* Common transformations include **reduceByKey()**, **groupByKey()**, **sortByKey()**, and **join()**.
* These transformations enable powerful operations like **grouping**, **sorting**, and **combining data** based on keys. ğŸ‰

With pair RDDs, you can easily process key-value datasets like user data, logs, etc.! ğŸš€

---

# ğŸš€ **PySpark Actions: Transforming and Aggregating RDDs**  

Actions in PySpark trigger computation and return results to the driver program. Unlike transformations, actions **execute immediately** and generate **final outputs**. Letâ€™s simplify these concepts with examples! ğŸ˜Š  

---

## ğŸ”¹ **reduce() Action**  

âœ… **Aggregates elements in an RDD using a specified function**.  
âœ… The function must be **commutative** (order does not change the result) and **associative** (grouping does not affect the result).  

ğŸ“œ **Example of reduce() in PySpark:**  
```python
x = [1, 3, 4, 6]  
RDD = sc.parallelize(x)  
RDD.reduce(lambda x, y: x + y)  # Output: 14
```
ğŸ’¡ **Think of `reduce()` like summing up values in a shopping cart**â€”it combines all elements efficiently!  

---

## ğŸ“ **saveAsTextFile() Action**  

âœ… Saves an RDD **into a text file inside a directory**, with **each partition stored as a separate file**.  
âœ… Use `coalesce()` to **combine partitions** and save the RDD as **a single text file**.  

ğŸ“œ **Example Usage:**  
```python
RDD.saveAsTextFile("tempFile")  # Saves data into multiple files  
RDD.coalesce(1).saveAsTextFile("tempFile")  # Saves as a single file  
```
ğŸš€ **This is a transformation operation, still generating a new RDD!**  

ğŸ’¡ **Imagine exporting a datasetâ€”by default, PySpark saves multiple files, but `coalesce(1)` combines everything into a single file for easier management!**  

---

## ğŸ”„ **Action Operations on Pair RDDs**  

âœ… **Pair RDDs store key-value data**, allowing easy aggregation and retrieval.  
âœ… Common actions used in **pair RDDs** include:  
   - `countByKey()`
   - `collectAsMap()`  

ğŸ“œ **Example of `countByKey()` on a simple list:**  
```python
rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])  
for key, val in rdd.countByKey().items():  
    print(key, val)  
# Output:
# 'a' â†’ 2
# 'b' â†’ 1
```
ğŸš€ **This counts occurrences of each key efficiently!**  

---

## ğŸ” **collectAsMap() Action**  

âœ… **Converts an RDD into a dictionary**, preserving **key-value pairs**.  
âœ… **Efficiently retrieves all values** without needing iterative lookups.  

ğŸ“œ **Example Usage:**  
```python
sc.parallelize([(1, 2), (3, 4)]).collectAsMap()
# Output: {1: 2, 3: 4}
```
ğŸ’¡ **Think of this like converting a list into a quick-access dictionary!** ğŸ”¥  

---

### ğŸ¯ **Key Takeaways:**  
âœ… **`reduce()` aggregates elements efficiently.**  
âœ… **`saveAsTextFile()` writes RDDs to files, with `coalesce()` ensuring a single output file.**  
âœ… **Pair RDD actions leverage key-value operations for easy aggregation (`countByKey()`, `collectAsMap()`).**  
âœ… **Actions return final results, while transformations create new RDDs.**  

---

# ğŸ—ï¸ **Lab: Action Operations in PySpark RDDs**  

This lab explores **RDD actions** such as `countByKey()` and `flatMap()` in PySpark, demonstrating their ability to **process data and return results** efficiently. Letâ€™s break it down step by step! ğŸ˜Š  

---

## ğŸ”¹ **Counting Keys in an RDD**  

âœ… Uses `countByKey()` to **count occurrences of each key** in a pair RDD.  
âœ… Returns a **default dictionary**, storing counts for each unique key.  
âœ… `countByKey()` is an **action**, meaning it triggers computation **immediately** and **does not create a new RDD**.  

ğŸ“œ **Example Code:**  
```python
dataset = [(1, 2), (3, 4), (3, 6), (4, 5)]
Rdd = sc.parallelize(dataset)

# Apply countByKey action
total = Rdd.countByKey()

# Check the type
print("The type of total is", type(total))

# Iterate over the result
for k, v in total.items(): 
  print("key", k, "has", v, "counts")
```
ğŸ“Œ **Expected Output:**  
```
The type of total is <class 'collections.defaultdict'>
key 1 has 1 counts
key 3 has 2 counts
key 4 has 1 counts
```
ğŸš€ **This confirms `countByKey()` returns key counts as a `defaultdict` instead of a new RDD!**  

---

## ğŸ·ï¸ **Reading and Processing a Text File**  

âœ… **Creates an RDD from a text file**, ensuring the file exists before reading.  
âœ… **Uses `flatMap()` to split lines into individual words**, expanding each line into multiple elements.  
âœ… Applies `.count()` to **compute the total number of words in the dataset**.  

ğŸ“œ **Example Code:**  
```python
file_path = "file:///home/talentum/test-jupyter/P2/M2/SM4/4_AdvancedRddActions/Dataset/Complete_Shakespeare.txt"

# Create RDD from file
baseRDD = sc.textFile(file_path)

# Split lines into words
splitRDD = baseRDD.flatMap(lambda x: x.split())

# Count total words
print("Total number of words in splitRDD:", splitRDD.count())
```
ğŸ“Œ **Expected Output:**  
```
Total number of words in splitRDD: 128576
```
ğŸ’¡ **This demonstrates how to efficiently process a large text file using RDD actions!**  

---

### ğŸ¯ **Key Takeaways:**  
âœ… **`countByKey()` counts occurrences of each key, returning a dictionary.**  
âœ… **Actions trigger computation immediately, unlike transformations.**  
âœ… **Reading a text file with `textFile()` and splitting data using `flatMap()` allows efficient processing.**  
âœ… **PySpark is optimized for handling large-scale datasets dynamically.**  

---

# ğŸ“Œ **Text Processing in PySpark: Word Frequency Analysis**  

This lab explores **RDD transformations and actions** using PySpark to **process and analyze text data** from the *Complete Works of Shakespeare*. Letâ€™s break down each step! ğŸ˜Š  

---

## ğŸ—ï¸ **Step 1: Reading and Preprocessing the Text File**  

âœ… **Load the dataset** using `sc.textFile()` to create an RDD from a file.  
âœ… **Split the text** into individual words using `flatMap()`.  
âœ… **Convert words to lowercase** and **remove stop words** using `filter()`.  

ğŸ“œ **Code Example:**  
```python
file_path = "file:///home/talentum/test-jupyter/P2/M2/SM4/4_AdvancedRddActions/Dataset/Complete_Shakespeare.txt"

# Create a baseRDD from the file path
baseRDD = sc.textFile(file_path)

# Split lines into words
splitRDD = baseRDD.flatMap(lambda x: x.split(' '))

# Convert words to lowercase and remove stop words
splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)
```
ğŸ’¡ **This step ensures we only process meaningful words without common stop words!**  

---

## ğŸ” **Step 2: Word Count Computation**  

âœ… **Create key-value pairs** (`(word, 1)`) using `map()`.  
âœ… **Use `reduceByKey()` to count occurrences** of each unique word in the dataset.  

ğŸ“œ **Code Example:**  
```python
# Create (word, 1) tuples
splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w.lower(), 1))

# Count occurrences of each word
resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)

# Display some results
print(resultRDD.take(10))
```
ğŸ“Œ **Expected Output:**  
```
[('project', 40), ('gutenberg', 35), ('ebook', 4), ('complete', 33), ('works', 35), ('william', 39), ('shakespeare,', 1), ('shakespeare', 42), ('', 65498), ('use', 68)]
```
ğŸ’¡ **This helps analyze word frequencies efficiently in large-scale text data!** ğŸš€  

---

## ğŸ“Š **Step 3: Sorting Word Frequencies**  

âœ… **Swap keys and values** (`(count, word)`) to enable sorting by frequency.  
âœ… **Use `sortByKey(False)` to arrange words in descending order** of occurrence.  
âœ… **Retrieve the top 10 most frequently used words**.  

ğŸ“œ **Code Example:**  
```python
# Swap (word, count) â†’ (count, word)
resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))

# Sort by count in descending order
resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)

# Display the top 10 most frequent words
for word in resultRDD_swap_sort.take(10):
    print("{} has {} counts".format(word[1], word[0]))
```
ğŸ“Œ **Expected Output:**  
```
thou has 650 counts
thy has 574 counts
shall has 393 counts
would has 311 counts
good has 295 counts
thee has 286 counts
love has 273 counts
Enter has 269 counts
th' has 254 counts
```
ğŸ’¡ **This confirms common Shakespearean words like "thou" and "thy" appear frequently!** ğŸ”¥  

---

### ğŸ¯ **Key Takeaways:**  
âœ… **PySpark efficiently processes massive text files** with distributed computing.  
âœ… **Actions like `reduceByKey()` compute word frequencies efficiently**.  
âœ… **Sorting allows quick identification of the most used words**.  
âœ… **Using `filter()` removes unnecessary stop words**, keeping only meaningful data.  

---

# ğŸ—ï¸ **Performing RDD Operations on `constitution.txt`**  

This lab demonstrates **how to process, count, and sort word occurrences** in a dataset using **PySpark RDD transformations and actions**. Letâ€™s break it down step by step! ğŸ˜Š  

---

## ğŸ” **Step 1: Read and Preprocess the Text File**  

âœ… **Load the text file** into an RDD using `sc.textFile()`.  
âœ… **Split the lines** into individual words using `flatMap()`.  
âœ… **Create key-value pairs** (`(word, 1)`) using `map()`.  
âœ… **Aggregate occurrences** of each word using `reduceByKey()`.  

ğŸ“œ **Example Code:**  
```python
# Read the file from HDFS
rdd_lines = sc.textFile("constitution.txt")

# Tokenize words
rdd_words = rdd_lines.flatMap(lambda line: line.split())

# Convert words into key-value pairs
rdd_tup = rdd_words.map(lambda word: (word, 1))

# Aggregate word counts
rdd_final = rdd_tup.reduceByKey(lambda x, y: x + y)

# Display top word counts
print(rdd_final.take(7))
```
ğŸ“Œ **Expected Output:**  
```
[('We', 2), ('the', 662), ('People', 2), ('of', 493), ('United', 85), ('States,', 55), ('in', 137)]
```
ğŸš€ **This confirms `reduceByKey()` efficiently aggregates word occurrences!**  

---

## ğŸ”„ **Step 2: Sorting Words by Frequency**  

âœ… **Swap key-value pairs** (`(count, word)`) for sorting purposes.  
âœ… **Use `sortByKey(False)` to order words by highest frequency first**.  
âœ… **Retrieve the top 7 most frequently used words**.  

ğŸ“œ **Example Code:**  
```python
# Swap (word, count) â†’ (count, word)
rdd_swap = rdd_final.map(lambda tup: (tup[1], tup[0]))

# Sort words by count in descending order
rdd_sorted = rdd_swap.sortByKey(ascending=False)

# Display top words
print(rdd_sorted.take(7))
```
ğŸ“Œ **Expected Output:**  
```
[(662, 'the'), (493, 'of'), (293, 'shall'), (256, 'and'), (183, 'to'), (178, 'be'), (157, 'or')]
```
ğŸ’¡ **This step ranks the most frequent words from the Constitution efficiently!**  

---

## ğŸï¸ **Step 3: One-Liner Implementation**  

ğŸ’¡ If you prefer a **single-line command**, you can simplify the process:  
```python
print(sc.textFile("constitution.txt").flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y).map(lambda tup: (tup[1], tup[0])).sortByKey(ascending=False).take(7))
```
ğŸ“Œ **Expected Output:**  
```
[(662, 'the'), (493, 'of'), (293, 'shall'), (256, 'and'), (183, 'to'), (178, 'be'), (157, 'or')]
```
ğŸš€ **This one-liner performs the entire workflow efficiently!**  

---

### ğŸ¯ **Key Takeaways:**  
âœ… **RDD actions like `reduceByKey()` efficiently count word occurrences**.  
âœ… **Swapping key-value pairs allows sorting by frequency (`sortByKey(False)`)**.  
âœ… **One-liner implementations streamline complex operations**.  

---

# ğŸš€ **PySpark: Verbosity & Data Pipeline as USPs**  

PySpark is known for its **verbosity**, meaning it provides detailed logs and error messages to aid debugging. Additionally, its **data pipeline capabilities** allow for efficient processing of distributed datasets. These features make PySpark a **preferred choice** for **big data transformations**. ğŸ˜Š  

---

## ğŸ—ï¸ **Standalone vs. Jupyter Notebook in PySpark**  

âœ… **Standalone applications** require creating a **separate file** and running it.  
âœ… **Jupyter Notebook is not standalone**â€”it is designed for **interactive development and testing**.  
âœ… **Jupyter is ideal for exploratory analysis**, but **not typically used in production**.  

ğŸ’¡ **Think of Jupyter like a sandbox**â€”you test and refine your code before deploying it in production using **standalone applications**! ğŸš€  

---

# ğŸ”¥ **Running PySpark via `spark-submit`**
### ğŸ›  **Setting Up a Standalone PySpark Script**
âœ… **Python Path Check:**  
```sh
which python  
/usr/bin/python  
```
âœ… **Create a Python file (`wordcount.py`)**  
âœ… **Ensure Spark is used instead of the Python shell (`spark-submit`)**  

ğŸ“œ **Running the script:**  
```sh
spark-submit wordcount.py  
```

### â— **Common Errors & Fixes**
ğŸš€ **Issue:** `"Jupyter not found"`  
âœ… **Fix:** Run â†’  
```sh
source /home/talentum/shared/unset_jupyter.sh  
```
ğŸš€ **Issue:** `"name 'sc' is not defined"`  
âœ… **Fix:** Add the following after the shebang (`#!`):  
```python
# Entrypoint for Spark (2.x+)
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Spark SQL basic example").enableHiveSupport().getOrCreate()
sc = spark.sparkContext
```
ğŸš€ **Issue:** Running with YARN  
âœ… **Fix:** Specify the `.master("yarn")`  
```python
spark = SparkSession.builder.appName("Spark SQL basic example").enableHiveSupport().master("yarn").getOrCreate()
```

ğŸ“œ **Final Execution:**  
```sh
spark-submit wordcount.py
echo $?
0  # Execution successful!
```

![image](https://github.com/user-attachments/assets/75daa605-d8dd-4729-97f8-4da2d286f1bd)

---

### ğŸ¯ **Key Takeaways**
âœ… **PySpark is verbose, providing helpful logs for debugging.**  
âœ… **Standalone applications require `spark-submit`, unlike Jupyter.**  
âœ… **RDD processing optimizes text parsing & word frequency analysis.**  
âœ… **Sorting allows quick identification of commonly used words.**  
âœ… **Setting up `SparkSession` resolves environment issues.**

---

# ğŸš€ **PySpark SQL & DataFrames**  

## ğŸ”¹ **Introduction to PySpark DataFrames**  
PySparkSQL is a **library for structured data processing** that provides insights into **data structure and computation**.  

âœ… **DataFrame is an immutable distributed collection of data with named columns**.  
âœ… Designed for **structured (RDBMS) and semi-structured (JSON) data processing**.  
âœ… **Supports SQL queries (`SELECT * FROM table`)** and **expression methods (`df.select()`)**.  
âœ… DataFrame API is available in **Python, R, Scala, and Java**.  

ğŸ’¡ **Think of PySpark DataFrames as an enhanced version of Pandas DataFrames but optimized for big data and distributed computing!** ğŸš€  

---

## ğŸ—ï¸ **SparkSession: Entry Point for DataFrame API**  

âœ… **SparkContext is the main entry point for creating RDDs**, but for DataFrames, you use **SparkSession**.  
âœ… **SparkSession provides a unified entry point** to manage DataFrames and execute SQL queries.  
âœ… Available in **PySpark shell as `spark`**.  

ğŸ“œ **Example:**  
```python
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.appName("PySpark SQL Example").getOrCreate()
```
ğŸš€ **This allows direct interaction with Spark's SQL API and DataFrame operations!**  

---

## ğŸ”„ **Creating DataFrames in PySpark**  

PySpark supports **two primary ways** to create DataFrames:  

1ï¸âƒ£ **From existing RDDs** using `createDataFrame()`  
```python
rdd = spark.sparkContext.parallelize([(1, "Alice"), (2, "Bob")])
df = spark.createDataFrame(rdd, ["ID", "Name"])
df.show()
```  

2ï¸âƒ£ **From data sources (CSV, JSON, TXT)** using `spark.read`  
```python
df_csv = spark.read.csv("data.csv", header=True, inferSchema=True)
df_csv.show()
```  

âœ… **Schema helps optimize queries** by providing metadata (column names, data types, missing values).  
âœ… **RDDs do not have schemas, but DataFrames contain both schema and data** for structured processing.  

ğŸ’¡ **Think of schema as a blueprint for your data**â€”it ensures consistency and enhances query performance!  

---

### ğŸ¯ **Key Takeaways**  
âœ… PySparkSQL processes **structured & semi-structured data** efficiently.  
âœ… **SparkSession is the entry point** for DataFrames and SQL queries.  
âœ… **DataFrames support SQL-like queries (`SELECT * FROM table`) and API methods (`df.select()`).**  
âœ… **Schemas improve data handling**, unlike RDDs which lack structured metadata.  

---

# ğŸš€ **Creating DataFrames in PySpark**  

DataFrames in PySpark are **structured and optimized** for efficient data processing, unlike RDDs which lack schema definitions. Hereâ€™s how you can create **DataFrames from RDDs and external files**. ğŸ˜Š  

---

## ğŸ”¹ **Create a DataFrame from an RDD**  

âœ… **Convert an RDD into a DataFrame** using `createDataFrame()`.  
âœ… **Provide schema (column names) to structure the data**.  

ğŸ“œ **Example Code:**  
```python
iphones_RDD = sc.parallelize([
    ("XS", 2018, 5.65, 2.79, 6.24),
    ("XR", 2018, 5.94, 2.98, 6.84),
    ("X10", 2017, 5.65, 2.79, 6.13),
    ("8Plus", 2017, 6.23, 3.07, 7.12)
])

# Define schema
names = ['Model', 'Year', 'Height', 'Width', 'Weight']

# Convert RDD to DataFrame
iphones_df = spark.createDataFrame(iphones_RDD, schema=names)

# Verify DataFrame type
print(type(iphones_df))
```
ğŸ“Œ **Expected Output:**  
```
<class 'pyspark.sql.dataframe.DataFrame'>
```
ğŸš€ **This ensures structured data processing with named columns!**  

---

## ğŸ—ï¸ **Create a DataFrame from CSV/JSON/TXT**  

âœ… **Use `spark.read` to load structured files** (CSV, JSON, TXT).  
âœ… **Specify `header=True` to use the first row as column names**.  
âœ… **Use `inferSchema=True` to automatically detect column types**.  

ğŸ“œ **Example Code:**  
```python
# Load CSV file
df_csv = spark.read.csv("people.csv", header=True, inferSchema=True)

# Load JSON file
df_json = spark.read.json("people.json", header=True, inferSchema=True)

# Load TXT file
df_txt = spark.read.text("people.txt")
```
ğŸ’¡ **Key Parameters:**  
âœ… **`header=True`** â†’ Uses the first row as headers.  
âœ… **`inferSchema=True`** â†’ Detects column data types automatically.  

---

### ğŸ¯ **Key Takeaways**  
âœ… **DataFrames provide structured processing**, unlike RDDs.  
âœ… **Schema improves readability and query optimization**.  
âœ… **Use `spark.read` to load CSV, JSON, and TXT files efficiently**.  
âœ… **Apply `header=True, inferSchema=True` to auto-define column names & types**.  

---

# ğŸ—ï¸ **PySpark Lab: Creating & Managing DataFrames**  

This lab explores **creating DataFrames in PySpark** from an **RDD and a CSV file**, checking schema structure, and verifying data types. Let's break it down step by step! ğŸ˜Š  

---

## ğŸ”¹ **Create a DataFrame from an RDD**  

âœ… **Convert an RDD into a DataFrame** using `createDataFrame()`.  
âœ… **Define a schema (`Name`, `Age`) to structure the DataFrame**.  

ğŸ“œ **Example Code:**  
```python
# Create a list of tuples
sample_list = [('Mona', 20), ('Jennifer', 34), ('John', 20), ('Jim', 26)]

# Create an RDD from the list
rdd = sc.parallelize(sample_list)

# Convert RDD into a DataFrame with a schema
names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])

# Check the DataFrame type
print("The type of names_df is", type(names_df))
```
ğŸ“Œ **Expected Output:**  
```
The type of names_df is <class 'pyspark.sql.dataframe.DataFrame'>
```
ğŸ’¡ **This confirms a structured DataFrame with named columns!** ğŸš€  

---

## ğŸ—ï¸ **Create a DataFrame from a CSV File**  

âœ… **Load structured data from a file using `spark.read.csv()`**.  
âœ… **Apply schema detection with `header=True, inferSchema=True`**.  
âœ… **Check schema details using `.schema`**.  

ğŸ“œ **Example Code:**  
```python
file_path = "file:///home/talentum/shared/6_Spark/data/1_AbstractingDatawithDataFrames/Dataset/people.csv"

# Load DataFrame from CSV file
people_df = spark.read.csv(file_path, header=True, inferSchema=True)

# Check the DataFrame type
print("The type of people_df is", type(people_df))

# Display a sample of the DataFrame
people_df.show()

# Print the schema of the DataFrame
print(people_df.schema)
```
ğŸ“Œ **Expected Output (Schema Format):**  
```
StructType(List(
    StructField(_c0,IntegerType,true),
    StructField(person_id,IntegerType,true),
    StructField(name,StringType,true),
    StructField(sex,StringType,true),
    StructField(date of birth,StringType,true)
))
```
ğŸ’¡ **PySpark DataFrames require an associated schema, ensuring optimized queries & structured data processing!**  

---

### ğŸ¯ **Key Takeaways:**  
âœ… **DataFrames can be created from RDDs using `createDataFrame()`**.  
âœ… **`spark.read.csv()` loads structured files efficiently**.  
âœ… **`header=True` uses the first row for column names**.  
âœ… **`inferSchema=True` automatically detects data types**.  
âœ… **A DataFrame **must have a schema**, unlike raw RDDs.  

---

# ğŸš€ **PySpark DataFrame Operators: Transformations & Actions**  

PySpark **DataFrame operations** can be divided into two categories:  
âœ… **Transformations** â†’ Modify or filter DataFrames without executing immediately.  
âœ… **Actions** â†’ Execute and return results to the driver program.  

---

## ğŸ”¹ **Common DataFrame Transformations**  
- `select()` â†’ Subset specific columns.  
- `filter()` â†’ Filter rows based on conditions.  
- `groupBy()` â†’ Group data based on a column.  
- `orderBy()` â†’ Sort DataFrame rows.  
- `dropDuplicates()` â†’ Remove duplicate records.  
- `withColumnRenamed()` â†’ Rename columns for better readability.  

## ğŸ”¹ **Common DataFrame Actions**  
- `printSchema()` â†’ Display DataFrame schema.  
- `head()` â†’ Retrieve the first row.  
- `show()` â†’ Display the first 20 rows (default).  
- `count()` â†’ Return total number of rows.  
- `columns` â†’ List DataFrame column names.  
- `describe()` â†’ Summarize DataFrame statistics.  

---

## ğŸ—ï¸ **select() & show() Operations**  

âœ… **`select()` extracts specific columns**.  
âœ… **`show()` prints the first 20 rows (default) in tabular format**.  

ğŸ“œ **Example Usage:**  
```python
df_id_age = test.select("Age")  
df_id_age.show(3)
```
ğŸ“Œ **Expected Output:**  
```
+---+
|Age|
+---+
| 17|
| 17|
| 17|
+---+
only showing top 3 rows
```
ğŸ’¡ **This allows efficient column selection while maintaining structured output!**  

---

## ğŸ” **filter() & show() Operations**  

âœ… **`filter()` removes rows based on a condition**.  
âœ… **`show()` displays the filtered results**.  

ğŸ“œ **Example Usage:**  
```python
new_df_age21 = new_df.filter(new_df.Age > 21)  
new_df_age21.show(3)
```
ğŸ“Œ **Expected Output:**  
```
+-------+------+---+
|User_ID|Gender|Age|
+-------+------+---+
|1000002|    M| 55|
|1000003|    M| 26|
|1000004|    M| 46|
+-------+------+---+
only showing top 3 rows
```
ğŸš€ **This efficiently filters data while keeping the structure intact!**  

---

## ğŸ”„ **groupBy() & count() Operations**  

âœ… **`groupBy()` groups DataFrame rows based on a column**.  
âœ… **`count()` returns the total count per group**.  

ğŸ“œ **Example Usage:**  
```python
test_df_age_group = test_df.groupby("Age")  
test_df_age_group.count().show(3)
```
ğŸ“Œ **Expected Output:**  
```
+---+------+
|Age| count|
+---+------+
| 26|219587|
| 17|     4|
| 55| 21504|
+---+------+
only showing top 3 rows
```
ğŸ’¡ **This is an action, not a transformationâ€”it triggers computation immediately!**  

---

### ğŸ¯ **Key Takeaways:**  
âœ… **Transformations modify the DataFrame but donâ€™t execute immediately**.  
âœ… **Actions compute results and return them to the driver program**.  
âœ… **`select()`, `filter()`, `groupBy()`, and `show()` allow efficient data exploration**.  
âœ… **Grouping and counting enables better analysis of categorical data**.  

---

# ğŸš€ **PySpark DataFrame Transformations & Actions**  

DataFrames in PySpark support **various operations** that help in data transformation and analysis. Let's break down the **key transformations and actions** with clear explanations and examples! ğŸ˜Š  

---

## ğŸ”„ **DataFrame Transformations**  
Transformations modify a DataFrame but **do not execute immediately**.  
They are applied **lazily** and require an action to trigger computation.

### ğŸ”¹ **`orderBy()` Transformation**
âœ… **Sorts the DataFrame based on one or more columns**.  

ğŸ“œ **Example Usage:**  
```python
test_df_age_group.count().orderBy("Age").show(3)
```
ğŸ“Œ **Expected Output:**  
```
+---+-----+
|Age|count|
+---+-----+
|  0|15098|
| 17|    4|
| 18|99660|
+---+-----+
only showing top 3 rows
```
ğŸ’¡ **`orderBy()` ensures rows are sorted efficiently** for better readability and analysis!  

---

### ğŸ”¹ **`dropDuplicates()` Transformation**  
âœ… **Removes duplicate rows from a DataFrame**.  

ğŸ“œ **Example Usage:**  
```python
test_df_no_dup = test_df.select("User_ID", "Gender", "Age").dropDuplicates()
print(test_df_no_dup.count())
```
ğŸ“Œ **Expected Output:**  
```
5892
```
ğŸš€ **Ideal for cleaning datasets while retaining only unique records!**  

---

### ğŸ”¹ **`withColumnRenamed()` Transformation**  
âœ… **Renames a specific column in the DataFrame**.  

ğŸ“œ **Example Usage:**  
```python
test_df_sex = test_df.withColumnRenamed("Gender", "Sex")
test_df_sex.show(3)
```
ğŸ“Œ **Expected Output:**  
```
+-------+---+---+
|User_ID|Sex|Age|
+-------+---+---+
|1000001|  F| 17|
|1000001|  F| 17|
|1000001|  F| 17|
+-------+---+---+
```
ğŸ’¡ **Useful when renaming column names for better clarity in analysis!**  

---

## âš¡ **DataFrame Actions**  
Actions **trigger computation** and return results to the driver program.

### ğŸ”¹ **`printSchema()` Action**  
âœ… **Displays column types in the DataFrame schema**.  

ğŸ“œ **Example Usage:**  
```python
test_df.printSchema()
```
ğŸ“Œ **Expected Output:**  
```
|-- User_ID: integer (nullable = true)
|-- Product_ID: string (nullable = true)
|-- Gender: string (nullable = true)
|-- Age: string (nullable = true)
|-- Occupation: integer (nullable = true)
|-- Purchase: integer (nullable = true)
```
ğŸš€ **Helps verify data structure before performing transformations!**  

---

### ğŸ”¹ **`columns` Action**  
âœ… **Lists all column names in the DataFrame**.  

ğŸ“œ **Example Usage:**  
```python
print(test_df.columns)
```
ğŸ“Œ **Expected Output:**  
```
['User_ID', 'Gender', 'Age']
```
ğŸ’¡ **Useful for checking available fields in the DataFrame!**  

---

### ğŸ”¹ **`describe()` Action**  
âœ… **Computes summary statistics for numerical columns**.  
âœ… **Can help detect outliers in the dataset**.  

ğŸ“œ **Example Usage:**  
```python
test_df.describe().show()
```
ğŸ“Œ **Expected Output:**  
```
+-------+------------------+------+------------------+
|summary| User_ID | Gender | Age |
+-------+------------------+------+------------------+
| count | 550068 | 550068 | 550068 |
| mean  | 1003028.8424013031 | null | 30.38 |
| stddev| 1727.5915855307312 | null | 11.86 |
| min   | 1000001 | F | 0 |
| max   | 1006040 | M | 55 |
+-------+------------------+------+------------------+
```
ğŸš€ **Perfect for identifying missing values, patterns, and outliers in numerical data!**  

---

### ğŸ¯ **Key Takeaways**  
âœ… **Transformations modify the DataFrame but do not execute immediately**.  
âœ… **Actions trigger computations and return results** to the driver.  
âœ… **Sorting, filtering, grouping, and renaming enhance structured data processing**.  
âœ… **Summary statistics help detect outliers and data inconsistencies**.  

---

# ğŸ” Lab 1: Inspecting Data in PySpark DataFrame ğŸš€  

## ğŸ“ Overview  
Before analyzing data (plotting, modeling, training), it's **crucial** to **inspect** it. In this lab, weâ€™ll examine the `people_df` DataFrame using fundamental PySpark operations.  

âœ… **Print the first 10 observations**  
âœ… **Count the number of rows**  
âœ… **Identify the number of columns & their names**  

---

## âš™ï¸ Loading the Data  

We start by **reading the CSV file** and creating a PySpark **DataFrame**:  

```python
file_path = "file:///home/talentum/test-jupyter/P2/M3/sm2/2_OperatingonDataFramesinPySpark/Dataset/people.csv"

# Load data into a DataFrame
people_df = spark.read.csv(file_path, header=True, inferSchema=True)
```

---

## ğŸ“Œ Inspecting Data  

### ğŸ”¹ **Step 1: View First 10 Rows**  
To quickly glance at the **top 10 records**, we use `.show(10)`:  

```python
# Print the first 10 observations 
people_df.show(10)
```

ğŸ“Œ **Output:**  
```
+---+---------+----------------+------+-------------+
|_c0|person_id|            name|   sex|date of birth|
+---+---------+----------------+------+-------------+
|  0|      100|  Penelope Lewis|female|   1990-08-31|
|  1|      101|   David Anthony|  male|   1971-10-14|
|  2|      102|       Ida Shipp|female|   1962-05-24|
|  3|      103|    Joanna Moore|female|   2017-03-10|
|  4|      104|  Lisandra Ortiz|female|   2020-08-05|
|  5|      105|   David Simmons|  male|   1999-12-30|
|  6|      106|   Edward Hudson|  male|   1983-05-09|
|  7|      107|    Albert Jones|  male|   1990-09-13|
|  8|      108|Leonard Cavender|  male|   1958-08-08|
|  9|      109|  Everett Vadala|  male|   2005-05-24|
+---+---------+----------------+------+-------------+
```

---

### ğŸ”¹ **Step 2: Count Total Rows**  
To **check data size**, we use `.count()`:  

```python
# Count the number of rows 
print("There are {} rows in the people_df DataFrame.".format(people_df.count()))
```

ğŸ“Œ **Output:**  
```
There are 100000 rows in the people_df DataFrame.
```

---

### ğŸ”¹ **Step 3: Check Columns & Their Names**  
To **list the column names and count them**, we use `.columns` and `len()`:  

```python
# Count the number of columns and their names
print("There are {} columns in the people_df DataFrame and their names are {}".format(len(people_df.columns), people_df.columns))
```

ğŸ“Œ **Output:**  
```
There are 5 columns in the people_df DataFrame and their names are ['_c0', 'person_id', 'name', 'sex', 'date of birth']
```

---

## ğŸ¯ Key Takeaways  
âœ… **Dataset loaded successfully!**  
âœ… **Contains 100,000 rows** and **5 columns** (`_c0, person_id, name, sex, date of birth`).  
âœ… **First 10 rows previewed** for inspection.  

ğŸ“Œ **Next Steps:** Now that we **inspected** the data, we can proceed with **data transformations, filtering, and analysis!** ğŸš€  

---

# âœ¨ Lab 2: PySpark DataFrame Subsetting & Cleaning  

## ğŸ“ Overview  
Once we **inspect** the dataset, the next step is **cleaning**. This involves:  

âœ… **Subsetting** specific columns  
âœ… **Removing duplicate rows**  
âœ… **Counting rows before & after cleaning**  

---

## âš™ï¸ Loading the Data  

We first **read the CSV file** to create a PySpark **DataFrame**:  

```python
file_path = "file:///home/talentum/test-jupyter/P2/M3/sm2/2_OperatingonDataFramesinPySpark/Dataset/people.csv"

# Load data into a DataFrame
people_df = spark.read.csv(file_path, header=True, inferSchema=True)
```

---

## ğŸ¯ Subsetting Relevant Columns  

Since we only need **'name', 'sex', and 'date of birth'**, we use `.select()`:  

```python
# Select name, sex, and date of birth columns
people_df_sub = people_df.select('name', 'sex', 'date of birth')
```

---

## ğŸ“Œ Inspecting Data  

### ğŸ”¹ **Step 1: View First 10 Rows**  
To **quickly check** the top 10 records, we use `.show(10)`:  

```python
# Print the first 10 observations from people_df_sub
people_df_sub.show(10)
```

---

## âŒ Removing Duplicate Entries  

### ğŸ” **Step 2: Drop Duplicates**  
To remove duplicate entries from `people_df_sub`, we use `.dropDuplicates()`:  

```python
# Remove duplicate entries
people_df_sub_nodup = people_df_sub.dropDuplicates()
```

---

## ğŸ”¢ Comparing Row Counts  

### ğŸ”¹ **Step 3: Count Rows Before & After**  

```python
# Count the number of rows
print("There were {} rows before removing duplicates, and {} rows after removing duplicates".format(people_df_sub.count(), people_df_sub_nodup.count()))
```

ğŸ“Œ **This helps verify how many duplicates were removed!**

---

## ğŸ” Identifying Duplicate Entries  

### ğŸ”¹ **Step 4: Find Duplicate Rows**  

```python
# Group by 'name', 'sex', 'date of birth' and count occurrences
df1 = people_df_sub.groupBy('name', 'sex', 'date of birth').count()

# Show duplicate entries
duplicates = df1.where('count > 1')
duplicates.show(10)
```

ğŸ“Œ This allows us to check **which rows** appear **more than once**.

---

## ğŸ”„ Alternate Method to Show Duplicates  

```python
# Display rows that were removed
people_df_sub.exceptAll(people_df_sub_nodup).show()
```

ğŸ“Œ This gives a **direct view** of the duplicate rows that were removed.

---

## ğŸ¯ Key Takeaways  
âœ… **Dataset successfully cleaned!**  
âœ… **Subsetted relevant columns (`name`, `sex`, `date of birth`)**  
âœ… **Removed duplicate entries**  
âœ… **Verified row count changes before & after cleaning**  

ğŸ“Œ **Next Steps:** Now that data is **clean**, we can perform **analysis & transformations!** ğŸš€  

---

# ğŸ” Lab 3: Filtering Data in PySpark DataFrame ğŸš€  

## ğŸ“ Overview  
In the previous exercise, we **subsetted data column-wise** using `.select()`. Now, we'll filter rows based on **specific conditions**, such as selecting:  

âœ… **Only female records**  
âœ… **Only male records**  
âœ… **Counting the number of rows in each dataset**  

---

## âš™ï¸ Loading the Data  

As always, we first **load the CSV file** into a PySpark **DataFrame**:  

```python
file_path = "file:///home/talentum/test-jupyter/P2/M3/sm2/2_OperatingonDataFramesinPySpark/Dataset/people.csv"

# Load data into a DataFrame
people_df = spark.read.csv(file_path, header=True, inferSchema=True)
```

---

## ğŸ¯ Filtering Records by Sex  

### ğŸ”¹ **Step 1: Select Only Female Entries**  
Using `.filter()`, we extract rows where `"sex"` is `"female"`:  

```python
# Filter people_df to select only female records
people_df_female = people_df.filter(people_df.sex == "female")
```

---

### ğŸ”¹ **Step 2: Select Only Male Entries**  
Similarly, we extract rows where `"sex"` is `"male"`:  

```python
# Filter people_df to select only male records
people_df_male = people_df.filter(people_df.sex == "male")
```

---

## ğŸ”¢ Counting Rows in Each Filtered DataFrame  

### ğŸ” **Step 3: Count Female & Male Records**  

```python
# Count rows in each DataFrame
print("There are {} rows in the people_df_female DataFrame and {} rows in the people_df_male DataFrame".format(people_df_female.count(), people_df_male.count()))
```

ğŸ“Œ **Output:**  
```
There are 49,014 rows in the people_df_female DataFrame and 49,066 rows in the people_df_male DataFrame.
```

ğŸ“Œ The slight difference in row count **suggests that our dataset is nearly balanced between male & female records.**

---

## ğŸ¯ Key Takeaways  
âœ… **Filtered data efficiently using `.filter()`**  
âœ… **Created separate DataFrames for male & female records**  
âœ… **Counted rows in each filtered dataset**  

ğŸ“Œ **Next Steps:** We can now perform **further analysis** on each subset, such as checking age distributions or running demographic trends! ğŸš€  

---

# âš¡ Interacting with DataFrames Using PySpark SQL  

## ğŸ” DataFrame API vs SQL Queries  

In **PySpark**, you can interact with **Spark SQL** using **two approaches**:  

âœ… **DataFrame API** (Programmatic Domain-Specific Language - DSL)  
âœ… **SQL Queries** (Concise & Portable)  

### ğŸ¯ When to Use Each?  

ğŸ”¹ **DataFrame API:**  
   - **Best for transformations & actions**  
   - Easier for **programmatic execution**  
   - Works well with **Sparkâ€™s distributed nature**  

ğŸ”¹ **SQL Queries:**  
   - **Readable, familiar syntax** (especially for SQL users)  
   - **Portable across databases**  
   - Works seamlessly within **Spark environments**  

ğŸ“Œ **You can use SQL queries to perform operations on PySpark DataFrames!**  

---

## ğŸ”„ Executing SQL Queries  

PySpark allows executing SQL **directly** using `.sql()` within `SparkSession`.  

```python
# Execute SQL queries using SparkSession
df.createOrReplaceTempView("table1")  # Stores the table in MetaStore.

df2 = spark.sql("SELECT field1, field2 FROM table1")
df2.collect()
```

ğŸ“Œ **Output:**  
```
[Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]
```

ğŸš¨ **Note:** Hive Metastore **must be running** for SQL-based querying!

---

## ğŸ” Extracting Data Using SQL Queries  

### ğŸ¯ Example Query  
```python
test_df.createOrReplaceTempView("test_table")

query = '''SELECT Product_ID FROM test_table'''
test_product_df = spark.sql(query) 
test_product_df.show(5)
```

ğŸ“Œ **Output:**  
```
+----------+
|Product_ID|
+----------+
| P00069042|
| P00248942|
| P00087842|
| P00085442|
| P00285442|
+----------+
```

ğŸš€ **Quick retrieval of specific columns!**  

---

## ğŸ“Š Summarizing & Grouping Data Using SQL Queries  

### ğŸ¯ Example: Find Maximum Purchase Per Age Group  

```python
test_df.createOrReplaceTempView("test_table")
query = '''SELECT Age, max(Purchase) FROM test_table GROUP BY Age'''
spark.sql(query).show(5)
```

ğŸ“Œ **Output:**  
```
+-----+-------------+
| Age | max(Purchase) |
+-----+-------------+
|18-25| 23958 |
|26-35| 23961 |
| 0-17| 23955 |
|46-50| 23960 |
|51-55| 23960 |
+-----+-------------+
```

ğŸ“Œ **Data is grouped & aggregated efficiently using SQL syntax!**  

---

## ğŸ” Filtering Columns Using SQL Queries  

### ğŸ¯ Example: Select Female Users with Purchases Over 20,000  

```python
test_df.createOrReplaceTempView("test_table")

query = '''SELECT Age, Purchase, Gender FROM test_table WHERE Purchase > 20000 AND Gender == "F"'''
spark.sql(query).show(5)
```

ğŸ“Œ **Output:**  
```
+-----+--------+------+
| Age | Purchase | Gender |
+-----+--------+------+
|36-45| 23792 | F |
|26-35| 21002 | F |
|26-35| 23595 | F |
|26-35| 23341 | F |
|46-50| 20771 | F |
+-----+--------+------+
```

ğŸ“Œ **Easy filtering with SQL conditions!**  

---

## ğŸ¯ Key Takeaways  

âœ… **PySpark SQL enables seamless querying within Spark environments!**  
âœ… **Use DataFrame API for programmatic execution & SQL for concise data manipulation.**  
âœ… **SQL queries allow aggregation, filtering, and extraction of structured data efficiently.**  

ğŸ“Œ **Next Steps:** We can now explore **joining tables, performing complex aggregations, and optimizing queries!** ğŸš€  

---

# âš¡ Lab 1: Running SQL Queries Programmatically  

## ğŸ“ Overview  
PySpark allows you to **run SQL queries** directly on DataFrames using the `sql()` function.  
By leveraging **SparkSession**, you can:  

âœ… **Create temporary tables** from PySpark DataFrames  
âœ… **Run SQL queries programmatically**  
âœ… **Store results in new DataFrames** for further analysis  

---

## âš™ï¸ Loading the Data  

First, we load the CSV file into a PySpark **DataFrame**:  

```python
file_path = "file:///home/talentum/test-jupyter/P2/M3/SM3/3_InteractingwithDataFramesusingPySparkSQL/Dataset/people.csv"

# Load data into a DataFrame
people_df = spark.read.csv(file_path, header=True, inferSchema=True)
```

---

## ğŸ¯ Creating a Temporary Table  

A **temporary table** acts as a **pointer** to our DataFrame, enabling SQL operations:  

```python
# Create a temporary table "people"
people_df.createOrReplaceTempView("people")
```

ğŸ“Œ **This step makes 'people' accessible for SQL queries!**  

---

## ğŸ” Executing SQL Query  

### ğŸ”¹ **Step 1: Select Names from Temporary Table**  
```python
# Construct SQL query
query = '''SELECT name FROM people'''

# Assign query results to a new DataFrame
people_df_names = spark.sql(query)
```

ğŸ“Œ **Now, `people_df_names` holds only the names from our dataset!**  

---

## ğŸ“Œ Inspecting the Query Results  

### ğŸ” **Step 2: Print the First 10 Names**  
```python
# Display the top 10 names
people_df_names.show(10)
```

ğŸ“Œ **Output:**  
```
+-----------------+
|             name|
+-----------------+
|   Penelope Lewis|
|    David Anthony|
|        Ida Shipp|
|     Joanna Moore|
|   Lisandra Ortiz|
|    David Simmons|
|    Edward Hudson|
|     Albert Jones|
| Leonard Cavender|
|   Everett Vadala|
+-----------------+
```

ğŸ“Œ **SQL queries return DataFrames, which can be further processed!**  

---

## ğŸ¯ Key Takeaways  

âœ… **Successfully executed SQL queries on PySpark DataFrames**  
âœ… **Created a temporary table (`people`) for querying**  
âœ… **Extracted and displayed names using SQL**  
âœ… **Results returned as a new DataFrame (`people_df_names`) for further analysis**  

ğŸ“Œ **Next Steps:** Try running **more advanced SQL queries** like grouping, filtering, or aggregations! ğŸš€  

---

# ğŸ” Lab 2: Filtering Data Using SQL Queries ğŸš€  

## ğŸ“ Overview  
Now that we've run a **basic SQL query**, let's move to **more advanced filtering**!  
Weâ€™ll:  

âœ… **Filter the dataset** by gender (`male` & `female`) using SQL  
âœ… **Create separate DataFrames** for filtered records  
âœ… **Count rows in each filtered dataset**  

---

## âš™ï¸ Loading the Data  

As always, we **load the CSV file** into a PySpark **DataFrame**:  

```python
file_path = "file:///home/talentum/test-jupyter/P2/M3/SM3/3_InteractingwithDataFramesusingPySparkSQL/Dataset/people.csv"

# Load data into a DataFrame
people_df = spark.read.csv(file_path, header=True, inferSchema=True)
```

---

## ğŸ—ï¸ Creating a Temporary Table  

Before running SQL queries, we create a **temporary table** for `people_df`:  

```python
# Create a temporary table "people"
people_df.createOrReplaceTempView("people")
```

ğŸ“Œ **Now, we can query the DataFrame using SQL statements!**  

---

## ğŸ” Filtering Data Using SQL Queries  

### ğŸ”¹ **Step 1: Select Female Records**  
```python
# Filter people table to select only female records
people_female_df = spark.sql('SELECT * FROM people WHERE sex=="female"')
```

---

### ğŸ”¹ **Step 2: Select Male Records**  
```python
# Filter people table to select only male records
people_male_df = spark.sql('SELECT * FROM people WHERE sex=="male"')
```

---

## ğŸ”¢ Counting Rows in Each Filtered DataFrame  

### ğŸ” **Step 3: Count Female & Male Records**  

```python
# Count rows in each DataFrame
print("There are {} rows in the people_female_df and {} rows in the people_male_df DataFrames".format(people_female_df.count(), people_male_df.count()))
```

ğŸ“Œ **Output:**  
```
There are 49,014 rows in the people_female_df and 49,066 rows in the people_male_df DataFrames.
```

ğŸ“Œ The **row count difference** suggests a nearly **balanced dataset** in terms of gender representation.  

---

## ğŸ¯ Key Takeaways  

âœ… **Used SQL queries to filter DataFrames based on specific conditions!**  
âœ… **Created separate DataFrames (`people_female_df`, `people_male_df`) for gender filtering**  
âœ… **Counted rows for validation**  

---

# ğŸš€ Introduction to Data Cleaning with Apache Spark  

## ğŸ“ What is Data Cleaning?  

Data cleaning is the **process of preparing raw data** for efficient use in **data processing pipelines**. Common tasks include:  

âœ… **Reformatting or replacing text** (standardizing input values)  
âœ… **Performing calculations** (converting units, applying transformations)  
âœ… **Removing garbage or incomplete data** (handling missing values)  

ğŸ“Œ **Ensuring data integrity improves analysis and model accuracy!**  

---

## âš¡ Why Perform Data Cleaning with Spark?  

Traditional data systems **struggle** with performance and data flow organization.  
Apache Spark solves these issues by offering:  

âœ… **Scalability** â€“ Handles **large datasets efficiently** via **distributed processing**  
âœ… **Lazy Evaluation** â€“ Executes transformations **only when needed**, optimizing performance  
âœ… **Powerful Data Handling** â€“ Built-in **resilient** and **fault-tolerant** capabilities  

ğŸ“Œ **Spark simplifies large-scale data cleaning while boosting speed & efficiency!**  

---

## ğŸ” Data Cleaning Example  

### ğŸ“Œ **Raw Data (Before Cleaning)**  
```
name              age (years)       city  
---------------------------------------
Smith, John         37           Dallas  
Wilson, A.          59           Chicago  
null               215            (invalid)
```

### âœ… **Cleaned Data (After Processing)**  
```
last name      first name    age (months)   state  
-------------------------------------------------
Smith           John           444         TX  
Wilson          A              708         IL  
```

### ğŸ’¡ **Key Transformations:**  
ğŸ”¹ **Splitting names** into **first** and **last**  
ğŸ”¹ **Converting age** from **years to months**  
ğŸ”¹ **Standardizing city names to state abbreviations**  
ğŸ”¹ **Removing garbage/missing values**  

ğŸ“Œ **Data is now structured & ready for analysis!**  

---

## ğŸ”§ Spark Schemas  

### ğŸ—ï¸ **Why Use Schemas?**  

âœ… **Defines DataFrame structure**  
âœ… **Supports multiple data types** (strings, dates, integers, arrays)  
âœ… **Filters garbage data during import**  
âœ… **Boosts performance** via **optimized storage & lazy evaluation**  

ğŸ“Œ **Schemas help maintain data integrity while improving query speed!**  

---

## ğŸ—ï¸ Example Spark Schema  

Spark **schemas** always follow a `StructType` format.  
All **Spark data types** are sourced from the `pyspark.sql.types` package.  

### ğŸ”¹ **Defining a Schema in PySpark**  
```python
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Define custom schema
peopleSchema = StructType([
    StructField('name', StringType(), True),   # Name column (string)
    StructField('age', IntegerType(), True),   # Age column (integer)
    StructField('city', StringType(), True)    # City column (string)
])
```

### ğŸ”¹ **Reading a CSV File Using Schema**  
```python
# Load CSV data with predefined schema
people_df = spark.read.format('csv').load("rawdata.csv", schema=peopleSchema)
```

ğŸ“Œ **Best Practice:** Always define **your own schema** instead of relying on **Sparkâ€™s default inference**!  

---

## ğŸ¯ Key Takeaways  

âœ… **Data cleaning is critical for reliable analysis**  
âœ… **Apache Spark enables scalable, efficient data transformation**  
âœ… **Schemas improve performance & data quality**  
âœ… **Programmatically defining schemas optimizes pipeline execution**  

---

### ğŸ§¹ Lab 1: Data Cleaning Review  

#### Question: Which of the following is **NOT** a benefit of Spark?  
âœ… Answer the question from the options below:  
- **A) Spark offers high performance.** ğŸš€  
- **B) Spark can only handle thousands of records.** âŒ _(Incorrect Statement)_  
- **C) Spark allows orderly data flows.** ğŸ”„  
- **D) Spark can use strictly defined schemas while ingesting data.** ğŸ“œ  

ğŸ“ **Explanation:**  
Spark is designed for **big data** processing and can handle **millions to billions of records**, not just thousands. It is optimized for **high performance**, supports **orderly data flows**, and allows **strict schemas** for structured data ingestion. Thus, the incorrect statement is **option B**.  

---

### ğŸ“œ Lab 2: Defining a Schema  

**Why define a schema?**  
âœ”ï¸ **Improves data quality** âœ…  
âœ”ï¸ **Enhances import performance** ğŸš€  
âœ”ï¸ **Maintains structured data processing** ğŸ“Š  

ğŸ’¡ **In this lab, we define a schema for reading a dataset with three columns:**  
- **Name** ğŸ· _(StringType)_  
- **Age** ğŸ”¢ _(IntegerType)_  
- **City** ğŸŒ† _(StringType)_  

#### ğŸ›  Steps:  
1ï¸âƒ£ **Import required classes** from `pyspark.sql.types`.  
2ï¸âƒ£ **Define a schema** using `StructType`.  
3ï¸âƒ£ **Specify StructField** for each column with appropriate datatypes.  
4ï¸âƒ£ **Read the CSV file** into a Spark DataFrame using the schema.  
5ï¸âƒ£ **Display the data** using `.show()`.

```python
# Import necessary classes
from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# Define schema using StructType
people_schema = StructType([
  StructField('name', StringType(), False),
  StructField('age', IntegerType(), False),
  StructField('city', StringType(), False)
])

# Read CSV with defined schema
file_path = "file:///home/talentum/test-jupyter/P2/M3/sm2/2_OperatingonDataFramesinPySpark/Dataset/people.csv"
people_df = spark.read.format('csv').load(file_path, schema=people_schema)

# Show DataFrame contents
people_df.show()
```

---

### ğŸ— Expanding Schema Definition  

ğŸ”¹ When dealing with **complex datasets**, we often require **additional columns**.  
Let's expand our schema to include:  
- `id` ğŸ†” _(IntegerType)_  
- `person_id` ğŸ†” _(IntegerType)_  
- `name` ğŸ· _(StringType)_  
- `sex` âš§ _(StringType)_  
- `date of birth` ğŸ‚ _(StringType)_  

#### ğŸ”§ Revised Code:
```python
# Import necessary classes
from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# Define expanded schema
people_schema = StructType([
  StructField('id', IntegerType(), False),
  StructField('person_id', IntegerType(), False),
  StructField('name', StringType(), False),
  StructField('sex', StringType(), False),
  StructField('date of birth', StringType(), False)
])

# Read CSV with expanded schema
file_path = "file:///home/talentum/test-jupyter/P2/M3/sm2/2_OperatingonDataFramesinPySpark/Dataset/people.csv"
people_df = spark.read.format('csv').load(file_path, schema=people_schema)

# Show top 10 rows
people_df.show(10)
```

---

âœ… **Key Takeaways:**  
ğŸ”¹ **Schemas help enforce data integrity** âœ…  
ğŸ”¹ **Defining data types ensures optimized querying** ğŸ’¡  
ğŸ”¹ **Expanding schemas allows structured data representation** ğŸ—    

---
### ğŸ”„ **Immutability & Lazy Processing in PySpark**  

---

### ğŸ›  **Variable Review**  
**Python Variables:**  
âœ” **Mutable** â€“ Can be changed after creation âœ…  
âœ” **Flexible** â€“ Easy to modify dynamically ğŸ”„  
âŒ **Concurrency Issues** â€“ Multiple threads modifying a variable can lead to unexpected behavior ğŸš¦  
âŒ **Added Complexity** â€“ Keeping track of changes increases difficulty ğŸ§©  

Python variables are **mutable**, meaning they can change anytime. However, this can cause **issues in multi-threaded environments**, especially when different tasks modify the same variable at the same time.  

---

### ğŸ”’ **Immutability: Why Does It Matter?**  

âœ” **Key Features of Immutable Variables:**  
ğŸ”¹ Used in **Functional Programming** âš™ï¸  
ğŸ”¹ **Defined Once** â€“ Cannot change after creation âœ…  
ğŸ”¹ **Direct Modification is NOT Allowed** ğŸš«  
ğŸ”¹ **Re-created if reassigned** ğŸ”„  
ğŸ”¹ **Efficiently Shared Among Processes** ğŸš€  

ğŸ“Œ **Why use immutability?**  
- **Predictability:** Since values don't change, debugging becomes easier.  
- **Concurrency Safety:** Eliminates race conditions when multiple threads access data.  
- **Better Performance:** Less memory overhead when working with big data.  

---

### ğŸ“Œ **Immutability Example in PySpark**  

ğŸ’¡ **Immutable DataFrame: Why?**  
PySpark DataFrames follow **immutability principles**, meaning **modifications create new DataFrames** instead of changing the original one.  

ğŸ”¹ **Example:**  
Defining a DataFrame using **CSV input** ğŸ‘‡  

```python
voter_df = spark.read.csv('voterdata.csv')
```

ğŸ”¹ **Modifying the DataFrame:**  
Since DataFrames are **immutable**, modifications create a new instance rather than modifying the existing one.  

```python
# Creating a new column based on existing data
voter_df = voter_df.withColumn('fullyear', voter_df.year + 2000)

# Dropping an old column (modification creates a new DF)
voter_df = voter_df.drop(voter_df.year)
```

âœ… Each modification creates a **new instance** of `voter_df` instead of altering the original one.  

---

### ğŸï¸ **Lazy Processing in PySpark**  
ğŸ’¡ **Why doesn't PySpark execute transformations immediately?**  
âœ” **Efficiency Planning** â€“ Delays execution until absolutely necessary â³  
âœ” **Minimizes Redundant Computation** â€“ Prevents unnecessary processing âš¡  
âœ” **Optimized Performance** â€“ Uses DAG (Directed Acyclic Graph) to plan execution ğŸ¯  

â“ **Example: Is Lazy Processing Slow?**  

PySpark follows a **lazy evaluation** model:  
- **Transformations** (e.g., `withColumn`, `drop`) are **not executed immediately** âŒ  
- **Actions** (e.g., `.show()`, `.count()`) **trigger execution** âœ…  

ğŸ”¹ **Example Code Execution**  
```python
# Adding a new column (Transformation)
voter_df = voter_df.withColumn('fullyear', voter_df.year + 2000)

# Dropping a column (Transformation)
voter_df = voter_df.drop(voter_df.year)

# Counting records (Action - Triggers execution)
voter_df.count()
```
ğŸ¯ **Why does `count()` trigger execution?**  
- `count()` forces Spark to **execute all previous transformations** and compute the final result.  
- Until an **action** is called, transformations **stay in an execution plan (DAG)** without running.  

---

### âœ¨ **Key Takeaways**  
ğŸ”¹ **Immutability ensures data consistency & concurrency safety** ğŸ”„  
ğŸ”¹ **PySpark DataFrames follow an immutable model** ğŸ“Š  
ğŸ”¹ **Lazy processing delays execution until necessary** â³  
ğŸ”¹ **Actions trigger actual computation** ğŸš€  

---

### ğŸ”„ **Lab 1: Immutability Review**  

#### â“ **Why does Spark use immutable DataFrames?**  
âœ… **Answer the question from the options below:**  
- **A) To add complexity to your Spark tasks.** âŒ _(Incorrect â€“ Complexity is a byproduct, not a goal)_  
- **B) To efficiently handle data throughout the cluster.** âœ… _(Correct â€“ Immutability ensures consistency in distributed systems)_  
- **C) To easily modify variable values as needed.** âŒ _(Incorrect â€“ Spark DataFrames are immutable, meaning modifications create new versions)_  
- **D) To conserve storage space.** âŒ _(Incorrect â€“ Storage efficiency is a different aspect, but immutability primarily ensures consistency)_  

ğŸ“Œ **Explanation:**  
Spark operates **in a distributed environment**, where tasks execute across multiple worker nodes. **Immutable DataFrames** ensure that no process can unexpectedly alter shared data. This **prevents inconsistencies** and makes **parallel execution safer and more efficient**.  

ğŸ’¡ **Key Takeaway:**  
Using immutable DataFrames helps Spark **efficiently process data across clusters**, ensuring stability, **fault tolerance**, and **concurrency safety**.

---

### ğŸš€ **Lab 2: Using Lazy Processing**  

ğŸ”¹ **What is Lazy Processing?**  
âœ” **Transformations** _(e.g., `withColumn`, `drop`) do **NOT** execute immediately_ âŒ  
âœ” **Actions** _(e.g., `.show()`, `.count()`) trigger execution_ âœ…  
âœ” **Prevents unnecessary computation** by building an optimized execution plan ğŸ¯  

ğŸ“Œ **Why does Spark delay execution?**  
Spark **waits** before running transformations so it can optimize the execution flow using **Directed Acyclic Graph (DAG)**. Once an **action** is performed (like `.show()`), Spark executes all pending operations **efficiently**.  

ğŸ’¡ **Let's see an example using airport data:**  

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# Initialize Spark session
spark = SparkSession.builder.appName("CSV Example").getOrCreate()

# Load the CSV file into a DataFrame
aa_dfw_df = spark.read.format('csv').options(header=True).load(
    'file:///home/talentum/test-jupyter/P3/M1/SM2/2_Immutabilityandlazyprocessing/Dataset/AA_DFW_2018_Departures_Short.csv.gz'
)

# Apply transformation (lowercase Destination Airport column)
aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(F.col('Destination Airport')))

# Drop the 'Destination Airport' column
aa_dfw_df = aa_dfw_df.drop('Destination Airport')

# Show the DataFrame (Triggers execution)
aa_dfw_df.show()
```

ğŸ”¹ **Timing Breakdown:**  
- **Step 1 (Load DataFrame)** âœ… _(Spark reads the data but doesnâ€™t process transformations yet)_  
- **Step 2 (Apply `.withColumn()` and `.drop()`)** ğŸ”„ _(These are lazy transformations, Spark doesnâ€™t execute them)_  
- **Step 3 (`show()`)** ğŸš€ _(Spark processes all transformations before displaying the data)_  

âœ… **Key Observation:**  
**Transformations are quick because Spark delays execution.** However, the actual processing happens **only** when an action (like `.show()` or `.count()`) forces computation.  

---

### âœ¨ **Final Takeaways:**  
ğŸ”¹ **Immutable DataFrames ensure consistency in distributed processing** ğŸ”„  
ğŸ”¹ **Lazy processing helps Spark optimize transformations efficiently** ğŸš€  
ğŸ”¹ **Actions trigger actual execution of transformations** â³  

---

### ğŸ“¦ **Understanding Parquet in Big Data**  

---

### ğŸš¨ **Difficulties with CSV Files**  

âœ” **No Defined Schema** â€“ CSV files donâ€™t store data types ğŸ“œ  
âœ” **Nested Data Handling** â€“ Complex structures require extra steps ğŸ—  
âœ” **Limited Encoding Formats** â€“ Doesnâ€™t support efficient compression ğŸ”„  

ğŸ“Œ **Why is this a problem?**  
- Without **schemas**, every time you process data, you need to manually specify types  
- Handling **nested structures** in JSON-like formats requires **additional processing**  
- **Row-based storage** means **slow query performance** on large datasets  

---

### ğŸŒ **Spark & CSV Files: Why Are They Slow?**  

âœ” **Row-Oriented Storage** ğŸ› â€“ Spark reads entire rows instead of just relevant columns  
âœ” **No Predicate Pushdown** âŒ â€“ Spark canâ€™t filter data efficiently before loading  
âœ” **Schema Redefinition Required** ğŸ”„ â€“ Must specify column types every time  

ğŸ“Œ **Why does this matter?**  
CSV files **donâ€™t store metadata** about the schema, which means every time you load a file, Spark needs to **infer the schema again**, leading to inefficiencies in processing **large-scale data**.

---

### ğŸ† **Parquet Format: The Solution**  

âœ… **Columnar Data Format** ğŸ“Š â€“ Stores data **column-wise**, making queries much faster  
âœ… **Schema Storage** ğŸ“œ â€“ Automatically retains metadata for efficient processing  
âœ… **Predicate Pushdown** ğŸ¯ â€“ Allows **filtering data before loading**, improving speed  
âœ… **Supported Across Frameworks** ğŸ”— â€“ Used in Spark, Hive, Presto, and other big data tools  

ğŸ“Œ **How does predicate pushdown help?**  
Instead of scanning an entire dataset, **Parquet allows filtering at the storage level**, reducing **I/O operations** and **speeding up queries**.  

---

### ğŸ”§ **Working with Parquet in Spark**  

ğŸ“Œ **Reading Parquet Files:**  
```python
df = spark.read.format('parquet').load('filename.parquet')
df = spark.read.parquet('filename.parquet')
```
ğŸ“Œ **Writing Parquet Files:**  
```python
df.write.format('parquet').save('filename.parquet')
df.write.parquet('filename.parquet')
```
ğŸ’¡ **Key Fact:** The number of output files equals the number of partitions! ğŸ”„  

---

### ğŸ”— **Parquet & SQL Integration**  

ğŸ“Œ **Using Parquet as a backing store in Spark SQL:**  
```python
flight_df = spark.read.parquet('flights.parquet')
flight_df.createOrReplaceTempView('flights')
```
âš  **Important:** Hive Metastore **must be running** for SQL operations ğŸš€  
ğŸ“Œ **Querying Parquet data with SQL:**  
```python
short_flights_df = spark.sql('SELECT * FROM flights WHERE flightduration < 100')
```
âœ… **Why use Parquet for SQL queries?**  
âœ” Faster queries due to **columnar storage**  
âœ” Supports **predicate pushdown**  
âœ” Works **efficiently with Spark SQL**  

---

### ğŸ¯ **Key Takeaways**  
âœ” **CSV files are inefficient for big data** ğŸ“œ  
âœ” **Parquet is optimized for storage & query performance** ğŸš€  
âœ” **Predicate pushdown speeds up filtering** ğŸ¯  
âœ” **Parquet works seamlessly with Spark SQL** ğŸ’¡

---

### ğŸ“¦ **Lab 1: Saving a DataFrame in Parquet Format**  

ğŸ”¹ **Why Save Data as Parquet?**  
âœ” **Optimized for Big Data** ğŸš€  
âœ” **Columnar Storage** ğŸ“Š â€“ Faster retrieval compared to CSV  
âœ” **Supports Predicate Pushdown** ğŸ¯ â€“ Processes only required data  
âœ” **Stores Schema Information** ğŸ” â€“ No need to redefine schema  

ğŸ“Œ **Exercise Breakdown:**  
1ï¸âƒ£ **View row counts of df1 & df2** ğŸ·ï¸  
2ï¸âƒ£ **Combine df1 & df2 using `union()`** ğŸ”—  
3ï¸âƒ£ **Rename column `_c3` to `flight_duration`** âœï¸  
4ï¸âƒ£ **Save DataFrame to Parquet format** ğŸ’¾  
5ï¸âƒ£ **Load & verify saved Parquet file** âœ…  

#### ğŸ›  Code Implementation  
```python
# Load CSV files
df1 = spark.read.format('csv').load('file:///home/talentum/test-jupyter/P3/M1/SM3/3_UnderstandingParquet/Dataset/AA_DFW_2017_Departures_Short.csv.gz')
df2 = spark.read.format('csv').load('file:///home/talentum/test-jupyter/P3/M1/SM3/3_UnderstandingParquet/Dataset/AA_DFW_2018_Departures_Short.csv.gz')

# View row counts
print(f"df1 Count: {df1.count()}")
print(f"df2 Count: {df2.count()}")

# Print schema details
df1.printSchema()
df2.printSchema()

# Combine DataFrames
df3 = df1.union(df2)

# Rename column '_c3' to 'flight_duration'
df3 = df3.withColumnRenamed('_c3', 'flight_duration')

# Save as Parquet
df3.write.parquet('file:///home/talentum/test-jupyter/P3/M1/SM3/3_UnderstandingParquet/AA_DFW_ALL.parquet', mode='overwrite')

# Load Parquet file & verify row count
df4 = spark.read.parquet('file:///home/talentum/test-jupyter/P3/M1/SM3/3_UnderstandingParquet/AA_DFW_ALL.parquet')
print(f"Total row count in Parquet file: {df4.count()}")

df4.printSchema()
```

ğŸ’¡ **Key Observations:**  
âœ” Parquet automatically **preserves schema** ğŸ“œ  
âœ” **Row count matches** the total from `df1 + df2` ğŸ”„  
âœ” Optimized for **efficient querying** ğŸ’¡  

---

### ğŸ” **Lab 2: Querying Parquet with SQL**  

ğŸ”¹ **Why Use SQL with Parquet?**  
âœ” **Efficient Data Processing** âš¡  
âœ” **Direct Query Support** with SparkSQL ğŸ›ï¸  
âœ” **Predicate Pushdown Speeds Up Queries** ğŸ¯  

#### ğŸ›  Code Implementation  
```python
# Load Parquet file into DataFrame
flights_df = spark.read.parquet('file:///home/talentum/test-jupyter/P3/M1/SM3/3_UnderstandingParquet/AA_DFW_ALL.parquet')

# Convert 'flight_duration' column to Double type
flights_df = flights_df.withColumn("flight_duration", flights_df.flight_duration.cast('double'))

# Register as temporary SQL table
flights_df.createOrReplaceTempView('flights')

# Query: Calculate average flight duration
avg_duration = spark.sql('SELECT avg(flight_duration) FROM flights').collect()[0]
print(f"The average flight time is: {avg_duration[0]:.2f} minutes")
```

âœ… **Key Takeaways:**  
ğŸ”¹ **Parquet & SparkSQL integrate seamlessly** ğŸ›ï¸  
ğŸ”¹ **Schema management simplifies querying** ğŸ“œ  
ğŸ”¹ **Optimized execution via predicate pushdown** ğŸš€  

---

### ğŸ“¦ **Writing Parquet Files in PySpark**  

ğŸ“Œ **Why Use Parquet?**  
âœ” **Columnar storage format** â€“ Faster querying ğŸ  
âœ” **Schema retention** â€“ No need to redefine every time ğŸ“œ  
âœ” **Efficient compression** â€“ Reduces storage space ğŸ¯  

---

### ğŸ”§ **Step-by-Step Implementation**  

#### **1ï¸âƒ£ Define Schema & Load Data from `people.csv`**
```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# Initialize Spark session
spark = SparkSession.builder.appName("Parquet Example").getOrCreate()

# Define schema for people.csv
people_schema = StructType([
    StructField("id", IntegerType(), False),
    StructField("name", StringType(), False),
    StructField("age", IntegerType(), False)
])

# Load the CSV file into a DataFrame
file_path = "file:///home/your_username/people.csv"
df1 = spark.read.format('csv').options(header=True).schema(people_schema).load(file_path)

# Show the loaded data
df1.show()
```

---

#### **2ï¸âƒ£ Save Data to CSV in Local File System (`people_csv`)**
```python
df1.write.format('csv').save('file:///home/your_username/people_csv')
```

ğŸ’¡ **Key Notes:**  
âœ” Data is **saved in CSV format** ğŸ·  
âœ” Stored under the **folder `people_csv`** in the local filesystem  

---

#### **3ï¸âƒ£ Load Data from `people_csv` & Store in Another Location**  
```python
df2 = spark.read.format('csv').options(header=True).schema(people_schema).load('file:///home/your_username/people_csv')

# Show loaded data
df2.show()
```

ğŸ’¡ **Key Notes:**  
âœ” **Data is loaded from `people_csv`** ğŸ—  
âœ” **Schema is retained** for accuracy ğŸ¯  

---

#### **4ï¸âƒ£ Define Schema for `df2` & Save to Another Folder**  
```python
# Define new schema (id, name, age)
new_schema = StructType([
    StructField("id", IntegerType(), False),
    StructField("name", StringType(), False),
    StructField("age", IntegerType(), False)
])

# Apply schema to df2
df2 = spark.createDataFrame(df2.rdd, schema=new_schema)

# Show transformed data
df2.show()

# Save to another location
df2.write.parquet('file:///home/your_username/people_parquet')
```

ğŸ’¡ **Key Notes:**  
âœ” **Schema redesigned with `id, name, age`** âœ…  
âœ” **Data stored in a separate Parquet folder `people_parquet`** ğŸ“¦  

---

### ğŸ¯ **Key Takeaways:**  
âœ” **Parquet preserves schema & optimizes query performance** ğŸš€  
âœ” **Loading CSV, transforming schema, and saving in different formats improves flexibility** ğŸ”„  
âœ” **Predicate pushdown accelerates queries in Parquet** ğŸ¯  

---

### ğŸ— **DataFrame Column Operations in PySpark**  

---

### ğŸ”„ **DataFrame Refresher**  

âœ” **DataFrames** consist of **rows & columns** ğŸ“Š  
âœ” They are **immutable** (meaning modifications create new instances) ğŸ”„  
âœ” Various **transformations** help modify data efficiently  

ğŸ“Œ **Example Queries:**  
ğŸ”¹ **Return rows where name starts with "M"**  
```python
voter_df.filter(voter_df.name.like('M%'))
```
ğŸ”¹ **Select only 'name' and 'position' columns**  
```python
voters = voter_df.select('name', 'position')
```

---

### ğŸ”¥ **Common DataFrame Transformations**  

âœ” **Filter / Where** â€“ Select rows based on conditions ğŸ¯  
```python
voter_df.filter(voter_df.date > '1/1/2019')  # OR voter_df.where(...)
```
âœ” **Select** â€“ Extract specific columns ğŸ“œ  
```python
voter_df.select(voter_df.name)
```
âœ” **withColumn** â€“ Create or modify a column ğŸ·  
```python
voter_df.withColumn('year', voter_df.date.year)
```
âœ” **Drop** â€“ Remove unnecessary columns âŒ  
```python
voter_df.drop('unused_column')
```

---

### ğŸ¯ **Filtering Data**  

âœ” **Remove null values**  
```python
voter_df.filter(voter_df['name'].isNotNull())
```
âœ” **Remove odd entries (e.g., unrealistic years)**  
```python
voter_df.filter(voter_df.date.year > 1800)
```
âœ” **Filter based on substrings**  
```python
voter_df.where(voter_df['_c0'].contains('VOTE'))
```
âœ” **Negate conditions using `~`** _(Example: Remove nulls from `_c1`)_  
```python
voter_df.where(~ voter_df._c1.isNull())  # Negation
```

---

### ğŸ”  **Column String Transformations**  

âœ” **Use `pyspark.sql.functions` for transformations**  
```python
import pyspark.sql.functions as F
```
âœ” **Apply transformations on specific columns**  
```python
voter_df.withColumn('upper', F.upper('name'))  # Convert to uppercase
```
âœ” **Split full names into first & last name**  
```python
voter_df.withColumn('splits', F.split('name', ' '))
```
ğŸ“Œ **Output:**  
This creates an **array-type column** where:  
- **First element â†’ First name**  
- **Second element â†’ Last name**  

âœ” **Cast column to another type (`IntegerType`)**  
```python
from pyspark.sql.types import IntegerType
voter_df.withColumn('year', voter_df['_c4'].cast(IntegerType()))
```

---

### ğŸ“¦ **ArrayType() Column Functions**  

âœ” **Get the length of an array column**  
```python
voter_df.withColumn("array_length", F.size(voter_df.splits))
```
âœ” **Retrieve a specific item from an array column**  
```python
voter_df.withColumn("first_name", voter_df.splits.getItem(0))  # First element
```

---

### ğŸš€ **Key Takeaways**  
âœ” **Transformations modify DataFrames efficiently** ğŸ”„  
âœ” **Filtering helps clean and refine data** ğŸ—  
âœ” **String operations enhance column values** âœ¨  
âœ” **ArrayType functions enable list-based transformations** ğŸ¯  

---

### ğŸ” **Lab 1: Filtering Column Content in PySpark**  

ğŸ’¡ **Cleaning Data: Why Itâ€™s Important?**  
âœ” Ensures accurate reporting ğŸ“Š  
âœ” Helps validate voter names âœ…  
âœ” Removes outliers or formatting issues ğŸ”„  

---

#### ğŸ›  **Step 1: Show Distinct VOTER_NAME Entries**  
```python
import pyspark.sql.functions as F

# Load CSV file into DataFrame
voter_df = spark.read.format('csv').options(header=True).load(
    'file:///home/talentum/test-jupyter/P3/M1/SM1/1_DataFramecolumnoperations/Dataset/DallasCouncilVoters.csv.gz'
)

# Show distinct names
voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)
```
âœ” `truncate=False` ensures full names are displayed âœ…  

---

#### ğŸ›  **Step 2: Remove Entries That Don't Fit Format**  
âœ” **Filter names between 1-20 characters**  
```python
voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')
```
âœ” **Filter out names containing `_` (underscore)**  
```python
voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))
```

---

#### ğŸ›  **Step 3: Show Distinct Names Again After Cleaning**  
```python
voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)
```
âœ… **Key Observations:**  
- **Removes null values & odd characters** ğŸ¯  
- **Keeps only valid voter names** ğŸ“œ  

---

### ğŸ§© **Lab 2: Filtering Question 1**  

ğŸ”¹ **Question:**  
âŒ **Which option would NOT work for filtering null entries?**  
âœ… **Answer:** **B) `users_df.where(users_df.ID == 18502)`**  

ğŸ“Œ **Explanation:**  
- `ID == 18502` **does NOT filter nulls** âŒ  
- **All other options properly exclude null entries** âœ…  

---

### ğŸ”¢ **Lab 3: Filtering Question 2**  

ğŸ”¹ **Goal:** Select `Name` and `State` for any **ID greater than 3000**  

âœ… **Correct Answer:**  
```python
users_df.filter('ID > 3000').select("Name", "State")
```
ğŸ’¡ **Key Takeaway:** Order matters!  
âœ” First, filter rows **where ID > 3000**  
âœ” Then, select **Name & State columns**  

---

### âœ‚ **Lab 4: Splitting Names into First & Last Name**  

ğŸ”¹ **Goal:**  
âœ” **Create first_name & last_name**  
âœ” **Split `VOTER_NAME` column**  

---

#### ğŸ›  **Step 1: Add Split Column**  
```python
voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\s+'))
```
âœ” Splits name into **a list of words** ğŸ“œ  

#### ğŸ›  **Step 2: Extract First Name**  
```python
voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))
```
âœ” Takes the **first word** of `VOTER_NAME` âœ…  

#### ğŸ›  **Step 3: Extract Last Name**  
```python
voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))
```
âœ” Takes the **last word** of `VOTER_NAME` âœ…  

---

#### ğŸ›  **Step 4: Remove Split Column & Show Clean Data**  
```python
voter_df = voter_df.drop('splits')
voter_df.show()
```

ğŸ”¹ **Final Output Example:**  
| DATE      | TITLE         | VOTER_NAME          | first_name | last_name |
|-----------|--------------|---------------------|------------|------------|
| 02/08/17  | Mayor        | Michael Rawlings   | Michael    | Rawlings   |
| 02/08/17  | Councilmember| Adam Medrano       | Adam       | Medrano    |
| 02/08/17  | Councilmember| Carolyn Arnold     | Carolyn    | Arnold     |

---

### ğŸš€ **Key Takeaways:**  
âœ” **Filtering improves data quality** ğŸ“Š  
âœ” **Column transformations enhance usability** ğŸ—  
âœ” **Splitting text simplifies analysis** ğŸ¯  

---

### ğŸ”„ **Conditional DataFrame Column Operations in PySpark**  

---

### ğŸ¯ **Conditional Clauses**  

âœ” **What are conditional clauses?**  
ğŸ”¹ They are **inline equivalents** of `if-then-else` statements ğŸ—  
ğŸ”¹ Used in **PySpark transformations** ğŸ¯  
ğŸ”¹ Methods: `.when()` and `.otherwise()`  

ğŸ“Œ **Basic Syntax:**  
```python
F.when(<condition>, <result>)  # Like "if condition then result"
F.otherwise(<default_value>)  # Like "else result"
```

---

### ğŸ“ **Conditional Example**  

ğŸ’¡ **Classify age groups in a DataFrame**  

```python
df.select(df.Name, df.Age, F.when(df.Age >= 18, "Adult"))
```
âœ” This returns `"Adult"` for anyone **18 or older**  

ğŸ›  **Example Output:**  
| Name   | Age | Classification |
|--------|----|---------------|
| Alice  | 14 | _Null_         |
| Bob    | 18 | Adult         |
| Candice| 38 | Adult         |

---

### ğŸ”€ **Multiple `.when()` Conditions**  

âœ” **Extend classification using multiple `.when()` statements**  
```python
df.select(df.Name, df.Age,
    F.when(df.Age >= 18, "Adult")
    .when(df.Age < 18, "Minor"))
```

ğŸ“Œ **Example Output:**  
| Name   | Age | Classification |
|--------|----|---------------|
| Alice  | 14 | Minor         |
| Bob    | 18 | Adult         |
| Candice| 38 | Adult         |

ğŸ’¡ **Key Point:** Multiple `.when()` statements allow us to specify **different conditions** for different groups.

---

### ğŸ›  **Using `.otherwise()` for Default Cases**  

âœ” **Acts like an `else` condition**  
```python
df.select(df.Name, df.Age,
    F.when(df.Age >= 18, "Adult")
    .otherwise("Minor"))
```

ğŸ“Œ **Example Output:**  
| Name   | Age | Classification |
|--------|----|---------------|
| Alice  | 14 | Minor         |
| Bob    | 18 | Adult         |
| Candice| 38 | Adult         |

ğŸ’¡ **Key Difference:** `.otherwise()` handles **all remaining cases** that donâ€™t match `.when()` conditions.

---

### âœ¨ **Key Takeaways**  
âœ” **Conditional clauses make inline logical decisions** ğŸ”„  
âœ” `.when()` **handles explicit conditions** ğŸ¯  
âœ” `.otherwise()` **acts as the default case** âœ…  
âœ” **Multiple `.when()` statements allow complex filtering** ğŸ—  

---

### ğŸ— **Lab 1: Using `.when()` for Conditional Column Modification**  

---

ğŸ’¡ **Why Use Conditional Logic?**  
âœ” Dynamically update specific values in a DataFrame  
âœ” Enables targeted modifications rather than updating entire columns  
âœ” Keeps DataFrames efficient & scalable  

---

### ğŸ”¹ **Step 1: Add `random_val` to Councilmembers**  

```python
import pyspark.sql.functions as F

# Load the CSV file
voter_df = spark.read.format('csv').options(header=True).load(
    'file:///home/talentum/test-jupyter/P3/M2/SM2/2_ConditionalDataFramecolumnoperations/Dataset/DallasCouncilVoters.csv.gz'
)

# Add random_val to Councilmembers
voter_df = voter_df.withColumn(
    'random_val', F.when(voter_df.TITLE == "Councilmember", F.rand())
)

# Show modified rows
voter_df.show()
```

ğŸ“Œ **Key Observations:**  
âœ” Only **Councilmembers** receive a random number âœ…  
âœ” Other roles remain **NULL** ğŸš«  

---

### ğŸ† **Lab 2: Adding Multiple Conditional Values**  

ğŸ”¹ **Requirements:**  
âœ” **Councilmembers** get a random number ğŸ²  
âœ” **Mayors** receive a fixed value of `2`  
âœ” **Other roles get `0`**  

#### ğŸ›  **Step-by-Step Implementation**  

```python
voter_df = voter_df.withColumn(
    'random_val', F.when(voter_df.TITLE == 'Councilmember', F.rand())
    .when(voter_df.TITLE == "Mayor", 2)
    .otherwise(0)
)

# Show modified rows
voter_df.show(10)

# Filter voters with random_val greater than 0
voter_df.filter(F.col('random_val') > 0).show()
```

---

### ğŸ” **Key Insights:**  
âœ” **`.when()` assigns specific values based on conditions** ğŸ”„  
âœ” **`.otherwise()` provides a default fallback value** âœ…  
âœ” **Efficient filtering using `.filter()`** ğŸ¯  

---

### ğŸš€ **Final Takeaways:**  
âœ” Conditional logic improves **DataFrame flexibility** ğŸ“Š  
âœ” `.when()` lets us **define multiple conditions** ğŸ¯  
âœ” **Great for cleaning & transforming real-world datasets** ğŸ”„  

---

### ğŸ›  **User-Defined Functions (UDFs) in PySpark**  

âœ” **What are UDFs?**  
ğŸ”¹ Custom **Python functions** applied to Spark DataFrames ğŸ¯  
ğŸ”¹ Wrapped using `pyspark.sql.functions.udf` ğŸ“¦  
ğŸ”¹ **Stored as variables** & used like normal Spark functions ğŸš€  

ğŸ“Œ **Basic UDF Structure:**  
```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
```

---

### ğŸ”„ **Reverse String UDF Example**  

âœ” **Step 1: Define a Python function to reverse strings**  
```python
def reverseString(mystr): 
    return mystr[::-1]
```

âœ” **Step 2: Wrap function into a UDF & store it**  
```python
udfReverseString = udf(reverseString, StringType())
```

âœ” **Step 3: Apply UDF to a DataFrame column**  
```python
user_df = user_df.withColumn('ReverseName', udfReverseString(user_df.Name))
```

âœ… **Result:**  
| Name   | ReverseName |
|--------|------------|
| Alice  | ecilA      |
| Bob    | boB        |
| Candice| ecidnaC    |

---

### ğŸ² **Argument-Less UDF Example**  

ğŸ’¡ **Goal:** Assign a **random class** without passing arguments  

âœ” **Step 1: Define a function that returns a random class**  
```python
import random

def sortingCap():
    return random.choice(['G', 'H', 'R', 'S']) 
```

âœ” **Step 2: Convert function into UDF**  
```python
udfSortingCap = udf(sortingCap, StringType())
```

âœ” **Step 3: Apply UDF without arguments**  
```python
user_df = user_df.withColumn('Class', udfSortingCap())
```

âœ… **Result:**  
| Name   | Age | Class |
|--------|----|------|
| Alice  | 14 | H   |
| Bob    | 18 | S   |
| Candice| 6  | G   |

---

### ğŸš€ **Key Takeaways**  
âœ” **UDFs allow custom operations** on Spark DataFrames  
âœ” **They can modify column values dynamically**  
âœ” **Argument-less UDFs generate random values** ğŸ²  
âœ” **Wrapped using `udf(function, return_type)`** ğŸ“œ  

---

### ğŸ— **Lab 1: Understanding User-Defined Functions (UDFs)**  

ğŸ”¹ **Question:**  
âŒ **Which value is NOT valid for the second argument of a UDF?**  

âœ… **Correct Answer:** `D) udf()`

ğŸ“Œ **Explanation:**  
The second argument in a UDF defines the **return type** of the function.  
âœ” **Valid options:** `ArrayType(IntegerType())`, `IntegerType()`, `LongType()`, `StringType()` ğŸ¯  
âŒ `udf()` is **NOT** a valid return typeâ€”it is used to wrap a Python function into a Spark UDF, not to define the output type.  

---

### ğŸ›  **Lab 2: Cleaning & Transforming Voter Data with a UDF**  

ğŸ“Œ **Steps to Clean the Data:**  
âœ” **Filter & Count Null Values** ğŸ“œ  
âœ” **Replace Null Entries with Default Name (`John Doe`)** ğŸ·  
âœ” **Split Full Name into `first_name` & `last_name`** âœ‚  
âœ” **Define a Custom UDF to Extract First and Middle Names** ğŸ†  

---

#### **1ï¸âƒ£ Removing Null Entries**
```python
df = voter_df.filter(voter_df.VOTER_NAME.isNull())
print(df.count())  # Show count of null entries

# Fill Nulls with "John Doe"
voter_df = voter_df.na.fill({'VOTER_NAME': 'John Doe'})
```

âœ” **Replaces missing names with "John Doe"** ğŸš€  

---

#### **2ï¸âƒ£ Splitting Names into First & Last**
```python
voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\s+'))
voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))
voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))
```
âœ” Extracts first & last names dynamically ğŸ“œ  

---

#### **3ï¸âƒ£ Custom UDF for First & Middle Names**
```python
def getFirstAndMiddle(names):
    return ' '.join(names)

udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())

# Apply UDF
voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))
```

âœ” **Creates a column capturing both first & middle names** âœ…  

---

#### **4ï¸âƒ£ Display Cleaned Data**
```python
voter_df.show()
```
âœ… **Final Output Example:**  
| Name                 | First Name | Last Name | First & Middle |
|----------------------|-----------|-----------|----------------|
| Jennifer S. Gates   | Jennifer  | Gates     | Jennifer S. Gates |
| Philip T. Kingston | Philip    | Kingston  | Philip T. Kingston |
| Michael S. Rawlings | Michael  | Rawlings  | Michael S. Rawlings |
| Carolyn King Arnold | Carolyn  | Arnold    | Carolyn King Arnold |

---

### ğŸ¯ **Key Takeaways**  
âœ” **UDFs allow custom operations** on Spark DataFrames ğŸ²  
âœ” **They dynamically process text & extract structured data** ğŸ”„  
âœ” **Replacing nulls ensures consistent formatting** ğŸ›   
âœ” **Splitting text simplifies data analysis** âœ‚  

---

# ğŸš€ Apache Spark Structured Streaming

## ğŸ”¹ Understanding Streaming vs. Batch Data
When working with **batch data**, we process a finite dataset that is already collected. However, **streaming data** is continuously generated, meaning we never have a complete dataset at any given moment. Apache Spark provides an **API for streaming data**, allowing us to analyze it in real time, much like batch data.

**Apache Spark Structured Streaming** is built on top of **Spark-SQL**, leveraging its powerful optimizations for efficient streaming processing. The **Spark Streaming engine** handles real-time data ingestion, transformation, and storage.

![image](https://github.com/user-attachments/assets/5c92d975-4ee5-4f46-8abd-65be1cf8c52d)

---

## ğŸ”¹ Components of Spark Structured Streaming
Spark Streaming consists of **three major components**:

1ï¸âƒ£ **Input Sources** ğŸ“¥: These generate real-time data, commonly from:
   - Apache Kafka (for event streaming)
   - Apache Flume (for collecting logs)
   - HDFS/S3 (for file-based data)

2ï¸âƒ£ **Streaming Engine** âš™ï¸: Spark processes incoming data streams from various sources using structured transformations.

3ï¸âƒ£ **Sinks (Storage Systems)** ğŸ“¦: Processed data is stored in locations such as:
   - HDFS (distributed storage)
   - Relational databases (MySQL, PostgreSQL)
   - NoSQL stores (MongoDB, Cassandra)

### ğŸ“Œ Spark Streaming Data as an Unbounded Table
Think of **streaming data** as an **infinite table**, where new rows are continuously appended rather than replacing existing data.

![image](https://github.com/user-attachments/assets/2639711e-c9b5-4bee-9a4c-90176863b3ee)

---

## â³ Micro-Batch Processing & Triggers
Spark Streaming **processes data in micro-batches**, meaning it collects small chunks of data at defined intervals (triggers). 

ğŸ›  Example:
ğŸ”¸ If a trigger is set to **1 second**, Spark will create a **new batch every second**, processing and storing it accordingly.

---

## ğŸ”¹ Output Modes - Storing Processed Data
Once Spark processes streaming data, it needs to **store results** somewhere persistently. Hereâ€™s how different **output modes** work:

âœ… **Append Mode** â¡ Outputs **only new rows** since the last trigger.  
âœ… **Update Mode** â¡ Outputs **only updated rows** since the last trigger. _(If no aggregation is used, this behaves like Append Mode.)_  
âœ… **Complete Mode** â¡ Outputs **all rows** that have been processed so far.  

---

## ğŸ”— Useful References
ğŸ“„ [Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)  
ğŸ“„ [Python Kafka Consumer Guide](https://www.svix.com/guides/kafka/python-kafka-consumer/)  
ğŸ“„ [GitHub: Spark Streaming Examples](https://github.com/NeerajBhadani/spark-streaming/blob/master/README.md)  

---























































