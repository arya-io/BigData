# üîç What is Big Data?  
Big Data refers to extremely large and complex datasets that traditional data-processing software struggles to handle. It involves storing, processing, and analyzing huge amounts of information efficiently.  

üìñ *Definition:* ‚ÄúBig data is a term used to refer to the study and applications of data sets that are too complex for traditional data-processing software.‚Äù ‚Äì Wikipedia  

---

# üìä The Three V's of Big Data  
Big Data is characterized by three main factors:  

- **üì¶ Volume:** The size of the data (huge amounts of information collected from various sources).  
- **üåç Variety:** The different formats and types of data (structured, semi-structured, and unstructured).  
- **‚ö° Velocity:** The speed at which data is generated, processed, and analyzed.  

üí° *Example:* Think of social media‚Äîmillions of users generate posts, comments, and messages every second. This enormous, fast-moving, and diverse data is classic Big Data!  

---

# üñ•Ô∏è Big Data Concepts & Terminology  

- **üñ•Ô∏è Clustered Computing:** Using multiple machines together to form a powerful system.  
- **üîÑ Parallel Computing:** Performing computations simultaneously to speed up processing.  
- **üåê Distributed Computing:** Multiple networked computers work together, sharing the workload.  
- **üì¶ Batch Processing:** Dividing large jobs into smaller chunks and running them separately.  
- **‚ö° Real-Time Processing:** Handling data instantly as it arrives (e.g., stock market updates).  

üí° *Example:* Online transactions‚Äîwhen you make a purchase, the system instantly processes payment details, checks availability, and confirms your order.  

---

# ‚öôÔ∏è Big Data Processing Systems  

### üîπ Hadoop/MapReduce  
- ‚úîÔ∏è A scalable, fault-tolerant framework written in Java.  
- üîì Open-source, widely used in Big Data.  
- ‚è≥ Primarily supports batch processing (processing data in chunks).  

### ‚ö° Apache Spark  
- üöÄ A high-speed, general-purpose cluster computing system.  
- üîì Open-source, like Hadoop.  
- üèéÔ∏è Supports both batch and real-time data processing, making it more versatile.  

üí° *Comparison:* Hadoop works like a traditional assembly line, processing one batch at a time. Spark, on the other hand, processes data much faster and supports real-time tasks!  

---

# ‚ö° Features of Apache Spark  

- üåê **Distributed computing framework** ‚Äì Runs efficiently across multiple machines.  
- üî• **In-memory computations** ‚Äì Stores intermediate results in RAM for ultra-fast performance.  
- üèéÔ∏è **Lightning-fast processing** ‚Äì Executes tasks much quicker than traditional systems.  
- üî£ **Multi-language support** ‚Äì Works with Java, Scala, Python, R, and SQL.  
- ‚ö° **Always faster than MapReduce** ‚Äì Uses optimized memory management for speed.  

üí° *Example:* If Hadoop is a truck carrying data from one place to another, Spark is a high-speed bullet train that gets the job done in record time! üöÑ  

---

# üî• Apache Spark Components  

![image](https://github.com/user-attachments/assets/4a5d8342-4251-45ce-9efc-12f3563b7965)  

Apache Spark consists of several components that enhance its functionality:  

- **üóÇÔ∏è Spark SQL** ‚Äì Used for structured data processing, similar to SQL databases.  
- **ü§ñ MLlib (Machine Learning Library)** ‚Äì Provides algorithms for machine learning tasks like classification, clustering, and regression.  
- **üîó GraphX** ‚Äì Optimized for graph-based computations and analytics (think social network connections).  
- **üåä Spark Streaming** ‚Äì Handles real-time data processing, such as analyzing live tweets or stock prices.  
- **‚ö° RDD API (Resilient Distributed Dataset)** ‚Äì The core data model of Spark, just like:  
  - Tables in Hive  
  - Relations in Pig  
  - DataFrames in Pandas  

üí° *Example:* If you‚Äôve ever worked with Pandas in Python, RDDs are somewhat similar‚Äîthey provide a flexible way to process data across multiple machines!  

---

# üöÄ Spark Modes of Deployment  

Spark can run in different modes depending on the environment:  

- **üíª Local Mode** ‚Äì Runs on a single machine, like your laptop.  
  - ‚úÖ Best for testing, debugging, and small-scale experiments.  
  - ‚úÖ Convenient for learning and demonstrations.  

- **üåê Cluster Mode** ‚Äì Runs on a group of predefined machines in a distributed setup.  
  - ‚úÖ Used for production environments.  
  - ‚úÖ Handles large-scale data efficiently.  

üîÑ **Workflow:**  
Development starts on **Local Mode**, and then moves to **Cluster Mode** for production‚Äîwithout requiring any code changes!  

üí° *Example:* Think of Local Mode as using your laptop for practice, while Cluster Mode is like deploying an app on powerful servers for real-world users.  

---

# üêç PySpark  

## üöÄ PySpark: Spark with Python  

Apache Spark is originally written in Scala, but to enable Python developers to use Spark, the community introduced **PySpark**!  

### ‚ö° Features of PySpark:  
- ‚úÖ Provides the same **speed** and **power** as Scala-based Spark.  
- ‚úÖ PySpark APIs are **similar to Pandas and Scikit-learn**, making it beginner-friendly.  

üí° *Example:* If you already use Pandas for data analysis, switching to PySpark for big data tasks will feel natural!  

---

# üèóÔ∏è What is Spark Shell?  

Spark Shell is an interactive environment where you can quickly execute Spark jobs. It helps in:  

‚úîÔ∏è **Fast prototyping** ‚Äì Quickly testing whether something works or not.  
‚úîÔ∏è **Interacting with data** ‚Äì Directly working with files on disk or data in memory.  

### üñ•Ô∏è Types of Spark Shells:  
- üü† **Spark-shell** for Scala  
- üîµ **PySpark-shell** for Python  
- üü¢ **SparkR** for R  

üîπ Spark can also run in **Java**, but there is **no dedicated shell for Java**.  

üí° *Example:* Think of Spark Shell as a Python REPL (interactive prompt), but for big data processing‚Äîit allows you to run small code snippets and experiment with Spark's capabilities without setting up full programs!  

---

# üõ†Ô∏è Opening Python Shell Using PySpark  

You can open the **Python shell using PySpark** instead of Jupyter Notebook by running the following command in your terminal:  

```bash
pyspark
```

This will launch the PySpark interactive shell, allowing you to run Spark commands directly.  

üí° *Example:* If you're familiar with Python's interactive prompt (`python` command in the terminal), PySpark works similarly but with Spark-based functions included!  

---

To run **PySpark** in the **Python shell** instead of launching Jupyter Notebook, you need to unset certain configurations. Here‚Äôs how you can do it:  

### üõ†Ô∏è Steps to Unset the Configuration & Use Python Shell  

1Ô∏è‚É£ **Unset the `PYSPARK_DRIVER_PYTHON` and `PYSPARK_DRIVER_PYTHON_OPTS` environment variables**  
Run the following command in your terminal:  

```bash
unset PYSPARK_DRIVER_PYTHON
unset PYSPARK_DRIVER_PYTHON_OPTS
```

üîπ These environment variables are responsible for launching Jupyter Notebook by default when you run `pyspark`.  

2Ô∏è‚É£ **Run PySpark in the Terminal**  
Once the configurations are unset, simply enter:  

```bash
pyspark
```

‚úîÔ∏è This will now open the **PySpark interactive shell**, instead of Jupyter Notebook.  

üí° *Example:* Think of this like switching from a **fancy graphical interface (Jupyter)** to a **raw terminal experience**, where you can directly run Python commands with Spark.  

---

## Understanding SparkContext

SparkContext is an entry point into the world of Spark
An entry point is away of connecting to Sparkcluster
An entry point is like a key to the house
PySpark has adefault SparkContext called sc
A SparkContext represents the entry point to Spark functionality. It's like a key to your car. PySpark automatically creates a SparkContext for you in the PySpark shell (so you don't have to create it by yourself) and is exposed via a variable sc.

# script.py

# Print the version of SparkContext
print("The version of Spark Context in the PySpark shell is", sc.version)

# Print the Python version of SparkContext
print("The Python version of Spark Context in the PySpark shell is", sc.pythonVer)

# Print the master of SparkContext
print("The master of Spark Context in the PySpark shell is", sc.master)

The version of Spark Context in the PySpark shell is 2.4.5
The Python version of Spark Context in the PySpark shell is 3.6
The master of Spark Context in the PySpark shell is local[*]


## Inspecting Spark Context

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.5
      /_/

Using Python version 3.6.9 (default, Mar 10 2023 16:46:00)
SparkSession available as 'spark'.

>>> id(sc)
139771463232984

>>> type(sc)
<class 'pyspark.context.SparkContext'>

Version: To retrieve SparkContext version
>>> sc.version
'2.4.5'

PythonVersion: To retrieve Python version of SparkContext
>>> sc.pythonVer
'3.6'

Master: URL of the cluster or ‚Äúlocal‚Äù string to run in local mode of SparkContext
>>> sc.master
'local[*]'


## Loading data in PySpark

SparkContext's parallelize() method
rdd = sc.parallelize([1,2,3,4,5])

SparkContext's textFile() method
rdd2 = sc.textFile("test.txt")

File Protocol

jupyter notebook is not a spark shell
read evaluate print loop (REPL) is same as Spark Shell

## Interactive Use of PySpark

    Spark comes with an interactive python shell in which PySpark is already installed in it. PySpark shell is useful for basic testing and debugging and it is quite powerful. The easiest way to demonstrate the power of PySpark‚Äôs shell is to start using it.
    The most important thing to understand here is that we are not creating any SparkContext object because PySpark automatically creates the SparkContext object named sc, by default in the PySpark shell.

    # Create a python list of numbers from 1 to 100 
numb = range(1,101)
print(type(numb))

# Load the list into PySpark  
spark_data = sc.parallelize(numb)
print(type(spark_data))

# spark_data.collect()
print(spark_data.collect())

help(spark_data.collect())

## Loading data in PySpark shell

    In PySpark, we express our computation through operations on distributed collections that are automatically parallelized across the cluster.

    file_path = 'file:////home/talentum/spark/README.md'
# Load a local file into PySpark shell
lines = sc.textFile(file_path)
print(lines.take(5))

file_path = 'file:////home/talentum/spark/README.md'

# Load a local file into PySpark shell

lines = sc.textFile(file_path)

print(lines.take(5))

['# Apache Spark', '', 'Spark is a fast and general cluster computing system for Big Data. It provides', 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that', 'supports general computation graphs for data analysis. It also supports a']

print(type(lines.take))
print(type(lines.take(5)))

<class 'method'>
<class 'list'>

lines.first()

'We the People of the United States, in Order to form a more perfect 

type(lines.first())
str

In Rdd, every line is known as an element.

---

# Use of Lambda function in python filter()

## What are anonymous functions in Python?
Lambda functions are anonymous functions in Python
Very powerful and used in Python. Quite effcient with map() and filter()
Lambda functions create functions to be called later similar to def
It returns the functions without any name (i.e anonymous)
Inline a function definition or to defer execution of acode

---

## Lambda function syntax
The general form of lambda functions is
lambda arguments: expression
Example of lambda function
double = lambda x: x * 2 
print(double(3))

---

## Difference between def vs lambda functions
Python code to illustrate cube of anumber
def cube(x):
return x ** 3
g = lambda x: x ** 3
print(g(10)) 
print(cube(10))
1000
1000

No return statement for lambda
Can put lambda function anywhere

---

## Use of Lambda function in python - map()
map() function takes a function and a list and returns a new list which contains items returned by that function for each item

General syntax of map()
map(function, list)

Example ofmap()

items = [1, 2, 3, 4] 
list(map(lambda x: x + 2 , items))

[3, 4, 5, 6]

The type of map is a Map Object.

The same functionality can be implemented using for loop. But that approach is verbose. So, in order to eliminate the verbosity, map function comes into picture.

---

## Use of Lambda function in python - filter()

Lambda function is also known as inline function.

filter() function takes a function and a list and returns a new list for which the function evaluates as true

General syntax of filter():
filter(function, list)

Example of filter()
items = [1, 2, 3, 4]
list(filter(lambda x: (x%2 != 0), items))

[1, 3]

Lambda Function is not a feature of Spark

---

üìå **Understanding RDD (Resilient Distributed Datasets) in Apache Spark** ‚ö°

![image](https://github.com/user-attachments/assets/4748b963-20e5-4cdd-a64f-fdc403ed6444)


RDD is the **fundamental building block** of Apache Spark, enabling distributed data processing with fault tolerance. Let‚Äôs break it down in simple terms! üòä  

---

### üîç **What is an RDD?**  
**Resilient Distributed Dataset (RDD)** is a **distributed collection of data** that is **fault-tolerant** and allows parallel processing across multiple nodes in a cluster.  

Think of an RDD like a **large dataset split into smaller chunks** and spread across multiple computers for efficient processing! üöÄ  

---

### üñ•Ô∏è **How RDD Works**  
1Ô∏è‚É£ The **Spark driver** reads data from a source like HDFS, local files, or databases.  
2Ô∏è‚É£ It **splits the data into multiple partitions** and distributes them across nodes in the cluster.  
3Ô∏è‚É£ Each partition is **processed in parallel**, making computations much faster! üí°  

üìú **Example Code to Create an RDD:**  
```python
rdd = sparkContext.textFile("hdfs://data/sample.txt")
```
‚úÖ This loads a text file as an RDD, which Spark processes in chunks across nodes.  

---

### üì∑ **Visual Representation**  
Your image illustrates this well:  
- The **Spark driver reads data** from a file on disk.  
- It **creates an RDD** and distributes the partitions across nodes in the cluster.  
- Each node processes its **RDD partition independently**, ensuring **parallel computation**.  

---

### üéØ **Key Benefits of RDDs**  
‚úÖ **Fault Tolerance** ‚Üí Data is automatically recovered if a failure occurs üîÑ  
‚úÖ **Parallel Processing** ‚Üí Speeds up computations across multiple machines üöÄ  
‚úÖ **Lazy Evaluation** ‚Üí Optimizes execution by processing data only when needed üèéÔ∏è  
‚úÖ **Immutability** ‚Üí Ensures consistency by keeping original data unchanged üîí  

---

## Decomposing RDDs
Resilient DistributedDatasets
Resilient: Abilityto withstandfailures
Distributed:Spanningacross multiplemachines
Datasets:Collectionofpartitioneddatae.g,Arrays,Tables,Tuplesetc.,

---

## Creating RDDs. How to do it?

Parallelizing anexistingcollectionofobjects
Externaldatasets: 
  Files in HDFS
  Objects in AmazonS3bucket 
  lines in a text file
From existingRDDs

---

## Parallelized collection(parallelizing)
parallelize() forcreatingRDDs frompythonlists
numRDD = sc.parallelize([1,2,3,4])
helloRDD = sc.parallelize("Hello world")
type(helloRDD)
<class 'pyspark.rdd.PipelinedRDD'>

---

## From external datasets
textFile() forcreatingRDDs fromexternaldatasets
fileRDD = sc.textFile("README.md")
type(fileRDD)
<class 'pyspark.rdd.PipelinedRDD'>

---

## Understanding Partitioning inPySpark
A partition isa logical division ofalargedistributeddataset
parallelize() method
numRDD = sc.parallelize(range(10), numSlices = 6)
textFile() method
fileRDD = sc.textFile("README.md", minPartitions = 6)
Thenumberofpartitionsin anRDD canbefound byusing getNumPartitions() method

rdd.glom()

hdfs fsck /user/talentum/stocks.csv -files -blocks -locations

---

### üî• Overview of PySpark Operations  

![image](https://github.com/user-attachments/assets/ccff4a44-8a7c-4c61-9277-2ac240308f2a)  

PySpark, the Python API for Apache Spark, is all about handling large-scale data processing efficiently. It operates on two key concepts:  

#### üêõ Transformations ‚Äì Creating New RDDs  
Transformations are operations that take an RDD (Resilient Distributed Dataset) and create a new one without modifying the original. Think of it like a caterpillar turning into a butterfly‚Äîeach transformation results in a new dataset.  

For example:  
```python
rdd1 = sparkContext.parallelize([1, 2, 3, 4])
rdd2 = rdd1.map(lambda x: x * 2)  # Transformation (map) creates a new RDD
```
Here, `map()` applies a function to each element, creating a new RDD (`rdd2`). The original RDD (`rdd1`) remains unchanged.  

#### üñ®Ô∏è Actions ‚Äì Performing Computations on RDDs  
Actions trigger computations and return values. Until an action is performed, transformations are **lazy**, meaning they don‚Äôt execute immediately but wait for an action to trigger processing.  

For example:  
```python
result = rdd2.collect()  # Action (collect) triggers computation
print(result)  # Output: [2, 4, 6, 8]
```
Here, `collect()` gathers the elements of `rdd2` and prints them.  

So, remember:  
‚úÖ **Transformations** create new RDDs but don‚Äôt execute immediately.  
‚úÖ **Actions** trigger computation and return results.  

This way, PySpark optimizes processing by applying transformations lazily and executing only when needed! üöÄ  

---

### ‚ö° RDD Transformations ‚Äì Lazy Evaluation  

![image](https://github.com/user-attachments/assets/d0be59c6-a464-4f04-a379-469a98b17b33)  

RDD transformations in PySpark are **lazy**, meaning they don‚Äôt execute immediately! Instead, they wait until an **action** is triggered, ensuring efficient computation by avoiding unnecessary processing.  

#### üöÄ How Lazy Evaluation Works  
Imagine a factory assembling a product but only running machines when an order is placed. Similarly, RDD transformations stack up, but PySpark waits until an action demands results before processing them.  

##### üîó Data Flow in Lazy Evaluation  
1Ô∏è‚É£ **RDD1** is created from storage (e.g., reading a file).  
2Ô∏è‚É£ Transformation applies ‚Üí **RDD2** is generated but not processed yet.  
3Ô∏è‚É£ Another transformation applies ‚Üí **RDD3** is ready but still waiting.  
4Ô∏è‚É£ Finally, an **action** executes ‚Üí Computation occurs, and results are returned.  

#### üî• Basic RDD Transformations  
These fundamental operations help manipulate data efficiently:  

‚úÖ `map()`: Applies a function to each element and returns a new RDD.  
```python
rdd = sparkContext.parallelize([1, 2, 3])
mapped_rdd = rdd.map(lambda x: x * 2)  # [2, 4, 6]
```  

‚úÖ `filter()`: Extracts elements based on a condition.  
```python
filtered_rdd = rdd.filter(lambda x: x % 2 == 0)  # [2]
```  

‚úÖ `flatMap()`: Flattens nested structures by splitting elements into multiple outputs.  
```python
rdd = sparkContext.parallelize(["Hello World"])
flat_mapped_rdd = rdd.flatMap(lambda x: x.split())  # ["Hello", "World"]
```  

‚úÖ `union()`: Merges two RDDs into one.  
```python
rdd1 = sparkContext.parallelize([1, 2])
rdd2 = sparkContext.parallelize([3, 4])
union_rdd = rdd1.union(rdd2)  # [1, 2, 3, 4]
```  

### üìù Recap:  
‚ú® **Transformations** create new RDDs but do not execute immediately.  
‚ú® **Lazy evaluation** optimizes performance by delaying computation.  
‚ú® Common transformations like `map()`, `filter()`, `flatMap()`, and `union()` help manipulate data efficiently.  

---

### üîÑ `map()` Transformation ‚Äì Applying a Function to All Elements  

![image](https://github.com/user-attachments/assets/54629600-f45d-4bde-9ccb-61508c2d35c8)  

#### ‚ú® What is `map()` Transformation?  
The `map()` transformation is used in PySpark to apply a **function** to every element in an RDD, producing a **new RDD** with transformed values.  

Think of it like a **magic converter**‚Äîevery item in the dataset passes through a function and comes out transformed!  

#### üîç Example Breakdown  
```python
RDD = sc.parallelize([1, 2, 3, 4])  # Creating an RDD with numbers  
RDD_map = RDD.map(lambda x: x * x)  # Squaring each number  
```
üîπ The `map()` function applies `lambda x: x * x` to each element in `RDD`.  
üîπ The new RDD (`RDD_map`) contains `[1, 4, 9, 16]`.  

#### üìä Visual Representation  
Imagine the original RDD as a conveyor belt:  
**Before `map()` Transformation**: `[1, 2, 3, 4]`  
‚û° Function applied (`x * x`)  
**After `map()` Transformation**: `[1, 4, 9, 16]`  

This transformation is **element-wise**, meaning it **processes each item independently**.  

#### üöÄ Key Points to Remember  
‚úÖ `map()` **always returns a new RDD** (original RDD remains unchanged).  
‚úÖ It is a **one-to-one transformation** (each input results in one output).  
‚úÖ Used for **modifying values** (e.g., squaring, doubling, converting formats).  

---

### üö¶ `filter()` Transformation ‚Äì Selecting Specific Elements  

![image](https://github.com/user-attachments/assets/6aa6dfaa-a6e2-4148-b552-69e55e2486b5)  

#### üîç What is `filter()` Transformation?  
The `filter()` transformation helps **extract only the elements that meet a specific condition**, creating a **new RDD** with the filtered results.  

Think of it like a **sieve**‚Äîit keeps what you need and removes the rest!  

#### ‚ú® Example Breakdown  
```python
RDD = sc.parallelize([1, 2, 3, 4])  # Creating an RDD with numbers  
RDD_filter = RDD.filter(lambda x: x > 2)  # Keep only numbers greater than 2  
```
üîπ The function `lambda x: x > 2` checks each element, keeping only `3` and `4`.  
üîπ The new RDD (`RDD_filter`) contains `[3, 4]`.  

#### üìä Visual Representation  
Imagine the original RDD is a list of items:  
**Before `filter()` Transformation**: `[1, 2, 3, 4]`  
‚û° Condition applied (`x > 2`)  
**After `filter()` Transformation**: `[3, 4]`  

### üöÄ Key Takeaways  
‚úÖ `filter()` **creates a new RDD** with only selected elements.  
‚úÖ It **removes** elements that don‚Äôt match the condition.  
‚úÖ Used for **data preprocessing** (e.g., filtering errors, selecting relevant data).  


---

### üåä `flatMap()` Transformation ‚Äì Expanding Elements  

![image](https://github.com/user-attachments/assets/4eebc285-9729-45f2-be1c-70eee10bc780)  

#### üîç What is `flatMap()` Transformation?  
Unlike `map()`, which transforms each element **one-to-one**, `flatMap()` **splits** elements and returns **multiple values** for each original item, creating a **flattened** RDD.  

Think of it like breaking sentences into individual words‚Äîeach input expands into multiple outputs!  

#### ‚ú® Example Breakdown  
```python
RDD = sc.parallelize(["hello world", "how are you"])  
RDD_flatmap = RDD.flatMap(lambda x: x.split(" "))  
```
üîπ The function `lambda x: x.split(" ")` splits each string into words.  
üîπ The new RDD (`RDD_flatmap`) contains `["hello", "world", "how", "are", "you"]`.  

#### üìä Visual Representation  
**Before `flatMap()` Transformation**:  
`["hello world", "how are you"]`  

‚û° Function applied (`split()` on space)  

**After `flatMap()` Transformation**:  
`["hello", "world", "how", "are", "you"]`  

#### üöÄ Key Takeaways  
‚úÖ `flatMap()` **splits elements into multiple outputs** (not one-to-one like `map()`).  
‚úÖ Creates a **flattened RDD**, removing nested structures.  
‚úÖ Useful for **text processing**, where sentences need to be broken into words.  



---

### üîó `union()` Transformation ‚Äì Merging RDDs  

![image](https://github.com/user-attachments/assets/4bb17119-cc1a-4892-b5ca-cd9c3286b816)  

#### üîç What is `union()` Transformation?  
The `union()` transformation **combines two RDDs into a single RDD**, merging all elements while keeping duplicates.  

Think of it like **merging two lists**‚Äîyou get everything from both, without any filtering!  

#### ‚ú® Example Breakdown  
```python
inputRDD = sc.textFile("logs.txt")  # Reading log file as RDD  

errorRDD = inputRDD.filter(lambda x: "error" in x.split())  # Filtering error messages  
warningsRDD = inputRDD.filter(lambda x: "warnings" in x.split())  # Filtering warnings  

combinedRDD = errorRDD.union(warningsRDD)  # Merging both RDDs  
```
üîπ `errorRDD` keeps lines containing **"error"**  
üîπ `warningsRDD` keeps lines containing **"warnings"**  
üîπ `union()` merges both, keeping all elements  

#### üìä Visual Representation  
Imagine two lists:  
**Before `union()` Transformation**  
`errorRDD`: `["Error: Disk Full", "Error: Timeout"]`  
`warningsRDD`: `["Warning: Low Memory", "Warning: High CPU Usage"]`  

‚û° `union()` merges them  

**After `union()` Transformation**  
`["Error: Disk Full", "Error: Timeout", "Warning: Low Memory", "Warning: High CPU Usage"]`  

#### üöÄ Key Takeaways  
‚úÖ `union()` **combines two RDDs** while keeping duplicates  
‚úÖ Useful for **merging filtered results** (like logs, events, data subsets)  
‚úÖ Doesn‚Äôt remove duplicates‚Äîboth datasets are preserved  

---

## RDD Actions
Operation return avalueafterrunning acomputation ontheRDD
BasicRDD Actions
collect()
take(N) 
hrst() 
count()

---

## collect() and take()Actions
collect() return all the elements of the dataset as an array 
take(N) returns anarray with thehrstN elements ofthedataset
RDD_map.collect()
[1, 4, 9, 16]
RDD_map.take(2)
[1, 4]

---

## first() andcount() Actions
hrst() printsthehrstelementoftheRDD
RDD_map.first()
[1]
count() returnthenumberofelements in theRDD
RDD_flatmap.count()
5

---

Lab:

numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
numbRDD = sc.parallelize(numbers)

# Create map() transformation to cube numbers
cubedRDD = numbRDD.map(lambda x: x ** 3)

# Collect the results
numbers_all = cubedRDD.collect()

# Print the numbers from numbers_all
for numb in numbers_all:
    print(numb)

Output:
1
8
27
64
125
216
343
512
729
1000

---

file_path = 'file:////home/talentum/spark/README.md'

# Create a fileRDD from file_path
fileRDD = sc.textFile(file_path)

# Filter the fileRDD to select lines with Spark keyword
fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)

# How many lines are there in fileRDD?
print("The total number of lines with the keyword Spark is", fileRDD_filter.count())

# Print the first four lines of fileRDD
for line in fileRDD_filter.take(4): 
  print(line)

Output:
The total number of lines with the keyword Spark is 19
# Apache Spark
Spark is a fast and general cluster computing system for Big Data. It provides
rich set of higher-level tools including Spark SQL for SQL and DataFrames,
and Spark Streaming for stream processing.

---

## Introduction to pair RDDs in PySpark
Real lifedatasetsareusually key/valuepairs
Each rowis akeyandmapstooneormorevalues
Pair RDD is aspecialdatastructureto workwith thiskindofdatasets
Pair RDD: Key istheidentiherandvalueisdata

---

## Creating pairRDDs
TwocommonwaystocreatepairRDDs 
  From a list ofkey-valuetuple
  From a regularRDD
Get thedatainto key/valueformforpairedRDD

my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)] 
pairRDD_tuple = sc.parallelize(my_tuple)

my_list = ['Sam 23', 'Mary 34', 'Peter 25'] 
regularRDD = sc.parallelize(my_list)
pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))

---

## Transformations on pair RDDs
All regulartransformationsworkonpairRDD
Haveto passfunctions that operateonkeyvaluepairsratherthanonindividual elements
Examples ofpaired RDD Transformations
reduceByKey(func): Combinevalueswith thesamekey
groupByKey(): Group valueswith thesamekey
sortByKey(): Return anRDD sortedby thekey
join(): Join two pairRDDs basedontheirkey

---

## reduceByKey() transformation
reduceByKey() transformation combines valueswith thesamekey
It runsparalleloperationsforeachkeyin thedataset 
It isatransformationand not action
regularRDD = sc.parallelize([("Messi", 23), ("Ronaldo", 34),
("Neymar", 22), ("Messi", 24)]) 
pairRDD_reducebykey = regularRDD.reduceByKey(lambda x,y : x + y) 
pairRDD_reducebykey.collect()
[('Neymar', 22), ('Ronaldo', 34), ('Messi', 47)]

---

## sortByKey() transformation
sortByKey() operationorders pairRDD by key
It returns an RDD sortedby keyin ascendingordescendingorder
pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0])) 
pairRDD_reducebykey_rev.sortByKey(ascending=False).collect()
[(47, 'Messi'), (34, 'Ronaldo'), (22, 'Neymar')]

---

## groupByKey() transformation
groupByKey() groupsallthevalueswith thesamekeyin thepairRDD
airports = [("US", "JFK"),("UK", "LHR"),("FR", "CDG"),("US", "SFO")]
regularRDD = sc.parallelize(airports) 
pairRDD_group = regularRDD.groupByKey().collect() 
for cont, air in pairRDD_group:
print(cont, list(air))
FR ['CDG']
US ['JFK', 'SFO'] 
UK ['LHR']

---

join() transformation
join() transformationjoinsthetwo pairRDDs basedontheirkey
RDD1 = sc.parallelize([("Messi", 34),("Ronaldo", 32),("Neymar", 24)])
RDD2 = sc.parallelize([("Ronaldo", 80),("Neymar", 120),("Messi", 100)])
RDD1.join(RDD2).collect()
[('Neymar', (24, 120)), ('Ronaldo', (32, 80)), ('Messi', (34, 100))]

---
































