# üìä **MapReduce** Overview

MapReduce is an ETL (Extract, Transform, Load) framework where the program is written to process large data sets in parallel across a Hadoop cluster. It consists of two main phases:

### 1Ô∏è‚É£ **Map Phase: Collect Data**

* In this phase, the input data is processed into \<key, value> pairs.
* The input file is divided into smaller blocks, and each block is stored on different DataNodes across the Hadoop cluster.

![image](https://github.com/user-attachments/assets/f2265f8c-0ced-458c-840f-0466a63eee60)

### 2Ô∏è‚É£ **Shuffle/Sort Phase**

* The \<key, value> pairs from the Map phase are shuffled and sorted by key.
* Pairs with the same key are grouped together, and the values for each key are collected for further processing.

---

### 3Ô∏è‚É£ **Reduce Phase**

* The reducer processes the grouped data and outputs final \<key, value> pairs that are typically written to a file in HDFS.

![image](https://github.com/user-attachments/assets/520418aa-23ac-4080-84eb-066b6819a359)

---

## üîç **Understanding MapReduce** Step-by-Step

### 1Ô∏è‚É£ **Input File Breakdown**

* The input file is divided into blocks that are distributed across DataNodes in the Hadoop cluster.

![image](https://github.com/user-attachments/assets/0036978e-cc30-4158-aa68-841ceaf24453)

### 2Ô∏è‚É£ **Map Phase - Processing Data**

* Each map task processes a block of data (an Input Split).
* The map tasks are Java processes that run on the DataNodes where the data blocks are stored.

### 3Ô∏è‚É£ **\<Key, Value> Pair Generation**

* After processing the input, the map task outputs a set of \<key, value> pairs.

![image](https://github.com/user-attachments/assets/9b66e4a2-9418-4e5d-8efc-54a500d02986)

### 4Ô∏è‚É£ **Shuffle/Sort Phase**

* Records with the same key are grouped together and sent to the same reducer.
* The data is sorted by the key, and values are aggregated into collections.

### 5Ô∏è‚É£ **Reduce Phase - Output**

* The reducer processes each \<key, value> collection, performs the required computation, and outputs the final result.

---

## üìù **Key/Value Pairs in MapReduce**

* The core of MapReduce is working with \<key, value> pairs.
* Mappers create these pairs, and reducers process them to generate final results.

![image](https://github.com/user-attachments/assets/9bbb4c7f-6a61-4556-92ea-6f10664421c2)

---

## üí° **Example: WordCount in MapReduce**

One of the most common use cases of MapReduce is counting the frequency of words in a large text file.

### Steps:

1. **Input File**: The file can be large (MB to PB).
2. **Map Phase**: The mapper reads the file's blocks line-by-line.
3. **Splitting Words**: The lines are split into words, and \<word, 1> pairs are sent to the reducer.
4. **Shuffle/Sort Phase**: Pairs with the same word (key) are grouped together.
5. **Reduce Phase**: The reducer sums up all the "1"s for each word and outputs the word count.

![image](https://github.com/user-attachments/assets/a0ddf7c1-8d50-4639-8f9f-537d10f3e147)

---

## üé• **Demonstration: Running MapReduce**

Here‚Äôs how you can run a MapReduce job using Hadoop:

```bash
yarn jar /usr/hdp/2.6.0.3-8
```

![image](https://github.com/user-attachments/assets/5aa07f74-7737-46d1-be89-93fabb655804)

---

## üñ•Ô∏è **Executing the JAR File**

To execute a MapReduce job, you run a JAR file using `yarn`. Here‚Äôs a breakdown:

### Command:

```bash
yarn jar [path-to-jar] [job-name] [input] [output]
```

![image](https://github.com/user-attachments/assets/d010f6b8-9f8a-4f81-8b34-35df6f6b4b65)
![image](https://github.com/user-attachments/assets/950d4c01-7e37-4911-9608-b0f6428c551f)

---

## üìÅ **Viewing the Output**

After executing the job, you can check the output directory to confirm the results:

### 1Ô∏è‚É£ **List the Files**:

```bash
hdfs dfs -ls wordcount_output
```

![image](https://github.com/user-attachments/assets/19eeeeda-431a-4b64-a96e-2cc7c6086ad9)

### 2Ô∏è‚É£ **View the Content**:

```bash
hdfs dfs -cat wordcount_output/part-r-00000
```

![image](https://github.com/user-attachments/assets/8271a8d3-2524-458a-84d2-53f8bc2646b3)

---

## üåç **Performing the MapReduce Task on BigDataVM**

On BigDataVM, we can execute the same WordCount MapReduce task using the Hadoop examples JAR. Here's the command:

### Command:

```bash
yarn jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount -D mapred.reduce.tasks=2 constitution.txt wordcount_output
```

This command runs the WordCount example on the `constitution.txt` file and outputs the result to `wordcount_output`.

![image](https://github.com/user-attachments/assets/60f7b43b-6f85-46a9-9cb8-02deb32e7936)
![image](https://github.com/user-attachments/assets/a2bd032d-185e-4971-b2f9-37539344a513)
![image](https://github.com/user-attachments/assets/774f7378-2f92-41bc-99bb-22f81e819265)

---

## üì¶ **What is a .jar File?**

A `.jar` (Java ARchive) file is a compressed package that contains Java classes, metadata, and resources (like images or property files). It‚Äôs essentially a `.zip` file, but specifically for Java applications.

### Key Points:

* `.jar` files are used to bundle Java programs and their dependencies into a single file.
* They make it easier to distribute and run Java applications.

---

## üîÑ **Merging Files in Hadoop**

We merged the files in the `wordcount_output` directory into a single file in the `/tmp/final_op` directory using the following command:

```bash
hdfs dfs -getmerge wordcount_output /tmp/final_op
```

This is useful to combine smaller output files into one for easier processing or viewing.

![image](https://github.com/user-attachments/assets/a7444982-4838-4c6a-9f1a-d99513890575)
![image](https://github.com/user-attachments/assets/54483b22-c30d-432a-9c63-b83b8d36e8ea)

---

### üìù **Word and Line Count of final\_op**

To verify the number of lines and words in the final output:

```bash
wc -l final_op
```

This gives the number of lines:

```
1683 final_op
```

To check the total number of words and characters:

```bash
wc final_op
```

This will show:

```
1683  3366 17049 final_op
```

![image](https://github.com/user-attachments/assets/201642ac-fc14-47c6-8eb6-24fa54b558e5)

---

## üñ•Ô∏è **Understanding the WordCountMapper Code**

### Code Overview

Here‚Äôs the code for the `WordCountMapper` class in Hadoop MapReduce:

```java
public class WordCountMapper
extends Mapper<LongWritable, Text, Text, IntWritable> { // This is generic

@Override

protected void map(LongWritable key, Text value,
Context context)
throws IOException, InterruptedException {
    String currentLine = value.toString();
    String[] words = currentLine.split(" ");
    for (String word : words) {
        Text outputKey = new Text(word);
        context.write(outputKey, new IntWritable(1));
    }
}
}
```

### **Explanation of the Code**

1. **Class Definition**

   ```java
   public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable>
   ```

   * The class extends `Mapper` with four generic types:

     * `LongWritable`: Represents the byte offset of the line in the input file.
     * `Text`: Represents the line of text being processed.
     * `Text`: The key (in this case, the word).
     * `IntWritable`: The value (initialized to `1`, representing the word count).

2. **Overriding the `map` Method**

   ```java
   @Override
   protected void map(LongWritable key, Text value, Context context)
   ```

   * The `map` method is called for each input record (in this case, each line of text).
   * It processes the input `key` (line offset) and `value` (the text of the line).

3. **Processing the Line**

   ```java
   String currentLine = value.toString();
   String[] words = currentLine.split(" ");
   ```

   * Converts the `Text` value (line of text) to a `String`.
   * Splits the line into individual words using a space (`" "`) as the delimiter.

4. **Emitting Key-Value Pairs**

   ```java
   for (String word : words) {
       Text outputKey = new Text(word);
       context.write(outputKey, new IntWritable(1));
   }
   ```

   * For each word in the line, it creates a `Text` object (representing the word).
   * It writes a key-value pair (`word`, `1`) to the context, which will be passed to the reducer.

---

### üß† **How It Works:**

* The **mapper** processes each line of input text.
* It splits the line into individual words.
* For each word, it emits a key-value pair: `(word, 1)`.
* These pairs are passed to the **reducer** to aggregate word counts.

---

## üßë‚Äçüíª **Explanation of the WordCountReducer Code**

The `WordCountReducer` class is responsible for aggregating word counts in the final output. Let‚Äôs break it down:

### 1Ô∏è‚É£ **Class Definition**

```java
public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable>
```

* It extends the `Reducer` class with the following generic types:

  * **`Text`**: Represents the key (the word).
  * **`IntWritable`**: Represents the value (the word count).
  * **`Text`**: Output key (the word).
  * **`IntWritable`**: Output value (final count of the word occurrences).

---

### 2Ô∏è‚É£ **Overriding the `reduce` Method**

```java
@Override
protected void reduce(Text key, Iterable<IntWritable> values, Context context)
```

* The `reduce` method is invoked for each unique word (key) emitted by the mapper.
* It receives the word (key) and a list of occurrences (values), where each occurrence is an `IntWritable` with value `1`.

---

### 3Ô∏è‚É£ **Summing Up Word Occurrences**

```java
int sum = 0;
for (IntWritable count : values) {
    sum += count.get();
}
```

* This loop iterates over the list of `IntWritable` values, each representing a `1` from the mapper.
* It sums up the occurrences of the word to get the total count.

---

### 4Ô∏è‚É£ **Writing the Final Word Count**

```java
IntWritable outputValue = new IntWritable(sum);
context.write(key, outputValue);
```

* After calculating the total word count, a new `IntWritable` is created with the summed value.
* It writes the final `(word, total count)` pair to the context, making it available for output.

---

### üß† **How It Works**

* The **reducer** receives a list of `(word, 1)` pairs from the mapper.
* It aggregates the occurrences of each word.
* It outputs the final result as `(word, total occurrences)` for further processing or storage.

This step finalizes the MapReduce job by summing and outputting the results.

---

## üèÉ‚Äç‚ôÇÔ∏è **Running a MapReduce Job**

To run a MapReduce job on Hadoop/YARN, follow these steps:

1. **Put Input Files into HDFS**
   Ensure that your input files are available in HDFS (Hadoop Distributed File System).

2. **Delete the Output Directory (if it exists)**
   If the output directory already exists, it should be deleted to avoid conflicts.

3. **Execute the Job**
   Use the `yarn` command to execute the MapReduce job:

   ```bash
   yarn jar [jarfilename] [package_name].[class_name] [textfile] [foldername to store the output]
   ```

   Example:

   ```bash
   yarn jar wordcount.jar my.WordCountJob input/file.txt result
   ```

4. **View the Output**
   After execution, you can check the output directory in HDFS to see the results of the MapReduce job.

---

## ‚öôÔ∏è **Executable Jar vs Normal Jar**

An **Executable Jar** is a JAR file that contains a `main` method or entry point for execution. This type of JAR is used to run applications directly.

* **Normal JAR**: May contain libraries or resources without an entry point, often used as a dependency for other applications.
* **Executable JAR**: Includes a `main` method or is configured to be executed directly with the command `java -jar`.


---

## üìù Shell Script Notes

### üìú **Doc Comments**:

```bash
# Author: Priyanka
# Date Created: 03-05-2025
# Modification Date: 03-05-2025
# Description: This is the first nano file
# Usage: doc/test.sh
```

---

### üñ•Ô∏è **Text Editors**: `vim` vs `nano`

* **vim** and **nano** are command-line editors.
* **nano** is simpler and more user-friendly compared to **vim**.
* By default, **nano** has all installation features available.
* In **nano**,

  * `^` refers to the \[Ctrl] key.
  * `M` refers to the \[Alt] key.

### üíæ **Saving in Nano**:

To save a file in **nano**:

* Press `CTRL + O`, then `ENTER`, and finally `CTRL + X` to exit.

---

### üìù **Shell Script Example**: `test.sh`

```bash
nano test.sh

#!/bin/bash

# Author: Priyanka
# Date Created: 03-05-2025
# Modification Date: 03-05-2025
# Description: This is the first nano file
# Usage: doc/test.sh

$(hdfs dfs -test -e /user/talentum/)

# if [[ $? -eq 0 ]]; then
#     echo "Path Exists..!!"
# else
#     echo "Path Doesn't Exists..!!"
# fi

a=$(echo "Hello")
echo $a
```

**Output**:

```bash
bash test.sh
Hello
```

![Nano Screenshot 1](https://github.com/user-attachments/assets/96d9fbb2-6e49-453d-9d0e-b8d21c4d0d6c)
![Nano Screenshot 2](https://github.com/user-attachments/assets/9b45997b-c57d-4eb3-bbf0-8fa87de20bcb)

---

### üõ†Ô∏è **Basic Conditions in Shell Scripts**

* After the `if` keyword, **use square brackets**.
* The command `hdfs dfs -ls` returns an **exit status** that indicates whether the command was successful.

---

### üîÅ **Functions & Code Reusability**

* **Avoid code repetition**‚Äîif the same block of code is used multiple times, create a **function** for it.
* You can also **create libraries of functions** for repeated tasks, making your code cleaner and more efficient.

---

### üîÑ **Flow of Execution Example**: Big Data Command

```bash
yarn jar invertedindex.jar <Main class> inverted/ inverted/output
```

**Flow**:

1. **Main Method**: The first method that gets triggered.
2. **ToolRunner**: Executes the `run()` method.

   * `Configuration conf = super.getConf();`
   * `Path in = new Path(args[0]);`
   * `Path out = new Path(args[1]);`
3. Input: `inverted` is `args[0]`.
4. Output: `inverted/output` is `args[1]`.
5. **Yarn**: Responsible for launching **Mapper** instances.

---

### **MapReduce Code Breakdown:**

---

### üìú **Class `IndexInverterJob`**

This is the main class that extends `Configured` and implements `Tool`. It manages the execution of the MapReduce job.

* **Mapper Class**: `IndexInverterMapper`
* **Reducer Class**: `IndexInverterReducer`

The job is run through the `run()` method, which configures the MapReduce job, including setting input/output paths, mapper/reducer classes, and output formats.

---

### **Mapper Class**: `IndexInverterMapper`

```java
public static class IndexInverterMapper extends Mapper<LongWritable, Text, Text, Text> {

    private Text outputKey = new Text();
    private Text outputValue = new Text();

    // Map function to process input records
    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {
        // Split the input line by commas
        String [] words = value.toString().split(",");
        outputValue.set(words[0]); // The first word is the URL

        // For each of the remaining words, emit a key-value pair
        for(int i = 1; i < words.length; i++) {
            outputKey.set(words[i]); // The word
            context.write(outputKey, outputValue); // Emit word and URL
        }
    }
}
```

* **Input**: A line from the input file, split by commas.
* **Output**: For each word (except the first one), it creates an output key-value pair with the word as the key and the URL as the value.

---

### **Reducer Class**: `IndexInverterReducer`

```java
public static class IndexInverterReducer extends Reducer<Text, Text, Text, Text> {
    private Text outputValue = new Text();

    // Reduce function to merge values
    protected void reduce(Text key, Iterable<Text> values, Context context)
            throws IOException, InterruptedException {
        StringBuilder builder = new StringBuilder();
        for (Text value : values) {
            builder.append(value.toString()).append(","); // Append each URL
        }
        builder.deleteCharAt(builder.length() - 1); // Remove the last comma
        outputValue.set(builder.toString()); // Set the final value for the key
        context.write(key, outputValue); // Write the output (word, concatenated URLs)
    }
}
```

* **Input**: A word (key) and a list of URLs (values).
* **Output**: A single line with the word as the key and all associated URLs concatenated by commas.

---

### **Main Job Execution**: `run()` method

```java
public int run(String[] args) throws Exception {
    Configuration conf = super.getConf();
    Job job = Job.getInstance(conf, "IndexInverterJob");
    job.setJarByClass(IndexInverterJob.class);

    Path in = new Path(args[0]);
    Path out = new Path(args[1]);
    out.getFileSystem(conf).delete(out, true);
    FileInputFormat.setInputPaths(job, in);
    FileOutputFormat.setOutputPath(job,  out);

    // Set Mapper, Reducer, and other job settings
    job.setMapperClass(IndexInverterMapper.class);
    job.setReducerClass(IndexInverterReducer.class);

    job.setInputFormatClass(TextInputFormat.class);
    job.setOutputFormatClass(TextOutputFormat.class);

    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(Text.class);

    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(Text.class);

    return job.waitForCompletion(true) ? 0 : 1;
}
```

* **Steps**:

  1. Set the input and output paths.
  2. Specify the Mapper and Reducer classes.
  3. Set the input/output formats (for text files).
  4. Set the map and output key/value types.
  5. Run the job and return the result.

---

### **Main Method**: `main()`

```java
public static void main(String[] args) {
    int result;
    try {
        result = ToolRunner.run(new Configuration(), new IndexInverterJob(), args);
        System.exit(result);
    } catch (Exception e) {
        e.printStackTrace();
    }
}
```

* **Executes the MapReduce job** using `ToolRunner` to configure and run the job, handling exceptions if any occur.

---

### üìù **Original Dataset: `hortonworks.txt`**

The dataset contains rows where each line consists of a URL followed by a list of keywords, separated by commas. The goal is to create an inverted index that associates each keyword with all the URLs it appears in.

---

### **Expected Output**

For example, after running the job on the `hortonworks.txt` file, the output might look like this:

```
hadoop    http://hortonworks.com/,http://hortonworks.com/products/,http://hortonworks.com/kb,http://hortonworks.com/community/
hdp       http://hortonworks.com/products/,http://hortonworks.com/download/,http://hortonworks.com/get-started/,http://hortonworks.com/events/
platform  http://hortonworks.com/resources/,http://hortonworks.com/events/
...
```

* **Key**: A word (e.g., `hadoop`, `hdp`, `platform`)
* **Value**: A comma-separated list of URLs that contain the word.

---

### üèÅ **Run Command**

To run the application on YARN:

```bash
yarn jar invertedindex.jar <Main class> inverted/ inverted/output
```

* Replace `<Main class>` with the fully qualified class name (`inverted.IndexInverterJob`).
* `inverted/` is the input directory containing the `hortonworks.txt`.
* `inverted/output` is the output directory where the inverted index will be stored.

---

## üßë‚Äçüíª **Mapper Phase**:

* In the **Mapper Phase**, `k1` and `v1` are passed, and then `k2` and `v2` are generated.
* `k1` contains the **link (URL)**, and `v1` contains the associated **keywords**:

  ```plaintext
  k1 = link (e.g., http://hortonworks.com/)
  v1 = hadoop, webinars, articles, download, enterprise, team, reliability
  ```
* After the Mapper Phase, `k2` and `v2` are generated:

  * `k2` will be a **word** (e.g., "hadoop").
  * `v2` will be a **list of URLs** where the word appears:

    ```plaintext
    k2 = "hadoop"
    v2 = ["http://hortonworks.com/", "http://hortonworks.com/products/", "http://hortonworks.com/products/hortonworksdataplatform/", ...]
    ```

---

## üîÑ **Reducer Phase**:

The **Reducer** phase handles the aggregation of results:

```java
reduce(Text key, Iterable<Text> values, Context context)
```

* **`Text key`**: Represents the **word** (e.g., "hadoop").
* **`Iterable<Text> values`**: Contains the list of **URLs** (from the Mapper Phase) associated with that word.

In the Reducer:

1. **The key (word)** is the word passed from the Mapper.
2. **The values** are all URLs associated with that word, concatenated into a final result.

---

## üìÅ **Output of the Program**:

The output of the program is stored in HDFS in the specified output directory. Here's an example of what you might see when you run:

```bash
hdfs dfs -cat IndexInverterJob_output/part-r-00000
```

The output will look like this, with each line representing a word and the list of URLs it is associated with:

```
about    http://hortonworks.com/about-us/
apache   http://hortonworks.com/products/hortonworksdataplatform/
apache   http://hortonworks.com/about-us/
articles http://hortonworks.com/community/
articles http://hortonworks.com/
...
```

* **Key (word)**: Each word from the input file.
* **Value (list of URLs)**: All the URLs where that word appears.

---

## üñºÔ∏è **MapReduce Process Diagram**:

Here‚Äôs a visual representation of how data flows through the **MapReduce** framework, showcasing the **Mappers**, the intermediate stages, and the **Reducer** that aggregates results.

![image](https://github.com/user-attachments/assets/94daa7b8-aec6-4b52-8639-1895f8ed9824)

---

## üñºÔ∏è **Breakdown of the MapReduce Diagram**:

1. **Input Split**:
   Data is divided into smaller chunks, making it easier to process in parallel across different nodes.

2. **InputFormat**:
   This step generates **`<k1, v1>`** key-value pairs from the input data for processing by the Mapper.

3. **Mapper**:
   The Mapper processes the **`<k1, v1>`** pairs and transforms them into **`<k2, v2>`** pairs, which are the output of the Mapper phase.

4. **Map Output Buffer**:
   Temporarily holds the output from the Mapper. Once the buffer reaches a certain threshold, the data is spilled to disk.

5. **Spill Files**:
   Once the buffer overflows, sorted records are written to spill files on disk.

6. **Merge Spill Files**:
   Multiple spill files are merged into one sorted file for efficient processing.

7. **Reducer Input**:
   The merged spill files become the input for the **Reducer** phase.

8. **Reducer**:
   The Reducer processes and aggregates values for each key and produces the final results.

---

## üîë **Key Points: Why This Process Is Important**:

* The flow helps explain **large-scale data processing** in Hadoop, making it easier to understand **data movement, storage, and computation**.
* By splitting work across multiple nodes, Hadoop optimizes processing, handling massive data efficiently.
* The **NodeManager** in the diagram plays a critical role in managing resources on the nodes.

### üñºÔ∏è **Data Flow in Hadoop MapReduce**:

The diagram illustrates how data moves from the **Mapper** phase to the **Reducer** phase and then to the **HDFS**:

![image](https://github.com/user-attachments/assets/b0772637-dd21-40ce-bf9f-0adcc25fa694)

### **Key Steps in the Data Flow**:

1. **Reducer Fetches Data**:
   The **Reducer** retrieves data from the **Mapper output** (stored in buffers or spill files).

2. **In-Memory Buffer**:
   The fetched data is stored temporarily in memory.

3. **Spill Files Creation**:
   When the buffer reaches a threshold, the sorted data is written into spill files.

4. **Merging Spill Files**:
   Multiple spill files are merged into a single sorted file for efficient processing.

5. **Reducer Processing**:
   The merged data is processed by the **Reducer** to aggregate the results.

6. **Final Output to HDFS**:
   The Reducer produces the final results, which are stored in **HDFS**.

### **Key Components in the Image**:

* **NodeManager** ‚Üí Manages execution and resource allocation on the nodes.
* **Buffer & Spill Files** ‚Üí Intermediate storage before merging.
* **Merged Input** ‚Üí The optimized data used by the **Reducer**.
* **HDFS Storage** ‚Üí Where the final output is stored.

---

## üßë‚Äçüíª **About YARN**:

**YARN** (Yet Another Resource Negotiator) is a resource management layer in Hadoop 2.x, which helps improve resource utilization and job management.

### **How YARN Works**:

YARN splits the responsibilities of **JobTracker** (from Hadoop 1.x) into two separate components:

* **ResourceManager**:
  Allocates resources and schedules applications.

* **ApplicationMaster**:
  Executes applications and provides failover support.

---

## ‚öôÔ∏è **JVM Process and Memory Management**:

* **ClassLoader**:
  Loads the classes required for execution into **RAM** memory.

* **JVM Process**:
  A **JVM process** is launched to run the application.

* **Stack and Heap**:

  * **Stack**: Stores method calls and local variables.
  * **Heap**: Stores objects and their data.

* **How a Program Runs**:

  * The **Main** method runs at the top of the stack.
  * Functions called inside the **Main** method are pushed on top of the stack.
  * After execution, functions are popped off the stack in the reverse order they were called.

* **Process Lifecycle**:
  Once the program terminates, it undergoes **de-processing** to free up resources.

---

This section provides a clear breakdown of the **MapReduce workflow**, the key role of **YARN**, and how the **JVM** manages memory during execution.

---

## üõ†Ô∏è **Open-Source YARN Use Cases**

* **Tez**: Enhances the execution of MapReduce jobs by providing a more efficient execution engine.
* **Slider**: Facilitates the deployment of existing distributed applications onto the YARN platform.
* **Storm**: A real-time computation framework, designed for processing real-time data streams.
* **Spark**: A MapReduce-like cluster computing framework, optimized for low-latency iterative jobs and interactive use through an interpreter.
* **Open MPI**: A high-performance message-passing library that implements the MPI-2 standard for parallel computing.
* **Apache Giraph**: A graph processing platform built for large-scale graph analytics in distributed environments.

---

## üîß **The Components of YARN**

The **ResourceManager** communicates with several components, including **NodeManagers**, **ApplicationMasters**, and **Client Applications**.

### üñºÔ∏è **YARN Components and Their Interaction**:

![image](https://github.com/user-attachments/assets/b8fdf72a-9a89-4933-9fcf-2d5f29324cb5)

This image illustrates the architecture of **YARN** (Yet Another Resource Negotiator) in a **distributed computing environment**, showing the key components and their interactions.

### **Key Components:**

1. **ResourceManager** üñ•Ô∏è

   * **Role**: The central authority that manages the allocation of resources across the cluster.
   * **Responsibilities**: Communicates with NodeManagers, ApplicationMasters, and Client Applications to ensure efficient resource distribution.

2. **NodeManager** ‚öôÔ∏è

   * **Role**: Manages resources on each individual node in the cluster.
   * **Responsibilities**: Reports available resources to the ResourceManager and executes tasks as directed.

3. **ApplicationMaster** üîó

   * **Role**: Manages the execution of a specific application on YARN.
   * **Responsibilities**: Requests resources from the ResourceManager and monitors the lifecycle and progress of the application.

4. **Client Application** üè¢

   * **Role**: The external program that submits tasks or jobs to YARN.
   * **Responsibilities**: Sends job requests to the ResourceManager, specifying the required resources.

---

### **YARN Workflow Summary:**

1. A **Client Application** submits a job to the **ResourceManager**.
2. The **ResourceManager** assigns an **ApplicationMaster** to the job.
3. The **ApplicationMaster** requests resources from the **ResourceManager**.
4. Once resources are allocated, the job runs on different **NodeManagers** across the cluster.
5. The **NodeManagers** execute tasks and report their progress back to the **ResourceManager**.

---

### **Key Benefits of YARN's Architecture:**

* **Efficient Resource Allocation**:
  YARN ensures that resources are used optimally across the cluster by managing tasks dynamically.

* **Scalability**:
  YARN can scale to support large clusters with many nodes, making it suitable for big data applications.

* **Multi-Tenancy**:
  YARN supports the execution of multiple different types of applications, improving overall utilization of cluster resources.

---

This section emphasizes how YARN enables **efficient resource management**, **scalability**, and **multi-tenancy** in distributed data processing systems like **Hadoop**.

---

## üìÖ **Lifecycle of a YARN Application**

![image](https://github.com/user-attachments/assets/011f7bfb-f514-49d7-a7b1-60d3ce41875d)

This image illustrates the **lifecycle of a YARN (Yet Another Resource Negotiator) application**, outlining the key steps in how a job is processed within a YARN-managed cluster.

### **Lifecycle Steps:**

1. **Client Submits Application** üì®

   * A **Client** submits a request to the **ResourceManager** to execute a job/application in the YARN cluster.

2. **ApplicationMaster Allocation** üõ†Ô∏è

   * The **ResourceManager** locates a **NodeManager** with sufficient available resources.
   * A **NodeManager** creates a container to launch the **ApplicationMaster** (AM) for the job.

3. **ApplicationMaster Requests Resources** üîÑ

   * The **ApplicationMaster** contacts the **ResourceManager** to request resources necessary to run the tasks for the application.
   * The **ResourceManager** then allocates and provides a list of containers where the tasks will run.

4. **Task Execution in Containers** üöÄ

   * The **NodeManagers** launch the tasks within containers (which are essentially JVM instances).
   * The **ApplicationMaster** oversees the progress of tasks within containers and ensures that computation or data processing is happening as intended.

5. **Completion & Resource Cleanup** ‚úÖ

   * Upon the completion of tasks, the results are stored in the designated storage (HDFS or other systems).
   * Resources, including containers, are cleaned up and released back to the cluster for reuse.

---

### **Why This Matters?**

* YARN ensures **efficient cluster resource management**, enabling distributed applications to run effectively.
* By managing resources and scheduling tasks, **YARN** allows **multiple applications** to run concurrently without overloading the system, optimizing the use of resources.
* This process enables frameworks like **Hadoop** to efficiently handle **big data processing** across a large number of **nodes**, ensuring scalability and reliability.

---

### **Key Concept: Distributed Programming**

The entire YARN application lifecycle exemplifies **distributed programming**, where tasks are distributed across various nodes and resources in the cluster, ensuring parallel processing, resource optimization, and scalability in large-scale computing environments.

---

This section breaks down the **YARN lifecycle** and emphasizes the importance of **distributed programming** for efficient big data processing.

---

## üìä **A Cluster View Example**

![image](https://github.com/user-attachments/assets/b57a8e01-85e7-48ba-9c53-366fc5e49ba4)

When accessing **localhost:8088** on a Linux browser, you will see a view like the one below:

![image](https://github.com/user-attachments/assets/d5e2a862-838b-403d-a18c-cef532ebb519)

In this view, multiple **application IDs** are displayed, showing how **YARN** manages and schedules applications within the cluster.

### **Key Components in the Cluster:**

1. **ResourceManager** üñ•Ô∏è

   * Manages resources across the entire cluster.
   * Includes a **Scheduler** for resource allocation and an **ApplicationMaster Scheduler (AsM)** for handling application requests.

2. **NodeManager** ‚öôÔ∏è

   * Runs on each node in the cluster.
   * Monitors node resources and manages **Containers** that execute tasks.
   * Communicates with the **ResourceManager** to update resource status.

3. **Application Masters (AM)** üîó

   * Each application has an associated **ApplicationMaster**.
   * The AM is responsible for managing the execution of the application, requesting resources, and monitoring progress.

4. **Containers** üì¶

   * Containers are the isolated environments where application tasks are executed.
   * The **ResourceManager** dynamically allocates containers to nodes for task execution.
   * Some nodes may run both **Containers** and **ApplicationMasters**.

### **Cluster Structure Insights:**

* **Nodes with Containers:** These are responsible for executing the computational workloads.
* **Nodes with Application Masters:** These coordinate and monitor the execution of tasks.
* **Resource Allocation:** The **ResourceManager** optimizes resource distribution, and tasks are executed in parallel across different **containers** and **nodes**.

### **Why This Architecture is Useful?**

* **Scalability:** The architecture supports dynamic resource adjustments, allowing for flexible scaling in a distributed environment.
* **Efficiency:** Separating resource management (by ResourceManager) from task execution (by NodeManagers) ensures optimal performance.
* **Fault Tolerance:** If nodes fail, tasks can be redistributed to ensure uninterrupted execution.

---

# üóÇÔ∏è Map Aggregation ‚Äì Simplifying Data Processing üöÄ  

![image](https://github.com/user-attachments/assets/bc256336-986a-4f03-b8ea-5c69aa3dee5b)  

Map Aggregation plays a **key role** in optimizing MapReduce workflows by **reducing intermediate data transfer** between the Mapper and Reducer. Let‚Äôs break it down in **simple terms** and understand why it's so important!  

---

## ‚ùå Without Aggregation ‚Äì Lots of Data, More Processing  
- The **Mapper** processes each word **individually**, generating separate key-value pairs.  
- Example key-value pairs produced by the Mapper:  
  - <"by", 1>, <"the", 1>, <"people", 1>  
  - <"for", 1>, <"the", 1>, <"people", 1>  
  - <"of", 1>, <"the", 1>, <"people", 1>  
- Since **every word occurrence** is sent as a separate entry, the **Reducer** has to deal with a **huge volume of data**, increasing:  
  - üì° **Network traffic** (data transfer between Mapper & Reducer)  
  - ‚è≥ **Processing time** (Reducer takes longer to consolidate results)  

üí° **Analogy:** Imagine every student in a school submits their own attendance record separately, instead of the teacher summing them up before passing the total count to the office. The office now has **hundreds** of records instead of just one summary‚Äîmaking their job harder!  

---

## ‚úÖ With Aggregation ‚Äì Less Data, Faster Processing  
- The **Mapper** pre-processes the data, combining duplicate words before sending output to the Reducer.  
- Example key-value pairs after aggregation:  
  - <"by", 1>, <"the", 3>, <"people", 3>  
  - <"for", 1>, <"of", 1>  

### üéØ **Why is this Better?**  
- **üöÄ Faster Execution** ‚Äì Less data means **quicker** transfers.  
- **üåê Reduced Network Traffic** ‚Äì The amount of intermediate data **shrinks**, leading to **smoother** processing.  
- **üí∞ Cost-Efficient** ‚Äì Optimized workflow **reduces computational overhead** and saves resources.  

üí° **Analogy:** Instead of students submitting **individual attendance reports**, the teacher **tallies everything first** and submits just **one final count**‚Äîsaving time and effort!  

---

## üèÅ **Final Thoughts**  
Aggregation **reduces redundant data movement** in MapReduce, making workflows **efficient and scalable**, especially when processing **massive datasets**!  

üí≠ **Remember:** If your data is **huge**, consider aggregation to **speed up** and **optimize** the process. üöÄ  

---

# ‚ö° Overview of Combiners ‚Äì Optimizing MapReduce Efficiency üöÄ  

![image](https://github.com/user-attachments/assets/eaaa1d77-0dac-4fbd-8f35-6e420bba14c5)  

The **Combiner** in Hadoop‚Äôs MapReduce framework plays a **crucial role** in optimizing **file I/O operations** and **reducing data transfer overhead**. Think of it as a **mini Reducer**, working at the Mapper level to **pre-process** data before it reaches the actual Reducer.  

---

## üèóÔ∏è How It Works ‚Äì Step by Step  

### 1Ô∏è‚É£ **MapOutputBuffer ‚Äì Where It All Starts**  
- The **Mapper** processes data and holds the **key-value pairs** (`<k2,v2>`) in a **buffer**.  
- But buffers have **limits**! üõë Once full, the **data spills** to disk, causing extra I/O operations.  

üí° **Analogy:** Think of this like packing a suitcase. If you don‚Äôt organize items properly, you might end up stuffing too many bags, leading to unnecessary **baggage weight** (extra disk storage).  

---

### 2Ô∏è‚É£ **Combiner ‚Äì Reducing the Load**  
- The **Combiner** steps in before the data hits the disk, grouping values locally.  
- It **acts like a Reducer**, **summarizing repeated values** before storage.  
- **Goal:** Reduce **data size** to minimize storage and **speed up** processing.  

üí° **Analogy:** Instead of throwing all items into your suitcase randomly, you neatly **fold and compress** clothes first, so fewer bags are needed! üéí  

---

### 3Ô∏è‚É£ **Disk Spill Files ‚Äì Less I/O, More Efficiency**  
- Since the **Combiner minimizes redundant records**, the disk stores **less data**.  
- The Mapper‚Äôs output eventually becomes the **Reducer‚Äôs input**, so **smaller spills mean faster execution**.  
- üöÄ **End result:** Less **I/O overhead**, **reduced network traffic**, and **optimized MapReduce performance**!  

üí° **Analogy:** If you‚Äôve **pre-sorted** your clothes before packing, you have **fewer bags to carry**, making travel **lighter and smoother**. ‚úàÔ∏è  

---

## üéØ **Why is the Combiner Important?**  
‚úÖ **Optimizes Disk Usage** ‚Äì Less storage needed per Mapper output.  
‚úÖ **Speeds Up Processing** ‚Äì Data reaches the Reducer in **smaller chunks**.  
‚úÖ **Reduces Network Load** ‚Äì Less intermediate data means **faster transfers**.  
‚úÖ **Boosts Hadoop Efficiency** ‚Äì A must-have for **large-scale data** workflows!  

---

## üèÅ **Final Thoughts**  
The **Combiner** is a powerful **local optimization tool** that makes **MapReduce workflows scalable** and **cost-effective**. If your dataset is **large**, using a Combiner can **significantly cut down processing time**!  

---

# üîÑ Reduce-side Combining ‚Äì Optimizing Data Flow in Hadoop üöÄ  

![image](https://github.com/user-attachments/assets/54fecbc7-f554-46d3-b2aa-329192b0e5d2)  

Reduce-side Combining is a **crucial mechanism** in Hadoop‚Äôs **Reduce phase**, helping optimize the flow of **intermediate key-value pairs** and **minimizing shuffle data**. Let's break it down in simple terms!  

---

## üèóÔ∏è **How Reduce-side Combining Works**  

### 1Ô∏è‚É£ **In-memory Buffer ‚Äì First Stop**  
- Data processed by **Mappers** gets **stored in an in-memory buffer** before further processing.  
- The buffer **temporarily holds** intermediate results to **minimize direct disk writes**.  

üí° **Analogy:** Imagine gathering test results from different schools in a temporary spreadsheet before organizing them‚Äîthis saves **time and effort**! üìä  

---

### 2Ô∏è‚É£ **Spill Files ‚Äì Managing Large Data**  
- When the buffer **exceeds a threshold**, data spills to disk into **spill files**.  
- This prevents memory overflow and ensures **smooth processing**.  

üí° **Analogy:** Think of writing quick notes on a whiteboard. Once the board is full, you **copy the notes into a notebook**‚Äîthat‚Äôs like spilling to disk! üìú  

---

### 3Ô∏è‚É£ **Merged Input ‚Äì Organizing Data**  
- Multiple **spill files are merged** to create a single **structured input** for the Reducer.  
- This **reduces redundancy** and makes handling large datasets more **efficient**.  

üí° **Analogy:** Instead of storing separate spreadsheets for every school‚Äôs test results, you **merge them into one file**, making analysis **easier**. üîÑ  

---

### 4Ô∏è‚É£ **Reducer ‚Äì Final Processing**  
- The **Reducer processes the merged data**, performing **grouping and computation** to generate final results.  
- This is where **actual logic is applied**, such as counting words, aggregating sums, or computing statistics.  

üí° **Analogy:** Imagine grading all student scores after merging test records‚Äîit‚Äôs the **final step** of creating a structured report! üìù  

---

### 5Ô∏è‚É£ **HDFS ‚Äì Storing Final Output**  
- The Reducer‚Äôs **processed data** gets stored in **HDFS**, ensuring **distributed and fault-tolerant storage**.  

üí° **Analogy:** Think of uploading a **final exam report** to a central database for long-term storage! üíæ  

---

## ‚ö° **What About the Combiner?**  
- **If spill files are created**, a **Combiner** can be used **before data reaches the Reducer**.  
- The **Combiner optimizes intermediate data** by **pre-grouping values**, reducing **shuffle overhead**.  
- **Result:** Less **data transfer** ‚Üí Faster **execution** üöÄ  

üí° **Analogy:** Instead of sending **raw student marks** from schools, the teacher **pre-calculates summaries**, making grading **way faster**! üìä  

---

## üéØ **Why is Reduce-side Combining Important?**  
‚úÖ **Minimizes disk writes** ‚Üí Improves **storage efficiency**  
‚úÖ **Reduces shuffle data** ‚Üí Speeds up **Reduce phase execution**  
‚úÖ **Optimizes network transfers** ‚Üí Boosts **Hadoop performance**  
‚úÖ **Enhances scalability** ‚Üí Works well for **large datasets**  

---

## üèÅ **Final Thoughts**  
Reduce-side Combining ensures data is **well-organized**, **efficiently processed**, and **optimally stored** in Hadoop. By **reducing redundant transfers**, it makes large-scale data processing **smoother and faster**! üöÄ 

---

# üìù Example of a Combiner ‚Äì Streamlining Data Processing üöÄ  

![image](https://github.com/user-attachments/assets/bdab0d10-fe90-491f-ac6f-16df60093184)  

A **Combiner** is a small yet powerful enhancement in MapReduce, helping **reduce data volume** before shuffling key-value pairs to the **Reducer**. Let's break down this classic Word Count Combiner step by step!  

---

## üìú **WordCountCombiner Code**
```java
public class WordCountCombiner 
extends Reducer<Text, IntWritable, Text, IntWritable> {
    private IntWritable outputValue = new IntWritable();

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context)
            throws IOException, InterruptedException {
        int sum = 0;
        for (IntWritable count : values) {
            sum += count.get();  // Summing up occurrences
        }
        outputValue.set(sum);
        context.write(key, outputValue);  // Writing reduced output
    }
}
```

---

## üèóÔ∏è **How It Works ‚Äì Step by Step**  

### 1Ô∏è‚É£ **Summing Values Locally**  
- The **Mapper** outputs key-value pairs like `<word, 1>` multiple times for the same word.  
- Instead of sending **individual occurrences** to the Reducer, the **Combiner groups them locally** first.  

üí° **Analogy:** Imagine counting votes in a large election. Instead of sending **individual votes** to the final counting station, each local booth first **tallies their votes**‚Äîthis saves a lot of processing time! üó≥Ô∏è  

---

### 2Ô∏è‚É£ **Local Reduction Before Shuffle**  
- The **reduce method** **iterates over values**, summing them for each word **before sending them to the Reducer**.  
- Only **aggregated counts** are passed forward, reducing **data transfer** between nodes.  

üí° **Analogy:** Instead of delivering raw sales records from every shop, each store **pre-summarizes** daily totals before sending them to headquarters. üìä  

---

### 3Ô∏è‚É£ **Writing Optimized Output**  
- The **combined total** is written out, minimizing **network overhead**.  
- This drastically **reduces the shuffle phase‚Äôs workload**, making the **Reducer‚Äôs job easier**!  

üí° **Analogy:** Think of summarizing students‚Äô scores before sending them to the school principal. Instead of analyzing **raw marks**, the principal just receives **pre-computed totals** for each subject. üè´  

---

## üéØ **Why Use a Combiner?**  
‚úÖ **Optimizes Bandwidth Usage** ‚Äì Less data transferred between Mapper and Reducer.  
‚úÖ **Enhances Performance** ‚Äì Faster processing by reducing unnecessary computation.  
‚úÖ **Prevents Redundant Computation** ‚Äì The Reducer handles **fewer records**, making it more efficient.  

üöÄ **End result:** More **scalable** and **optimized** Hadoop workflows!  

---

## üèÅ **Final Thoughts**  
Using a **Combiner** helps **streamline MapReduce operations**, making distributed data processing **more efficient**. The key takeaway: **Reduce before you shuffle!** üéØ  

---

# üöö What is a Partitioner in Hadoop MapReduce?  

![image](https://github.com/user-attachments/assets/60309caf-5821-42d4-b28e-c8273aaff03d)  

In **Hadoop MapReduce**, a **Partitioner** is responsible for **determining which Reducer** will process each **key-value pair** from the Mapper‚Äôs output. It ensures that **all values associated with the same key are sent to the same Reducer**, enabling correct and efficient data aggregation.

---

## üîÑ **How the Partitioner Works**  

### **1Ô∏è‚É£ Mapper ‚Üí Partitioner**  
- The **Mapper** generates key-value pairs from the input data.  
- These pairs are then passed to the **Partitioner**, which decides where they should go.  

üí° **Analogy:** Think of a **sorting machine** in a mailroom‚Äîit organizes packages so they go to the right recipients! üì¶  

---

### **2Ô∏è‚É£ Partitioner ‚Üí Reducers**  
- The **Partitioner assigns keys to specific Reducers**, ensuring that **all values for the same key are processed together**.  
- Without partitioning, data might be scattered across multiple Reducers, **breaking the logic** of aggregation.  

üí° **Analogy:** Imagine sorting student answers in a school exam‚Äî**all answers from the same student must go to the same examiner**, not randomly distributed! üéì  

---

### **3Ô∏è‚É£ Reducers ‚Üí NodeManagers**  
- Each **Reducer** processes the assigned data chunks.  
- The **NodeManager** ensures the Reducers execute efficiently within their respective containers.  

üí° **Analogy:** The **exam papers are handed to the right examiners**, ensuring smooth evaluation without confusion! üìú  

---

## üéØ **Why is Partitioning Important?**  

‚úÖ **Ensures Load Balancing** ‚Äì Work is evenly distributed across Reducers.  
‚úÖ **Prevents Data Skew** ‚Äì Avoids overwhelming one Reducer with too much data.  
‚úÖ **Optimizes Parallel Processing** ‚Äì Allows multiple nodes to process data **simultaneously** for faster results.  

---

# ‚ö° Understanding the Default Partitioner  

Hadoop uses the **HashPartitioner** by default, which assigns reducers **based on the hash value of the key**.

### **Default HashPartitioner in Hadoop**  

```java
public class HashPartitioner<K, V> extends Partitioner<K, V> {
    public int getPartition(K key, V value, int numReduceTasks) {
        return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;
    }
}
```

### üîç **How It Works**  
- **`key.hashCode()`** ‚Äì Converts the key into a numerical hash.  
- **`& Integer.MAX_VALUE`** ‚Äì Ensures the hash is **non-negative**.  
- **`% numReduceTasks`** ‚Äì Distributes the key-value pairs **evenly** across available Reducers.  

üí° **Analogy:** It‚Äôs like organizing students alphabetically into exam halls‚Äînames starting with **A-C go to Room 1, D-F go to Room 2, etc.** üéØ  

---

# üõ† **Writing a Custom Partitioner**  

Sometimes, the default **HashPartitioner** might not distribute data optimally. You can create a **Custom Partitioner** based on logic suited to your dataset.

### ‚úÖ **Example: Word Count Partitioner**  

```java
public class WordCountPartitioner extends Partitioner<Text, IntWritable> {
    public int getPartition(Text key, IntWritable value, int numReduceTasks) {
        if (numReduceTasks == 1) {
            return 0;
        }
        return (key.toString().length() * value.get()) % numReduceTasks;
    }
}
```

### üîç **How This Custom Partitioner Works**  
- **Uses word length** multiplied by **word frequency** (`value.get()`) to assign Reducers.  
- Applies **modulo operation** (`% numReduceTasks`) to **distribute work evenly**.  

üí° **Example Output (For 3 Reducers)**  

| Word         | Count | `(length * count) % numReduceTasks` | Assigned Reducer |
| ------------ | ----- | ----------------------------------- | ---------------- |
| "data"       | 2     | `(4 * 2) % 3 = 2`                   | Reducer 2        |
| "hadoop"     | 3     | `(6 * 3) % 3 = 0`                   | Reducer 0        |
| "big"        | 1     | `(3 * 1) % 3 = 0`                   | Reducer 0        |
| "processing" | 5     | `(10 * 5) % 3 = 2`                  | Reducer 2        |

üîπ **This prevents data skew and ensures balanced load across Reducers!**  

---

# üèÅ **Final Thoughts**  

Partitioning is **critical** in MapReduce. A well-designed **Partitioner ensures efficient distribution of work**, preventing **bottlenecks** and improving **performance**. Whether using the **default HashPartitioner** or creating a **custom one**, the goal is to **balance Reducer workloads** and **speed up data processing**! üöÄ  


---

# üîÑ **Shuffle & Sort in Hadoop MapReduce**  

![image](https://github.com/user-attachments/assets/93372eee-ad72-4d6b-bce2-e15d5be7fcc0)  

The **Shuffle & Sort phase** is one of the most critical stages in **MapReduce**, responsible for efficiently organizing **key-value pairs** before they reach the **Reducer**. Let's break down how it works in simple terms! üöÄ  

---

## üèóÔ∏è **Understanding Key-Value Pairs (K2, V2)**  
- **K2 (Keys):** Represent categories such as `"SC"` and `"LA"`.  
- **V2 (Values):** Numerical data linked to the key, such as `(40460, 1), (40061, 1)`.  

üí° **Analogy:** Think of K2 as different **departments**, and V2 as the **transactions** happening within them. For example, `"Sales"` could have multiple daily transactions like `(1000,1), (500,1)`.  

---

## üîÑ **Shuffle & Sort ‚Äì Step by Step**  

### **1Ô∏è‚É£ Mapper Generates Key-Value Pairs**  
Each **Mapper** processes raw input data and **outputs individual key-value pairs**, such as:  
```plaintext
<SC, (40460, 1)>
<SC, (48847, 1)>
<LA, (35055, 1)>
```
Instead of sending these directly to the **Reducer**, Hadoop optimizes the process using **Shuffle & Sort** first!  

---

### **2Ô∏è‚É£ Sorting Phase ‚Äì Organizing Data**  
- Keys are **grouped together** so that all values associated with `"SC"` are collected.  
- Example after sorting:  
```plaintext
SC ("40460, 1", "48847, 1", "35055, 1")
```
üí° **Analogy:** Imagine sorting exam papers by subject before grading‚Äîthis ensures that **all Math papers go to the right teacher**! üìú  

---

### **3Ô∏è‚É£ Combiner Operation ‚Äì Reducing Intermediate Data**  
- The **Combiner** is applied to optimize **network traffic**, using **commutative and associative operations** to **reduce** the amount of intermediate data sent to the Reducer.  
- Instead of storing each individual count, the **Combiner performs local aggregation**:  
```plaintext
context.write(key, outputValue);
```
üí° **Analogy:** If each student‚Äôs **math scores** were individually recorded, it would take too long to process. Instead, **local aggregation** summarizes scores before passing them on! üè´  

---

### **4Ô∏è‚É£ Reducer Processes Aggregated Data**  
- The **Reducer** takes all the summed values (`sumCount`) and performs the **final computation**:  
```java
outputValue.set(((double) sum)/count);
```
üí° **Analogy:** Instead of grading every quiz separately, the teacher **averages scores for each student** before finalizing the results! üéØ  

---

# üìä **Checking Shuffle & Sort in YARN Logs**  
Hadoop allows you to **track Shuffle & Sort performance** using YARN logs.  

### ‚úÖ **Fetching Logs for a Specific Application**
```bash
yarn logs -applicationId application_1746198666490_0022
```

üí° **This command retrieves logs** for the Hadoop job, helping debug performance issues.  

![image](https://github.com/user-attachments/assets/dcf55ef3-1817-4859-b7c0-5bd74349031b)

### ‚úÖ **Checking Map Counters in Logs**  
```bash
yarn logs -applicationId application_1746198666490_0022 | grep "MAP counter = "
```
üí° **This helps monitor mapper statistics**, ensuring efficient data flow in the pipeline!  

![image](https://github.com/user-attachments/assets/d6c7e0e5-fddd-4f32-95a8-512304a2b69f)

---

# üèÅ **Final Thoughts**  
The **Shuffle & Sort phase** ensures that data is **organized efficiently**, reducing **network congestion** and speeding up **parallel processing** in Hadoop. The **Combiner** further optimizes performance by **aggregating values locally** before they reach the Reducer! üöÄ  

---

# üöÄ Partition Continued ‚Äì Understanding Hadoop Data Flow  

![image](https://github.com/user-attachments/assets/2e3926e6-2868-455e-9753-91c549ecc05b)  

The **partitioning process in Hadoop MapReduce** is essential for **efficient data distribution and processing**. This data flow diagram highlights how raw data transitions **step by step** before reaching meaningful output. Let‚Äôs dive in!  

---

## üìå **Step-by-Step Breakdown of Hadoop MapReduce Data Flow**  

### **1Ô∏è‚É£ Input Splits from HDFS**  
- The data stored in **HDFS** is **divided** into smaller pieces called **input splits**.  
- These splits allow **parallel processing** for efficiency.  

üí° **Analogy:** Imagine dividing a **big book** into separate chapters so different people can read parts simultaneously! üìñ  

---

### **2Ô∏è‚É£ InputFormat ‚Äì Converting Data into Key-Value Pairs**  
- The **InputFormat** defines how each split is structured into **<key, value> pairs**, making data ready for the **Mapper**.  
- Different formats suit different types of data sources.  

üí° **Analogy:** Think of a **translator** converting a book into different languages based on the reader‚Äôs needs! üìö  

---

### **3Ô∏è‚É£ Mapper Execution ‚Äì Processing Key-Value Pairs**  
- The **Mapper** processes each key-value pair independently.  
- It applies **transformations, filtering, and local aggregations** before passing results forward.  

üí° **Analogy:** A **chef** preparing individual meal components before assembling the final dish! üçΩÔ∏è  

---

### **4Ô∏è‚É£ Reducer Execution ‚Äì Final Processing**  
- The output from Mappers is **shuffled, sorted, and grouped**, allowing the **Reducer** to consolidate results.  
- This step ensures meaningful **data aggregation** based on keys.  

üí° **Analogy:** A **final report** being compiled after receiving categorized survey responses from multiple locations! üìù  

---

### **5Ô∏è‚É£ OutputFormat ‚Äì Structuring the Final Storage**  
- The processed results are **formatted properly** before being stored in **HDFS**.  
- The chosen **OutputFormat** determines how the results appear for further use.  

üí° **Analogy:** Formatting a **report neatly** before submitting it to management! üìä  

---

# üìÇ **Built-in Hadoop Input Formats**  

Hadoop provides **various built-in InputFormats** to structure raw data efficiently. Here's a quick rundown:  

### ‚úÖ **FileInputFormat<K, V>**  
- Acts as the **parent class** for most input formats.  
- Reads data from HDFS and splits it into **InputSplits** for parallel execution.  

---

### ‚úÖ **TextInputFormat<LongWritable, Text>** _(Default)_  
- Processes **text files line by line**, treating:  
  - `LongWritable` ‚Üí Line offset as the key  
  - `Text` ‚Üí The line content as the value  
- Ideal for **log files and structured text data**.  

---

### ‚úÖ **SequenceFileInputFormat<K, V>**  
- Works with **binary files** using Hadoop‚Äôs SequenceFile format.  
- Great for storing **compressed and efficiently retrievable** data.  

---

### ‚úÖ **KeyValueTextInputFormat<Text, Text>**  
- Reads text data as **key-value pairs** where:  
  - The **first token** is the key  
  - The remaining line is the value  
- Perfect for structured records where key-based grouping is required.  

---

### ‚úÖ **CombineFileInputFormat<K, V>**  
- Designed for **handling many small files efficiently**.  
- Combines multiple tiny files into **larger splits**, reducing overhead.  

---

### ‚úÖ **MultipleInputs**  
- Allows **different InputFormats** for various datasets in a single job.  
- Useful for **mixing data types** (XML, CSV, JSON, etc.).  

---

# üì§ **Built-in Hadoop Output Formats**  

Once the **Reducer** finishes processing, data needs to be written **in a structured format** to storage. Here‚Äôs how Hadoop handles output formatting:  

### ‚úÖ **FileOutputFormat<K, V>**  
- Serves as the **parent class** for all output formats.  
- Sends processed data to **HDFS** in a structured way.  

---

### ‚úÖ **TextOutputFormat<K, V>** _(Default)_  
- Saves data in **plain text** with a separator between keys and values.  
- Best suited for **human-readable outputs**.  

---

### ‚úÖ **SequenceFileOutputFormat<K, V>**  
- Writes output as **binary Hadoop SequenceFile format**.  
- Excellent for **efficient storage & retrieval of large datasets**.  

---

### ‚úÖ **MultipleOutputs<K, V>**  
- Allows **writing results to multiple destinations** within a job.  
- Ideal for cases where output needs **segmentation per category or format**.  

---

### ‚úÖ **NullOutputFormat<K, V>**  
- Used when **no output is required**.  
- Beneficial for **debugging or testing** jobs without generating actual results.  

---

### ‚úÖ **LazyOutputFormat<K, V>**  
- Writes output **only when a reducer generates non-empty results**.  
- Avoids **creating unnecessary empty files** in HDFS.  

---

# üèÅ **Final Thoughts**  

Partitioning is **essential** for efficient **data flow, parallel processing, and load balancing** in Hadoop. Choosing the right **InputFormat and OutputFormat** ensures **optimized performance** for large-scale data applications! üöÄ  

---

# üöÄ Optimizing MapReduce Jobs ‚Äì Best Practices for High Performance  

Optimizing Hadoop **MapReduce jobs** is essential for **improving efficiency, reducing computational overhead, and ensuring smooth execution** of large-scale data processing. Here‚Äôs a refined breakdown of **key optimization strategies!** üî•  

---

## üîÑ **1Ô∏è‚É£ Distribute Workload Evenly Across NodeManagers**  
- Tasks should be **balanced** across available nodes.  
- **Custom partitioners** help distribute data evenly, preventing one node from getting overloaded.  

üí° **Analogy:** Imagine a classroom with **group projects**‚Äîspreading work evenly ensures everyone contributes equally rather than one student doing it all! üìö  

---

## üîß **2Ô∏è‚É£ Use a Combiner to Reduce Shuffle Overhead**  
- The **Combiner** minimizes **data transfer** between the Mapper and Reducer.  
- Essential for **aggregations** like sum and count, reducing unnecessary computation.  

üí° **Analogy:** Instead of counting individual votes at the **national level**, each district **first summarizes local votes**, reducing workload before final tabulation! üó≥Ô∏è  

---

## üèóÔ∏è **3Ô∏è‚É£ Avoid Instantiating New Objects**  
- **Creating new objects** inside loops increases **garbage collection overhead**.  
- **Reuse existing objects** whenever possible to save memory.  

üí° **Tip:** Instead of declaring new `IntWritable` inside a loop, **reuse one instance**, updating its value dynamically.  

---

## üöÄ **4Ô∏è‚É£ Use StringBuilder Instead of String Concatenation**  
- Strings are **immutable**‚Äîevery modification creates a **new object**, increasing memory usage.  
- **StringBuilder** optimizes string operations by modifying existing memory instead of creating new instances.  

üí° **Example:** Instead of this ‚õî:  
```java
String result = "";
for (String value : values) {
    result += value;
}
```
Use this ‚úÖ:  
```java
StringBuilder result = new StringBuilder();
for (String value : values) {
    result.append(value);
}
```
**Outcome:** Less memory allocation, faster execution! üöÄ  

---

## üóúÔ∏è **5Ô∏è‚É£ Enable Data Compression to Reduce Disk I/O**  
- Use **SequenceFile compression**, **Snappy**, or **Gzip** for **intermediate Mapper output and final Reducer output**.  
- Reduces **storage footprint** and speeds up data transfers.  

üí° **Tip:**  
```java
job.setOutputFormatClass(SequenceFileOutputFormat.class);
SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);
```
**Result:** Faster processing due to reduced file sizes!  

---

## üî¢ **6Ô∏è‚É£ Store Numbers in Binary Format Instead of Text**  
- Numbers in **text format** take **more space** and slow down processing.  
- Using **binary representations** saves space and speeds up computation.  

üí° **Tip:** Instead of storing `"100"` as text, **use binary storage** for efficiency.  

---

## üéØ **7Ô∏è‚É£ Define and Configure a RawComparator for Faster Sorting**  
- **RawComparators** speed up **sorting** by comparing **binary representations directly**, reducing deserialization overhead.  
- Instead of converting objects to Java types, it works on raw bytes.  

üí° **Example of a RawComparator:**  
```java
public class MyRawComparator extends WritableComparator {
    protected MyRawComparator() {
        super(Text.class);
    }
    @Override
    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
        return WritableComparator.compareBytes(b1, s1, l1, b2, s2, l2);
    }
}
```
üöÄ **Result:** Sorting becomes significantly **faster**, improving job execution!  

---

## üîç **8Ô∏è‚É£ Use StringUtils.split Instead of String.split for Faster Parsing**  
- `String.split()` relies on **regex-based parsing**, which is **slower**.  
- **StringUtils.split()** is **optimized** for faster text processing.  

üí° **Example Comparison:**  
‚õî Using `String.split()` (slower):  
```java
String[] words = line.split(" ");
```
‚úÖ Using `StringUtils.split()` (faster):  
```java
String[] words = StringUtils.split(line, ' ');
```
üöÄ **Outcome:** Faster parsing, reduced overhead!  

---

# üî• **Why Doesn't Hadoop Use Java's Primitive Datatypes?**  
Although Hadoop is **built on Java**, it doesn‚Äôt use **primitive datatypes** (`int`, `double`, etc.). Here‚Äôs why:  

### üèóÔ∏è **1Ô∏è‚É£ Hadoop is Fully Object-Oriented**  
- Java uses **primitive datatypes**, but Hadoop is **designed for handling complex objects**.  

---

### üîÑ **2Ô∏è‚É£ Serialization ‚Äì The Key Factor**  
- Hadoop relies on **serialization** to **store, process, and transfer data efficiently**.  
- Java primitives like `int` and `double` **aren't serializable**, but Hadoop‚Äôs **Writable classes** (`IntWritable`, `DoubleWritable`) **are**.  

üí° **Example:**  
‚õî **Using Java primitives (Not Serializable)**
```java
int num = 100;  // Not ideal for Hadoop serialization
```
‚úÖ **Using Hadoop‚Äôs Writable types (Serializable)**
```java
IntWritable num = new IntWritable(100);  // Optimized for Hadoop processing
```
üöÄ **Result:** Proper serialization, making distributed processing more efficient!  

---

# üèÅ **Final Thoughts**  
Optimizing Hadoop **MapReduce jobs** leads to **better performance, reduced processing time, and enhanced scalability**.  

üí° **Key Takeaways:**  
‚úÖ **Use Combiners** to **minimize shuffle** overhead.  
‚úÖ **Enable compression** for **faster data transfers**.  
‚úÖ **Avoid unnecessary object creation** to **reduce memory usage**.  
‚úÖ **Optimize sorting with RawComparators**.  
‚úÖ **Use Hadoop‚Äôs Writable datatypes** instead of Java primitives.  

---

# üóúÔ∏è Data Compression in Hadoop üöÄ  

Data compression in Hadoop plays a **critical role** in **reducing storage requirements, improving processing speed, and optimizing network bandwidth**. Different compression codecs help **efficiently manage large-scale data**, minimizing I/O overhead.  

---

## üîß **Why Use Data Compression?**  
‚úÖ **Reduces Storage Usage** ‚Äì Compressed files occupy **less disk space**.  
‚úÖ **Speeds Up Processing** ‚Äì Less data means **faster reads/writes**.  
‚úÖ **Optimizes Network Transfer** ‚Äì Compressed data reduces **shuffling costs**.  
‚úÖ **Enhances Performance in Distributed Systems** ‚Äì Hadoop jobs **run faster** due to minimized file sizes.  

---

## üèóÔ∏è **Common Compression Codecs Used in Hadoop**  

### üöÄ **1Ô∏è‚É£ Snappy (Fast & Lightweight)**  
- **Codec:** `org.apache.hadoop.io.compress.SnappyCodec`  
- **Best For:** **High-speed** compression/decompression with **moderate space savings**.  
- **Advantages:**  
  - ‚úÖ **Super fast**, optimized for performance.  
  - ‚úÖ Works well for real-time applications requiring quick I/O.  
- **Limitations:**  
  - ‚ùå Not the most **space-efficient** compared to other methods.  

üí° **Analogy:** Think of Snappy like a **zippered backpack**‚Äîquick to open/close but doesn‚Äôt shrink content much! üéí  

---

### üóúÔ∏è **2Ô∏è‚É£ Gzip (Strong Compression & Widely Used)**  
- **Codec:** `org.apache.hadoop.io.compress.GzipCodec`  
- **Best For:** Achieving **high compression** ratios, suitable for storing large files.  
- **Advantages:**  
  - ‚úÖ **Widely supported**, useful for archival data.  
  - ‚úÖ **Good compression ratio** for space efficiency.  
- **Limitations:**  
  - ‚ùå **Slower decompression** than Snappy.  
  - ‚ùå **Not splittable**, meaning files can‚Äôt be divided easily for parallel processing.  

üí° **Analogy:** Think of Gzip as **vacuum-sealed bags**‚Äîcompact but slower to unpack! üëú  

---

### üîÑ **3Ô∏è‚É£ Bzip2 (High Compression, Splittable)**  
- **Codec:** `org.apache.hadoop.io.compress.BZip2Codec`  
- **Best For:** **Efficiently compressing large files** while remaining **splittable**, making it Hadoop-friendly.  
- **Advantages:**  
  - ‚úÖ **Splittable**, meaning Hadoop can process chunks of compressed files in parallel.  
  - ‚úÖ **Higher compression ratio** than Gzip.  
- **Limitations:**  
  - ‚ùå **Slower compression/decompression** compared to Snappy & Gzip.  

üí° **Analogy:** Think of Bzip2 like **efficient luggage packing**‚Äîtakes longer, but saves space! üß≥  

---

### üîÑ **4Ô∏è‚É£ LZO (Splittable & Optimized for Speed)**  
- **Codec:** `com.hadoop.compression.lzo.LzopCodec`  
- **Best For:** **Balancing speed and compression ratio**, often used for **large-scale log files**.  
- **Advantages:**  
  - ‚úÖ **Splittable**, great for Hadoop‚Äôs distributed processing.  
  - ‚úÖ **Fast decompression**, making data retrieval smoother.  
- **Limitations:**  
  - ‚ùå **Lower compression ratio** compared to Bzip2 or Gzip.  

üí° **Analogy:** Think of LZO like a **roll-up bag**‚Äîgood balance between space and speed! üéí  

---

### üèóÔ∏è **5Ô∏è‚É£ DEFLATE (Versatile & Balanced Compression)**  
- **Codec:** `org.apache.hadoop.io.compress.DefaultCodec`  
- **Best For:** **General-purpose compression**, balancing **speed and storage efficiency**.  
- **Advantages:**  
  - ‚úÖ **Well-optimized**, commonly used in Hadoop.  
  - ‚úÖ Works well across different types of data.  
- **Limitations:**  
  - ‚ùå **Not specialized** for any one particular use case.  

üí° **Analogy:** Think of DEFLATE as **a hybrid suitcase**‚Äîflexible but not the best in either compression or speed! üõÑ  

---

# üî• **Choosing the Right Compression Codec**  

| **Codec**  | **Best For**  | **Splittable?** | **Compression Strength** | **Speed** |
|------------|--------------|----------------|------------------|------------|
| **Snappy** | Fast processing | ‚ùå No  | üîµ Moderate | üü¢ High |
| **Gzip** | Archival data | ‚ùå No | üî¥ High | üî¥ Slow |
| **Bzip2** | Large files, Hadoop-friendly | ‚úÖ Yes | üü¢ High | üî¥ Slow |
| **LZO** | Large-scale logs | ‚úÖ Yes | üîµ Moderate | üü¢ Fast |
| **DEFLATE** | General compression | ‚ùå No | üîµ Moderate | üîµ Medium |

---

# üèÅ **Final Thoughts**  

Compression **enhances storage efficiency and speeds up Hadoop jobs**, making **data movement faster and cheaper**. Choosing the right codec **depends on the use case**‚Äîwhether prioritizing **speed, compression strength, or splittability**.  

---

# üö® Limitations of Compression ‚Äì Balancing Performance & Efficiency  

Data compression **enhances storage efficiency and speeds up Hadoop workflows**, but it comes with trade-offs. Let‚Äôs explore the **key limitations** that impact decision-making in **big data environments**!  

---

## ‚öñÔ∏è **1Ô∏è‚É£ Space vs. Time Trade-off**  
- Compression **reduces file size**, saving **disk space** and **network bandwidth**.  
- However, **compressing & decompressing** data **adds extra computational time**.  

üí° **Key Question:**  
Is the **time cost** of compression worth the **storage savings**?  

### ‚úÖ **Scenario Where Compression Helps**  
- If the **dataset is massive**, compression **reduces storage needs** and **speeds up transfers**.  
- Example: **Log files in Hadoop** ‚Üí Smaller **compressed logs** minimize I/O overhead.  

### ‚ùå **Scenario Where Compression Hurts**  
- If the **data needs frequent access**, decompression **adds unnecessary delays**.  
- Example: **Real-time streaming analytics** ‚Üí Compression might **slow down** data retrieval.  

üí° **Analogy:** Imagine stuffing clothes into a **vacuum-sealed bag**. It **saves space**, but you need **extra time** to unpack it when needed! üß≥  

---

## üìù **2Ô∏è‚É£ Splittable vs. Non-Splittable Compression Formats**  
One of the biggest concerns in Hadoop **MapReduce** is whether a **compressed file can be split into chunks** for **parallel processing**.  

### ‚úÖ **Splittable Compression Formats** (Good for Hadoop)  
| **Compression Format** | **Splittable?** | **Best Use Case** |
|------------------------|----------------|-------------------|
| **Bzip2** | ‚úÖ Yes | Large datasets needing parallel processing |
| **LZO** | ‚úÖ Yes | Distributed logs and transactional data |

üí° **Why Splittable Matters?**  
Hadoop **divides large files** into smaller chunks for **parallel execution**. If compression **prevents splitting**, only **one node** processes the file, defeating Hadoop‚Äôs **distributed architecture**!  

---

### ‚ùå **Non-Splittable Compression Formats** (Can Slow Down Hadoop)  
| **Compression Format** | **Splittable?** | **Best Use Case** |
|------------------------|----------------|-------------------|
| **Gzip** | ‚ùå No | Archival data where splitting isn‚Äôt needed |
| **Snappy** | ‚ùå No | Real-time applications requiring fast I/O |
| **DEFLATE** | ‚ùå No | General-purpose compression |

üí° **Problem With Non-Splittable Compression**  
- If a **large Gzip file** is stored, **one reducer** processes it instead of **multiple parallel reducers**.  
- This slows down Hadoop jobs by **forcing sequential execution** rather than parallel computing.  

üí° **Analogy:** Imagine scanning a **massive book** for keywords. If you **can't divide the pages**, you must **scan everything manually** rather than having multiple people help! üìö  

---

## üèÅ **Final Thoughts**  
Choosing the right compression format **depends on the use case**! üöÄ  

‚úÖ **If processing large datasets in Hadoop ‚Üí Use splittable formats (Bzip2, LZO).**  
‚úÖ **If storing data for archival purposes ‚Üí Use non-splittable formats (Gzip, Snappy).**  
‚úÖ **If optimizing speed over compression ‚Üí Choose Snappy or LZO for high-speed applications.**  

---

# üî¨ **Lab 6: Configuring Compression in Hadoop MapReduce**  

In **BigDataVM's IdeaIntelliJ**, **SnappyCodec** wasn‚Äôt working, so we switched to **BZip2Codec** for **compressing Mapper output and final Reducer output** in Hadoop MapReduce jobs.  

---

## ‚öôÔ∏è **Compression Configuration in Hadoop**  

### ‚úÖ **Enabling Compression for Mapper Output**  
```java
conf.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true);
// conf.setClass(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, SnappyCodec.class, CompressionCodec.class);
conf.setClass(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, org.apache.hadoop.io.compress.BZip2Codec.class, CompressionCodec.class);
```
üîπ **Purpose:** Reduces network overhead by compressing Mapper output before shuffling.  

---

### ‚úÖ **Enabling Compression for Final Reducer Output**  
```java
conf.setBoolean(FileOutputFormat.COMPRESS, true);
// conf.setClass(FileOutputFormat.COMPRESS_CODEC, SnappyCodec.class, CompressionCodec.class);
conf.setClass(FileOutputFormat.COMPRESS_CODEC, org.apache.hadoop.io.compress.BZip2Codec.class, CompressionCodec.class);
```
üîπ **Purpose:** Saves HDFS storage space by writing compressed reducer output files (`part-r-00000`).  

---

## ‚ö†Ô∏è **What Happens If We Remove Compression Configuration?**  

If we **remove this part of code** and run the Hadoop job via **YARN**, the output **will be different** in key ways:  

### üî¥ **1Ô∏è‚É£ No Compression for Mapper Output**  
- **Higher network I/O overhead** during the shuffle phase.  
- **Increased data transfer between nodes**, slowing down execution time.  

üí° **Example:** Without compression, Mapper output increases **network congestion** like sending **raw images** instead of compressed ones! üñºÔ∏è  

---

### üî¥ **2Ô∏è‚É£ No Compression for Final Reducer Output**  
- **Larger storage footprint** in HDFS.  
- Output files (`part-r-00000`) **will not be compressed**, making them **heavier**.  

üí° **Example:** It‚Äôs like storing **uncompressed high-resolution videos**‚Äîit takes up more space! üé•  

---

## üéØ **Key Takeaways**  
‚úÖ **Compression Reduces Shuffle & Storage Overhead** ‚Äì Makes Hadoop jobs **faster & efficient**.  
‚úÖ **BZip2Codec is Splittable** ‚Äì Unlike Snappy & Gzip, BZip2 allows **parallel processing**.  
‚úÖ **Removing Compression Increases File Size** ‚Äì HDFS storage consumption grows.

---

### üìú Can We Perform MapReduce Without Using Java? Can We Use Python?

Yes, you **can** perform MapReduce without using Java! This is done using a concept called **Hadoop Streaming**. 

#### üîç What is Hadoop Streaming?
Hadoop Streaming is a utility that allows you to **run MapReduce jobs with any programming language**. Instead of writing Mappers and Reducers in Java, you can use **Python, R, shell scripts, or even other executables**.

üí° **Example**:  
Imagine you have a text file containing names, and you want to count how often each name appears. Using **Hadoop Streaming**, you can:
- Write a **Python script** as the Mapper to process input data.
- Write another **Python script** as the Reducer to aggregate the results.

---

### üö´ Why Do Many People Not Use Hadoop Streaming?

Even though Hadoop Streaming allows non-Java languages for MapReduce, it is **not widely used** for a few reasons:

1Ô∏è‚É£ **Apache Spark Was Introduced**  
   - Apache Spark is **faster** and more efficient for processing large-scale data.  
   - Unlike traditional MapReduce, Spark operates in-memory, reducing disk I/O operations.  
   - It supports **high-level APIs** in Python, Java, Scala, and R, making it much easier to work with than Hadoop Streaming.

2Ô∏è‚É£ **MapReduce‚Äôs Native Implementation is in Java**  
   - Hadoop‚Äôs MapReduce engine is designed to **work best with Java**.  
   - Java-based MapReduce jobs perform better because they **directly integrate** with the Hadoop ecosystem.  
   - Using Python or other languages via Hadoop Streaming adds a slight performance overhead.

3Ô∏è‚É£ **Spark Supports Multiple Languages**  
   - Spark allows programming in **Python, R, Java, and Scala**, which makes it **more flexible** than Hadoop Streaming.  
   - Python users prefer **PySpark**, which is a **more efficient** way of writing distributed processing jobs than using Hadoop Streaming.  
   
---

### üñºÔ∏è Image Description: Hadoop Streaming Flow

![image](https://github.com/user-attachments/assets/78fe4217-78fd-4efd-a3cb-b54daa816e1c)

The image explains **how Hadoop Streaming works**:

1Ô∏è‚É£ **Input Split** ‚Üí The input data is broken into pieces and formatted as `<key1, value1>` pairs.  
2Ô∏è‚É£ **Mapper (External Script)** ‚Üí Converts `<key1, value1>` into lines of text and **sends them to an external program** (e.g., Python script).  
3Ô∏è‚É£ **Python Mapper Script** ‚Üí Processes stdin (input) and **outputs `<key2, value2>` pairs**.  
4Ô∏è‚É£ **Reducer (External Script)** ‚Üí Receives `<key2, (value2, value2, ‚Ä¶)>` and processes them via another external program.  
5Ô∏è‚É£ **Python Reducer Script** ‚Üí Outputs final `<key3, value3>` pairs as the result.  

üí° **This image helps visualize how non-Java languages interact with Hadoop Streaming!**

---

### üöÄ Running a Hadoop Streaming Job

A **Streaming Job** is just like a traditional **MapReduce Job**, except it **does not require Java-based Mappers and Reducers**. Instead, it allows using **any executable script** (like Python, Shell, or Perl).

#### üèóÔ∏è Command Structure:
To run a Hadoop Streaming job, we use the **hadoop-streaming.jar** file:

```bash
hadoop jar $HADOOP_HOME/lib/hadoop-streaming.jar \
-input <input_directories> \
-output <output_directories> \
-mapper <mapper_script> \
-reducer <reducer_script>
```

### üõ†Ô∏è Breakdown of the Command:
üîπ **hadoop jar $HADOOP_HOME/lib/hadoop-streaming.jar** ‚Üí Runs the Hadoop Streaming utility.  
üîπ **-input input_directories** ‚Üí Specifies the HDFS directory containing input files.  
üîπ **-output output_directories** ‚Üí Specifies the HDFS directory where output will be saved.  
üîπ **-mapper mapper_script** ‚Üí Defines the script to process input (Python, Shell, etc.).  
üîπ **-reducer reducer_script** ‚Üí Defines the script to aggregate/compute results.

üí° **Example for Python MapReduce:**  
Imagine you have a large dataset containing words and want to **count word occurrences** using Python.

#### üéØ Python Mapper (`mapper.py`)
```python
import sys
for line in sys.stdin:
    words = line.strip().split()
    for word in words:
        print(f"{word}\t1")  # Output as key-value pairs (word, 1)
```

#### üìù Python Reducer (`reducer.py`)
```python
import sys
from collections import defaultdict

word_counts = defaultdict(int)
for line in sys.stdin:
    word, count = line.strip().split("\t")
    word_counts[word] += int(count)

for word, count in word_counts.items():
    print(f"{word}\t{count}")  # Output as key-value pairs (word, total count)
```

üîπ **Executing the Job:**
```bash
hadoop jar $HADOOP_HOME/lib/hadoop-streaming.jar \
-input /user/hadoop/input \
-output /user/hadoop/output \
-mapper mapper.py \
-reducer reducer.py
```

‚ú® This command runs a MapReduce job **without Java**, using Python for both **Mapper** and **Reducer**.

---
