# ğŸ¾ **Understanding the Code: Abstract Class & Polymorphism in Java**

## ğŸ”¥ **Key Concept: Abstraction & Polymorphism**

* **Abstraction**: Hiding internal details and showing only necessary features.
* **Polymorphism**: "Many forms" â€” the same method behaves differently based on the object.

> **`Animal dog = new Dog();`**
> This is called **dynamic polymorphism**:
> Super class **reference** â†’ points to sub class **object**.

---

## ğŸ• **Code Breakdown: Step by Step**

### 1ï¸âƒ£ **Abstract Class: Animal**

```java
abstract class Animal {
    abstract void makeSound();
}
```

* `abstract` means **cannot create object** of `Animal` directly.
* **Abstract method** `makeSound()` has **no body** here.
  â†’ Subclasses **must** override this method.

---

### 2ï¸âƒ£ **Dog, Cat, Cow â€” Subclasses**

Each class **extends** `Animal` and **overrides** `makeSound()` method.

```java
class Dog extends Animal {
    @Override
    void makeSound() {
        System.out.println("Woof");
    }
}
```

Similarly, `Cat` says `"Meow"` and `Cow` says `"Moo"`.

---

### 3ï¸âƒ£ **Main Class Execution**

```java
public class AnimalSound {
    public static void main(String[] args) {
        Animal dog = new Dog();
        Animal cat = new Cat();
        Animal cow = new Cow();

        dog.makeSound(); // Woof
        cat.makeSound(); // Meow
        cow.makeSound(); // Moo
    }
}
```

#### ğŸ’¡ What's Happening Here?

* `Animal dog = new Dog();` â†’ Superclass reference holding subclass object.
* When you call `dog.makeSound()`, **Dog's** version runs, not Animal's (because Animal is abstract).
* This is called **Runtime Polymorphism** (or **Dynamic Method Dispatch**).

---

## ğŸ¯ **Important Points to Remember**

* An **abstract class** can have abstract and non-abstract methods.
* You **cannot instantiate** an abstract class directly.
* **`@Override`** ensures you are correctly overriding the method from superclass.
* Using `Animal dog = new Dog();` allows **flexibility** â€” you can change object types (Dog, Cat, Cow) but call them using `Animal`.

---

## ğŸ“Š **Visual Concept**

*(Keep your existing images linked here â€” e.g., class hierarchy diagram)*

* Abstract `Animal` â¡ï¸  Subclasses: `Dog`, `Cat`, `Cow`
* Each subclass **defines** its own `makeSound()`.

---

## ğŸ§  **Think Like This**

| Code                      | What It Means                     |
| ------------------------- | --------------------------------- |
| `Animal dog = new Dog();` | Superclass ref â†’ Subclass object  |
| `dog.makeSound();`        | Runs Dog's `makeSound()` â†’ `Woof` |
| `Animal cat = new Cat();` | Runs Cat's `makeSound()` â†’ `Meow` |

---

## ğŸŒ± **Beginner Tip: Why Use This?**

* If you have **many animals**, you donâ€™t write separate code for each.
* You can just say:

```java
Animal a = new Lion(); 
a.makeSound();
```

And it will call Lion's version.
â¡ï¸ Makes code **flexible** and **easy to extend**!

---

# ğŸ˜ **Sqoop: Importing RDBMS Data into Hadoop HDFS**

## ğŸ¯ **Why Sqoop?**

* Our data is in **RDBMS** (like MySQL, Oracle, etc.).
* But **Hadoop** processes data only when it's in **HDFS**.

> **Goal:** Pull (import) data from **RDBMS** â¡ï¸ **HDFS** for big data processing.

---

## ğŸš€ **How Sqoop Works: Step by Step**

1. âœ… **Client executes** a Sqoop command.
2. ğŸ”„ **Sqoop converts** the command into a **MapReduce** job.
3. ğŸ”Œ **Plugins** connect Sqoop to different RDBMS databases (like MySQL, Oracle, etc.).
4. ğŸ“¦ Data is **imported** into HDFS.

> ğŸ’¡ Sqoop uses **Map-only tasks** in MapReduce for importing.

![Sqoop Flow](https://github.com/user-attachments/assets/943fe696-4f41-40dd-aa19-964e7be220e2)

---

## ğŸ› ï¸ **The Sqoop Import Tool**

### âœ… **Mandatory things to specify in the import command:**

* **Connection string** â†’ `--connect`
* **Username & Password** â†’ `--username` and `--password`

  * Uses **JDBC** to authenticate the user.
* **Data source:**

  * **Table** â†’ `--table` (for full table import)
  * or **Custom SQL query** â†’ `--query` (for selective import)

---

## ğŸ“¥ **Example: Importing a Table**

```bash
sqoop import \
--connect jdbc:mysql://host/nyse \
--table StockPrices \
--target-dir /data/stockprice/ \
--as-textfile
```

### ğŸ” **Explanation:**

| Command Part                       | Meaning                                              |
| ---------------------------------- | ---------------------------------------------------- |
| `--connect jdbc:mysql://host/nyse` | Connect to MySQL DB `nyse` on `host`                 |
| `--table StockPrices`              | Import table `StockPrices`                           |
| `--target-dir /data/stockprice/`   | Store imported data in HDFS path `/data/stockprice/` |
| `--as-textfile`                    | Store data in **textfile** format                    |

---

## âš™ï¸ **What Happens When We Execute?**

* This command **launches a MapReduce job**.
* By default, it uses **4 map tasks**.
* âœ”ï¸ All 4 tasks run **in parallel** (because MapReduce is parallel processing).

### ğŸ’¡ **Why parallel?**

* MapReduce splits the data and processes chunks **simultaneously** â¡ï¸ making the import faster!

---

## ğŸŒ± **Beginner Tip: Why Use Sqoop?**

* When you want to **move large volumes of structured data** from RDBMS â¡ï¸ HDFS.
* Saves you from writing complex ETL code â€” Sqoop handles it all for you!
* Supports importing in **various formats**: textfile, Avro, Parquet, etc.

---

## âœ… **Quick Revision Points**

* Sqoop is a **bridge** between RDBMS and Hadoop.
* Uses **MapReduce jobs** under the hood (map-only jobs).
* Supports **parallel imports** using multiple map tasks.
* Uses **JDBC connection** for authentication and data pull.
* You specify:

  * **DB connection**
  * **Table or query**
  * **HDFS target path**
  * **File format**

---

# ğŸ¯ **Sqoop: Importing Specific Columns (Projection)**

## ğŸŒ± **Goal: Get Only Required Columns**

* Sometimes, we donâ€™t want **all columns** from a table â€” just a few.
* This is called **Projection** in database terms (selecting specific columns).

---

## ğŸ“¥ **Example: Importing Specific Columns**

```bash
sqoop import \
--connect jdbc:mysql://host/nyse \
--table StockPrices \
--columns StockSymbol,Volume,High,ClosingPrice \
--target-dir /data/dailyhighs/ \
--as-textfile \
--split-by StockSymbol \
-m 10
```

---

## ğŸ” **Breaking It Down**

| Command Part                                     | Meaning                                                    |
| ------------------------------------------------ | ---------------------------------------------------------- |
| `--columns StockSymbol,Volume,High,ClosingPrice` | Only these columns will be imported (**projection**)       |
| `--target-dir /data/dailyhighs/`                 | Store in HDFS path `/data/dailyhighs/`                     |
| `--as-textfile`                                  | Save as textfile format                                    |
| `--split-by StockSymbol`                         | Split data across **map tasks** using `StockSymbol` column |
| `-m 10`                                          | Launch **10 map tasks** (parallel workers)                 |

---

## âš¡ **What is `--split-by`? (Load Balancing Explained)**

* Sqoop imports data using multiple **map tasks** (parallel workers).
* `--split-by` decides **how the data is split** among them.
* You should choose a column that:

  * Has **unique** or evenly spread values.
  * Ensures **equal distribution** â¡ï¸ so each map task gets a fair share.

### ğŸ“Š **Formula:**

```
No. of rows in split column / No. of map tasks
```

â¡ï¸ This ensures **no overlapping** and avoids **performance hit**.

> ğŸ’¡ If `--split-by` is not specified, **primary key** is used by default.

---

## ğŸ› ï¸ **About `-m` (Map Tasks Count)**

* `-m` decides **how many parallel map tasks** will be launched.
* In this case: `-m 10` â¡ï¸ launches **10 tasks** pulling data simultaneously.

### ğŸ”¥ **Default:**

If `-m` is **not specified**, Sqoop uses **4 map tasks** by default.

> **Earlier table import example** didnâ€™t have `-m`, so it used **4** map tasks.

---

## âœ… **Quick Revision Points**

* **Projection** â†’ Select only specific columns using `--columns`.
* **Load Balancing** â†’ Use `--split-by` for **equal data split** across tasks.
* **Parallelism** â†’ Control number of tasks using `-m` (default is 4).
* Use a **good column** (like `StockSymbol`) for splitting:

  * Unique or evenly spread
  * Not necessarily the primary key

---

## ğŸš€ **Best Practice Tips**

* Always use `--split-by` for **large imports** to avoid performance issues.
* Use `-m` to **increase parallelism** (e.g., `-m 10` or higher) when you have more cluster capacity.
* Choose **split-by column** carefully to ensure **balanced** load across map tasks.

---

# ğŸ” **Sqoop: Importing Data Using a SQL Query**

## ğŸŒ± **Goal: Filter & Import Only Matching Data**

* Instead of importing the whole table â¡ï¸ we import **only rows matching our SQL query**.
* Example: Import **stock prices** where **Volume â‰¥ 1,000,000**.

---

## ğŸ“¥ **Example: Importing Using Query**

```bash
sqoop import \
--connect jdbc:mysql://host/nyse \
--query "SELECT * FROM StockPrices WHERE s.Volume >= 1000000 AND \$CONDITIONS" \
--target-dir /data/highvolume/ \
--as-textfile \
--split-by StockSymbol
```

---

## ğŸ” **Breaking It Down**

| Command Part                                    | Meaning                                                          |
| ----------------------------------------------- | ---------------------------------------------------------------- |
| `--query "SELECT * FROM StockPrices WHERE ..."` | Import only rows matching the **query**                          |
| `AND \$CONDITIONS`                              | **Mandatory placeholder** â€” Sqoop replaces this during splitting |
| `--target-dir /data/highvolume/`                | Store imported data in HDFS `/data/highvolume/`                  |
| `--as-textfile`                                 | Save as **textfile** format                                      |
| `--split-by StockSymbol`                        | Split rows using `StockSymbol` column                            |

---

## âš ï¸ **Important Rule: When Using `--query`**

* **`--split-by` is mandatory** â¡ï¸ because Sqoop must split query results across tasks.
* **`\$CONDITIONS`** is **required** inside query:

  * Sqoop injects split ranges using this placeholder.
  * Example:

    ```sql
    WHERE s.Volume >= 1000000 AND StockSymbol >= 'A' AND StockSymbol < 'F'
    ```

> ğŸ’¡ Without `\$CONDITIONS`, **Sqoop query fails**.

---

## ğŸ§  **Why Use Query Import?**

* Filter data directly **at source** (RDBMS) â¡ï¸ reduces data transfer.
* Useful when:

  * Importing only **recent records**
  * Importing **filtered subset** (like high-volume stocks)

---

# ğŸ“‚ **Copying and Verifying Files (Lab Task)**

## âœ… **Step 1: Copy `salaries.txt` to LABS\_HOME**

> Copy from **staging area** to your working lab directory.

![Copy Image](https://github.com/user-attachments/assets/6378aba5-6161-45c5-9d93-9d7b1e681de1)

---

## âœ… **Step 2: Confirm Your Lab**

We are currently in:

> **Lab3.1**

![Lab Image](https://github.com/user-attachments/assets/38fd515f-d743-403e-95e2-379a7475d2b9)

---

## âœ… **Step 3: Copy File to `/tmp` Directory**

```bash
cp salaries.txt /tmp/
```

![Copy to tmp](https://github.com/user-attachments/assets/eb511973-a879-4597-a9fa-e793e68a74d4)

---

## âœ… **Step 4: Verify File is Present**

```bash
ls /tmp
```

Look for `salaries.txt` in the list.

![Verify](https://github.com/user-attachments/assets/dd2098e7-c86e-4f3e-94e0-f342e6e10e7a)

---

## âœ… **Step 5: Connecting to Database**

> Connect to your RDBMS to check tables or run queries.

![DB Connect](https://github.com/user-attachments/assets/f87e86f3-91aa-40dd-8818-180e6b1a7ee3)

---

## ğŸ”¥ **Quick Revision Points**

* Use **`--query`** when importing filtered/selected rows.
* **`\$CONDITIONS`** placeholder is **mandatory** in query.
* Must specify **`--split-by`** column when using query import.
* Always verify files with `ls` after copying.
* **Lab flow:**

  1. Copy file â¡ï¸
  2. Verify file â¡ï¸
  3. Connect to DB â¡ï¸
  4. Run Sqoop import

---

## ğŸ¯ **Beginner Tip: Remember This!**

* **Table import** is for **full table** â¡ï¸ easy but heavy.
* **Query import** is for **filtered data** â¡ï¸ efficient for big data jobs.

---

# ğŸ¬ **MySQL: Creating Database & Table for Sqoop Practice**

## âœ… **Step 1: Login to MySQL**

```bash
mysql -u root -p
```

* Enter password: `cloudera`

ğŸ” **Password Prompt Appears**

> Type: `cloudera` (default password)

![MySQL Login](https://github.com/user-attachments/assets/1bc137c6-b772-404a-8a46-82c077164613)

---

## âœ… **Step 2: Create a New Database `test`**

```sql
CREATE DATABASE test;
```

âœ”ï¸ Database created successfully!

![Create DB](https://github.com/user-attachments/assets/cf8d2aa9-ea11-4773-a034-4f6b47818673)

---

## âœ… **Step 3: Check Existing Databases**

```sql
SHOW DATABASES;
```

* Youâ€™ll see a list of databases including your new **`test`** DB.

![Show DBs](https://github.com/user-attachments/assets/6a86aa99-9095-4f7d-b5ff-0fae5073929b)

---

## âœ… **Step 4: Switch to the `test` Database**

```sql
USE test;
```

âœ”ï¸ Switched to database **`test`**!

![Use DB](https://github.com/user-attachments/assets/c7238cd0-e422-4003-b155-8badcbf65808)

---

## âœ… **Step 5: Create Table `salaries`**

```sql
CREATE TABLE salaries (
  gender varchar(1),
  age int,
  salary double,
  zipcode int
);
```

âœ”ï¸ Table **`salaries`** created successfully!

![Create Table](https://github.com/user-attachments/assets/d2491f82-e154-4b8c-9aac-b00903e0ea9e)

---

## ğŸ“ **Table Structure Explanation**

| Column    | Data Type  | Meaning                           |
| --------- | ---------- | --------------------------------- |
| `gender`  | varchar(1) | Stores `M` (male) or `F` (female) |
| `age`     | int        | Stores age (number)               |
| `salary`  | double     | Stores salary (decimal values)    |
| `zipcode` | int        | Stores area zip code              |

---

## ğŸ”¥ **Quick Revision Points**

* âœ… Login using `mysql -u root -p`
* âœ… Create DB: `CREATE DATABASE test;`
* âœ… See all DBs: `SHOW DATABASES;`
* âœ… Switch DB: `USE test;`
* âœ… Create table: `CREATE TABLE salaries (...)`

---

## ğŸ¯ **Beginner Tip: Remember This!**

* **Databases** â¡ï¸ like folders ğŸ—‚ï¸ for storing tables.
* **Tables** â¡ï¸ like spreadsheets ğŸ“Š with rows and columns.
* **SQL Commands** â¡ï¸ always end with `;`

---

# ğŸ“Š **MySQL: Loading Data & Altering Table**

---

## âœ… **Step 6: Show Tables in Database**

```sql
SHOW TABLES;
```

âœ”ï¸ Shows all tables present in current DB (like `salaries`)

![Show Tables](https://github.com/user-attachments/assets/a71f47ce-44fa-4509-85c1-e45bebeb2d70)

---

## âœ… **Step 7: Describe Table Structure**

```sql
DESC salaries;
```

âœ”ï¸ Shows **columns**, their **types**, and if they accept `NULL`.

![Describe Table](https://github.com/user-attachments/assets/65ddef56-5ee1-4942-9e84-124082e02170)

---

## âœ… **Step 8: Load Data from File into Table**

```sql
LOAD DATA LOCAL INFILE '/tmp/salaries.txt' 
INTO TABLE salaries 
FIELDS TERMINATED BY ',';
```

âœ”ï¸ This **loads data** from file `/tmp/salaries.txt` into `salaries` table.

ğŸ“¦ **File Content Format**:

```
M,25,50000,94107
F,30,60000,94016
...
```

* Data is **comma-separated** (CSV).

![Load Data](https://github.com/user-attachments/assets/18948897-6790-4e4d-9573-9e1d438ad89b)

---

## âœ… **Step 9: Verify Row Count**

```sql
SELECT COUNT(*) FROM salaries;
```

âœ”ï¸ Confirms how many rows got loaded.

![Count Rows](https://github.com/user-attachments/assets/c046383c-398e-4f48-9153-9f7f3ef9eb83)

---

## âœ… **Step 10: Alter Table â€” Add Primary Key**

```sql
ALTER TABLE salaries 
ADD COLUMN `id` INT(10) UNSIGNED PRIMARY KEY AUTO_INCREMENT;
```

âœ”ï¸ Adds new column:

* **`id`** â¡ï¸ becomes unique row number (`1,2,3â€¦`)
* **Primary Key** â¡ï¸ avoids duplicate rows
* **Auto Increment** â¡ï¸ automatically numbers new rows

![Alter Table](https://github.com/user-attachments/assets/b9593375-976f-4ed3-8233-b1e369af15b0)

---

## ğŸ“ **Result: Updated `salaries` Table**

| id  | gender | age | salary  | zipcode |
| --- | ------ | --- | ------- | ------- |
| 1   | M      | 25  | 50000.0 | 94107   |
| 2   | F      | 30  | 60000.0 | 94016   |
| ... | ...    | ... | ...     | ...     |

---

## ğŸ”¥ **Quick Revision Points**

* âœ… `SHOW TABLES;` â¡ï¸ lists tables
* âœ… `DESC table;` â¡ï¸ shows structure
* âœ… `LOAD DATA LOCAL INFILE` â¡ï¸ bulk load from CSV
* âœ… `SELECT COUNT(*)` â¡ï¸ verify rows
* âœ… `ALTER TABLE ADD COLUMN id` â¡ï¸ add auto-increment primary key

---

## ğŸ¯ **Beginner Tip: Remember This!**

* `LOAD DATA LOCAL INFILE` â¡ï¸ fastest way to bulk insert from file ğŸ“‚
* Always **add a Primary Key** (`id`) â¡ï¸ better for querying, indexing & Sqoop split operations!

---

# ğŸš€ **Import MySQL Table into Hadoop (HDFS) using Sqoop**

---

## âœ… **Step 11: Describe Updated Table**

```sql
DESC salaries;
```

âœ”ï¸ Now the table shows **`id`** as **Primary Key** and **Auto Increment**.

![Describe Updated Table](https://github.com/user-attachments/assets/e3835905-2204-4e60-93bf-3d8f958f5a6a)

---

## âœ… **Step 12: Display Table Records**

```sql
SELECT * FROM salaries;
```

âœ”ï¸ Displays **all rows** with newly added **`id`** column.

![Display Table](https://github.com/user-attachments/assets/377384a6-2b1f-4a43-82ac-49b1d5ea129a)

---

## ğŸŒŸ **ğŸ“¦ Now the Database is Ready for Hadoop Import!**

---

# ğŸ˜ **Step 13: Import Table into HDFS using Sqoop**

```bash
sqoop import \
--connect jdbc:mysql://quickstart.cloudera:3306/test \
--driver com.mysql.jdbc.Driver \
--username root \
--password cloudera \
--table salaries
```

âœ”ï¸ This command will:

* Connect to **MySQL DB** at `quickstart.cloudera:3306/test`
* Use user: `root`, password: `cloudera`
* Import the **`salaries`** table
* Store it into **HDFS** (default folder like `/user/cloudera/salaries`)

---

### ğŸ¯ **What Happens Internally?**

* ğŸš€ **Sqoop launches a MapReduce job** âœ…
* ğŸ—‚ï¸ Data is **pulled from MySQL** and **written to HDFS**
* âš™ï¸ By default, 4 parallel tasks (mappers) are launched

---

## ğŸ–¥ï¸ **Output of MapReduce Program**

![Sqoop MapReduce](https://github.com/user-attachments/assets/f84b567f-36ac-456c-b2d4-4c6e4c215809)
![Sqoop MapReduce 2](https://github.com/user-attachments/assets/91073306-8e84-4328-95dd-0e77124b093c)

âœ”ï¸ This shows **tasks running** and **completed** successfully!

---

## ğŸŒ **Understanding Web UI Ports**

| ğŸ–¥ï¸ **Tool**                 | ğŸŒ **Port** | ğŸ” **Purpose**           |
| ---------------------------- | ----------- | ------------------------ |
| **Resource Manager (YARN)**  | `8088`      | Shows **active jobs**    |
| **MapReduce History Server** | `19888`     | Shows **completed jobs** |

### â¡ï¸ Access via Browser:

* Resource Manager: `http://localhost:8088`
* History Server: `http://localhost:19888`

![YARN 19888](https://github.com/user-attachments/assets/d22209da-2f7c-4794-b800-bd724646955f)

---

## ğŸ“ **Quick Revision Points**

* âœ… `DESC salaries;` â¡ï¸ shows table structure
* âœ… `SELECT * FROM salaries;` â¡ï¸ displays rows
* âœ… `sqoop import ...` â¡ï¸ pulls table into HDFS
* âœ… MapReduce job runs to do the actual import

---

## ğŸ¯ **Beginner Tip: Remember This!**

* **Sqoop** bridges ğŸ›£ï¸ between **RDBMS** â¡ï¸ **Hadoop HDFS**
* MapReduce does the import job in **parallel** ğŸš€
* `8088` â¡ï¸ Active jobs; `19888` â¡ï¸ Completed jobs ğŸ—‚ï¸

---

Absolutely! Here's your next polished and complete **notes section**, including **all your images**, keeping the same **clear, beginner-friendly** style with **emojis** ğŸŒŸğŸš€.

---

# ğŸ˜ **Verifying Imported Data in HDFS**

---

## âœ… **Step 14: Verify if the 'salaries' Folder Exists in HDFS**

Check whether the data has been successfully imported by listing the HDFS directories.

```bash
hdfs dfs -ls
hdfs dfs -ls salaries
```

ğŸ“‚ This will list the **salaries** folder and its files inside HDFS.

![Verify HDFS Directory](https://github.com/user-attachments/assets/48ceb853-bd81-4e71-9261-0439990f65f7)

---

## ğŸ” **Step 15: Analyze Data Distribution Across Map Tasks**

* The data size for each file (map task output) is approximately:
  **0 bytes, 272 bytes, 241 bytes, 238 bytes, 272 bytes**
* ğŸ“Š **Almost equally distributed!**
* **Why?**

  * **Split-by** was not manually specified.
  * **Default behavior:** Sqoop automatically picks the **primary key** (`id` column in our case) for splitting the data among mappers.

---

## ğŸ“Œ **Important Points About Map Tasks**

* ğŸ› ï¸ **All map tasks run in parallel**, not sequentially!
* ğŸï¸ This parallel execution ensures **faster data import**.

---

# ğŸ”¥ **Exploring MapReduce Job Details**

---

## ğŸ—ºï¸ **Step 16: View MapReduce Job in Resource Manager UI**

You can see the running/completed applications in YARN Resource Manager (port 8088).

![MapReduce Job View](https://github.com/user-attachments/assets/2f04843e-c795-4cda-83ef-360c1368e859)

---

## ğŸ†” **Step 17: Find the Application ID**

ğŸ“ Copy the **application ID** of your Sqoop import job from the Resource Manager.

Example:
`application_1745983395808_0001`

![Find Application ID](https://github.com/user-attachments/assets/e8faf031-4f5a-4cfb-9ba8-0144f05ba208)

---

## ğŸ“œ **Step 18: View Detailed Logs for Application**

Run the following command to fetch the complete logs of your import job:

```bash
yarn logs -applicationId application_1745983395808_0001
```

ğŸ“‹ This will display **all the logs** generated by the job.

![Application Logs](https://github.com/user-attachments/assets/a3e5ff01-6564-4e83-b574-f14992ae3921)

---

## ğŸ” **Step 19: Filter Logs to See SQL Query Only**

If you are interested **only in the SQL query executed**, you can filter it using:

```bash
yarn logs -applicationId application_1745983395808_0001 | grep "query:"
```

ğŸ” This helps you focus on **SQL-related operations** without going through the full log.

![Filtered Query Log](https://github.com/user-attachments/assets/278b2691-80b6-413d-8b75-42d4d79fa7ae)

---

# ğŸ¯ **Quick Revision Points**

| ğŸ“                         | âœ¨                                        |
| -------------------------- | ---------------------------------------- |
| `hdfs dfs -ls salaries`    | Check imported files                     |
| No split-by?               | Sqoop uses **Primary Key** for splitting |
| Map tasks                  | **Run parallelly** for faster import     |
| `yarn logs -applicationId` | View full job logs                       |
| `grep "query:"`            | Fetch only executed SQL query from logs  |

---

# ğŸ“š **Summary**

* **Data successfully imported** from MySQL â¡ï¸ HDFS ğŸ—‚ï¸
* **MapReduce job** handled splitting and importing in parallel ğŸ”¥
* We now have full control over **job monitoring** and **debugging** through YARN logs.

---

# ğŸ§‘â€ğŸ’» **Importing Specific Columns**

---

## 1ï¸âƒ£ **Specify Columns to Import with Sqoop**

To import only **selected columns** from a table:

```bash
sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/test \
--driver com.mysql.jdbc.Driver --username root --password cloudera \
--table salaries --columns salary,age -m 1 --target-dir salaries2
```

ğŸ“ Hereâ€™s what each part does:

* **`--columns salary,age`**: Specifies the columns to import (only `salary` and `age`).
* **`-m 1`**: Using **1 map task** for importing the data.
* **`--target-dir salaries2`**: Specifies the **HDFS destination** directory.

---

## âœ… **Check Imported Data in HDFS**

Verify if the data is stored correctly in HDFS:

```bash
hdfs dfs -ls
```

![HDFS Directory](https://github.com/user-attachments/assets/0fe99409-a8b0-43d4-9bf5-4315cb931869)

---

## ğŸ“ **View Logs of MapReduce Job for salaries2**

Check logs using the **application ID**:

![Job Logs](https://github.com/user-attachments/assets/dbcd755f-b506-4ecc-b658-9c15ae544360)

---

# ğŸ“‘ **Importing Data Using a SQL Query**

---

## 2ï¸âƒ£ **Import Data Based on Query with Sqoop**

To filter and import data based on a **SQL query**:

```bash
sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/test \
--driver com.mysql.jdbc.Driver --username root --password cloudera \
--query "select * from salaries s where s.salary > 90000.00 and \$CONDITIONS" \
--split-by gender -m 2 --target-dir salaries3
```

ğŸ“ Breakdown:

* **`--query`**: Provides the SQL query to filter the data.
* **`--split-by gender`**: Data will be split based on the **gender** column.
* **`-m 2`**: Specifies **2 map tasks**.

---

## âœ… **Check Imported Data in HDFS**

```bash
hdfs dfs -ls salaries3
```

![HDFS Query Import](https://github.com/user-attachments/assets/55eab0ef-625c-4dc8-976a-f0cef40631c1)

---

## ğŸ“ **Logs for Query Import**

To view the logs of the import:

![Query Import Logs](https://github.com/user-attachments/assets/c694bb4d-cf9d-43f1-a7ea-b829bff3889a)

---

## âš ï¸ **Error: Missing \$CONDITIONS**

If you try to run the **query without \$CONDITIONS**, youâ€™ll get an error like:

![Missing \$CONDITIONS](https://github.com/user-attachments/assets/8f97b6a4-5b85-423a-91f7-48cb16f7f845)

---

## âš ï¸ **Error: Missing Split Function**

If you run the import without the **split function**, youâ€™ll see an error:

![Missing Split Function](https://github.com/user-attachments/assets/268efaed-4f35-49ec-9fb3-92347461f68d)

---

## ğŸš¨ **Key Takeaway:**

* Both **\$CONDITIONS** and **`--split-by`** are **mandatory** for running a query-based import with Sqoop.

---

# ğŸ¤– **Automating the Import with Shell Script**

To **automate** the process, use a **bash script** like this:

```bash
#!/bin/bash

$(hdfs dfs -test -e $1)

if [[ $? -eq 0 ]]; then
        hdfs dfs -rm -R salaries
fi

sqoop import --connect jdbc:mysql://talentum-virtual-machine:3306/test?useSSL=false \
--driver com.mysql.jdbc.Driver --username bigdata -password Bigdata@123 --table salaries
```

ğŸ“ What it does:

1. Checks if a directory (`$1`) exists in HDFS.
2. If exists, it **removes** the directory (`salaries`).
3. Imports the **`salaries`** table from MySQL to HDFS.

---

# ğŸš€ **DistCp**

---

## ğŸ” **What is DistCp?**

* **DistCp** (Distributed Copy) is a tool to **copy large datasets** across clusters.
* It uses **MapReduce** to copy data in parallel, making the transfer faster and efficient.

---

## ğŸ“ **DistCp Recommendations**

* By default, **DistCp** uses **20 mappers** for the copy process.
* This parallelism speeds up the data transfer significantly!

---

# ğŸ“š **Summary of Key Concepts**

| ğŸ“ **Command/Concept**        | âœ¨ **Explanation**                               |
| ----------------------------- | ----------------------------------------------- |
| **`sqoop import --columns`**  | Import only specified columns                   |
| **`--query`**                 | Import data based on a custom SQL query         |
| **`--split-by`**              | Split data across multiple mappers (tasks)      |
| **`DistCp`**                  | Efficiently copy large datasets across clusters |
| **`Automated Import Script`** | Automate data import with bash scripting        |

---
