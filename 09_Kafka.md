# ğŸš€ How Companies Start  

### Understanding Data Exchange in Business  
Before diving into **Kafka**, let's understand how companies begin handling data.

ğŸ”¹ At the start, things are **simple**:  
   - There's **one** source system (where data is generated).  
   - There's **one** target system (where data is needed).  
   - These two systems **exchange data** between each other seamlessly.

![image](https://github.com/user-attachments/assets/aea045a7-28b2-45ce-a698-cad3b5b54013)

---

# âš¡ Scaling Up: More Systems, More Complexity  

ğŸ”¹ As a company **grows**, things get complicated:  
   - **Multiple** source systems emerge, generating diverse data.  
   - **Multiple** target systems appear, each requiring different data.  
   - **Tight coupling** occurs, meaning systems become heavily dependent on each other.  
   - We **can't easily separate them**, making modifications a headache!  

ğŸ“Œ The more systems interact, the harder it becomes to **manage data flow**.

![image](https://github.com/user-attachments/assets/83960fc7-035f-4d38-9ade-becb21130480)

---

# âŒ Challenges in Traditional Architectures  

Let's say we have **4 source systems** and **6 target systems**.  
ğŸ”¹ To **connect** them all, we'd need **24 integrations** ğŸ˜µ!  

Each integration comes with **major challenges**:  
âœ… **Protocol Issues** â€“ How should data move? (TCP, HTTP, REST, FTP, JDBC...)  
âœ… **Data Format Problems** â€“ How should we structure data? (Binary, CSV, JSON, Avro...)  
âœ… **Data Schema Evolution** â€“ How will data **change** over time?  
âœ… **High System Load** â€“ Every new connection **adds stress** to the system.  

ğŸ¯ **Clearly, this isnâ€™t scalable!** Companies **struggle** to manage data efficiently.  
So... how do we **solve** this problem? ğŸ¤”  

---

# ğŸŒŸ Solution: Apache Kafka  

ğŸ¯ This is where **Apache Kafka** comes in! ğŸš€  

ğŸ”¹ Kafka **decouples** data streams, making systems **independent**.  
ğŸ”¹ Now, instead of direct exchanges between systems:  
   - **Source systems** send data to **Kafka**.  
   - **Target systems** **fetch data** from **Kafka**.  

ğŸ’¡ **No more tight coupling!** Kafka acts as a **middle layer**, simplifying data flow.

![image](https://github.com/user-attachments/assets/4adf3bc9-5877-4b2b-a167-6c5a1b64e81f)

---

# ğŸ”„ Kafka Enables Flexible Data Flow  

Kafka allows **any type of data stream** to flow through it:  
âœ… Log files  
âœ… Transactions  
âœ… Sensor data  
âœ… Messages from applications  

ğŸ”¹ Once data is inside **Kafka**, it can be sent to **any system** efficiently!  
ğŸ“Œ Think of it as a **super-fast messenger** that **delivers data smoothly**. ğŸš€  

![image](https://github.com/user-attachments/assets/589a18b7-84c4-4dea-ab97-29c210a9ebb0)

---

# ğŸŒ Why Apache Kafka?  

### Origins & Growth ğŸ“Œ  
ğŸ”¹ **Kafka** was originally developed by **LinkedIn** and is now an **open-source** project.  
ğŸ”¹ It's mainly maintained by **Confluent**, but it falls under the **Apache Foundation** stewardship.  

### Why is Kafka Powerful?  
âœ… **Distributed Architecture** â€“ Handles data across multiple machines.  
âœ… **Resilient & Fault-Tolerant** â€“ Can **recover** from failures (similar to RDD in Spark and fault tolerance in Hadoop).  
âœ… **Scales Horizontally** â€“ Easily adds more machines to process high data volumes.  
âœ… **Massive Clusters** â€“ Some Kafka setups have **100+ brokers** handling data.  
âœ… **High Throughput** â€“ Proven by LinkedIn & others to process **millions** of messages per second.  
âœ… **Low Latency (Real-Time Processing)** â€“ In optimal conditions, Kafka transfers data between systems in **less than 10ms**!  

ğŸ“Œ **Real-time** = Super low latency = Instant data flow âš¡  

---

# ğŸ¢ Who Uses Apache Kafka?  

Kafka plays a key role in real-time systems at top companies:  
ğŸ”¹ **Airbnb** â€“ Real-time analytics for customer interactions  
ğŸ”¹ **LinkedIn** â€“ Spam detection & recommendations  
ğŸ”¹ **Uber** â€“ Real-time demand forecasting & surge pricing  
ğŸ”¹ **Walmart** â€“ Logistics tracking  
ğŸ”¹ **Netflix** â€“ Instant content recommendations  

![image](https://github.com/user-attachments/assets/511343c7-2427-4916-9614-ca65088e7e12)

---

# ğŸ” Apache Kafka: Core Use Cases  

Kafka solves several critical data challenges:  

âœ… **Messaging System** â€“ Acts as a high-speed event bus for passing data.  
âœ… **Activity Tracking** â€“ Gathers metrics from various sources, including IoT devices.  
âœ… **Application Logging** â€“ Collects logs for debugging, monitoring, and analysis.  
âœ… **Stream Processing** â€“ Enables real-time data manipulation via Kafka Streams API or **Spark**.  
âœ… **Decoupling Dependencies** â€“ Reduces complexity between interacting systems.  
âœ… **Big Data Integration** â€“ Works seamlessly with **Spark, Flink, Storm, Hadoop**, and more!  

---

# ğŸŒŸ Kafka in Action: Real-World Examples  

Kafka is a **backbone technology** for major businesses:  

ğŸ”¹ **Netflix** â€“ Uses Kafka to **recommend** shows **instantly** after a user finishes watching.  
ğŸ”¹ **Uber** â€“ Relies on Kafka to collect **user, taxi, and trip data** and compute **surge pricing** dynamically.  
ğŸ”¹ **LinkedIn** â€“ Uses Kafka for **spam prevention**, **user behavior tracking**, and **connection recommendations** â€“ all in **real-time**!  

---

# ğŸ”¥ Why Kafka is Game-Changing  

Companies use Kafka for **three key benefits**:  

âœ… **Real-Time Recommendations** â€“ Like Netflix suggesting your next show!  
âœ… **Real-Time Decisions** â€“ Uber adjusting prices based on demand.  
âœ… **Real-Time Insights** â€“ Businesses analyzing customer behavior instantly.  

ğŸ“Œ **But remember**, Kafka is just a **transport mechanism**!  
ğŸ“Œ Applications still **need logic** to process and use data effectively.  
ğŸ“Œ Kafka ensures **data moves FAST at scale!** ğŸš€  

---

# ğŸ—ï¸ Kafka Fundamentals  

### ğŸ¯ Topics, Partitions & Offsets  
Kafka organizes data into **topics**, which are like database **tables**:  
âœ… **Topics** â€“ A categorized stream of data (e.g., GPS locations, logs).  
âœ… **Partitions** â€“ Topics are **split** into partitions for **parallelism**.  
âœ… **Ordered Messages** â€“ Each partition maintains an **ordered sequence**.  
âœ… **Offsets** â€“ Each message gets a **unique ID** (**incremental counter**) for tracking.  

ğŸ“Œ Kafka stores **an infinite** number of messages over timeâ€”data **never stops** flowing! ğŸŒŠ  

![image](https://github.com/user-attachments/assets/097b49a7-273c-4cfc-b1b3-49a4da50c51a)

---

# ğŸšš Topic Example: `trucks_gps`  

### ğŸš› Real-Time Truck Tracking with Kafka  
ğŸ”¹ Imagine a **fleet of trucks**, each reporting **GPS** location data.  
ğŸ”¹ Kafka can handle a **trucks_gps** topic, storing positions for all trucks.  
ğŸ”¹ Each truck **sends a message** every **20 seconds** containing:  
   - **Truck ID**  
   - **Latitude & Longitude**  

ğŸ“Œ The topic can have **10 partitions** to distribute load efficiently.  

ğŸ”¹ Two applications use this data:  
âœ… **Location Dashboard** â€“ Displays truck positions visually.  
âœ… **Notification Service** â€“ Alerts users when trucks enter/exist regions.  

![image](https://github.com/user-attachments/assets/bda5c912-09bd-4db0-b7ae-38ccceac9d3a)

---

# ğŸ—ï¸ Kafka Fundamentals: Topics, Partitions & Offsets  

### ğŸ” Understanding Offsets in Kafka  

Kafka breaks data into **topics**, which are further divided into **partitions**.  
Each partition ensures **message ordering** and assigns an **offset** (a unique ID).  

âœ… **Offsets only apply within a single partition** â€“ meaning:  
   - Offset **3 in Partition 0** â‰  Offset **3 in Partition 1** â€“ **They hold different data!**  
âœ… **Ordering is only guaranteed within a partition**, not across multiple partitions.  
âœ… **Data retention is temporary** â€“ usually **one week** by default.  
âœ… **Immutability** â€“ Once data is **written**, it **cannot be changed**.  
âœ… **Random partition assignment** â€“ Unless a **key** is provided, Kafka assigns data randomly.  

ğŸ“Œ Kafka ensures fast and **reliable** data processing while keeping old data available for a limited time.  

![image](https://github.com/user-attachments/assets/931a0100-732f-4bee-b875-da013afd3f7f)

---

# ğŸ’¡ Brokers: The Backbone of Kafka  

### ğŸ” What Are Brokers?  
A **Kafka cluster** is made up of **brokers** â€“ powerful servers that store and manage data.  

âœ… **Brokers hold topics & partitions**, acting as storage nodes.  
âœ… **Each broker** has a unique **ID** (integer).  
âœ… **Each broker contains different partitions from multiple topics**.  
âœ… Once connected to **any broker** (called a **bootstrap broker**), you are connected to the **entire cluster**.  
âœ… A beginner-friendly setup starts with **3 brokers**, but massive clusters **can have 100+ brokers**!  

ğŸ“Œ **Think of brokers as the "warehouse managers" of Kafka** â€“ they ensure data storage and accessibility at scale!  

![image](https://github.com/user-attachments/assets/81b502f9-4e98-4618-8a5b-cc4ce6112abf)

---

# ğŸ”„ Brokers & Topics: Data Distribution  

Kafka distributes **topics and partitions** across multiple brokers for **scalability**.  

Example setup:  
âœ… **Topic-A** with **three partitions**, spread across **three brokers**.  
âœ… **Topic-B** with **two partitions**, but **Broker 103 doesnâ€™t store any Topic-B data**.  

ğŸ“Œ Data gets **spread efficiently** among brokers, preventing overload on a single machine.  

![image](https://github.com/user-attachments/assets/4abfb86a-192f-4a09-8764-084f8665b4b9)

---

# ğŸ› ï¸ Fault Tolerance: Topic Replication Factor  

Kafka ensures **data resilience** through replication.  

âœ… **Replication Factor** > 1 ensures **fault tolerance**.  
âœ… Standard replication is **2 or 3** copies per partition.  
âœ… If **one broker fails**, other brokers **still serve the data**.  

### ğŸ”¥ Real-World Example  
ğŸ¯ **Cluster with 3 Brokers**  
ğŸ¯ **Topic-A with 2 partitions & replication factor = 2**  

ğŸš¨ **Broker 102 fails**â€¦  
âœ… **Broker 101 & 103 still have the data, ensuring zero data loss!**  

ğŸ“Œ **Replication is critical for Kafka clustersâ€”it prevents failures from disrupting data flow!**  

![image](https://github.com/user-attachments/assets/c368d58e-8abe-45e1-beae-4c04ec9efea9)  

ğŸ”¥ Even if **one server goes down**, **Kafka ensures business continuity**.  

![image](https://github.com/user-attachments/assets/13408999-7909-4fc0-a074-41516c287e21)

---

# âš¡ Concept of Leader for a Partition  

Kafka **organizes data into partitions**, but **who manages** them?  

### ğŸ”¹ Partition Leadership  
âœ… **Only ONE broker** acts as the **leader** for each partition at a time.  
âœ… **The leader broker** is responsible for **receiving & serving data** for that partition.  
âœ… **Other brokers act as followers** and **synchronize the data** from the leader.  
âœ… Each partition has **one leader** and multiple **ISRs (In-Sync Replicas)**.  

ğŸ“Œ If the leader broker **fails**, Kafka **elects** a new leader automatically to **ensure data availability**.  

![image](https://github.com/user-attachments/assets/3bf28d1d-3290-451a-9bfd-2ec101fbb3f0)

---

# ğŸ“ Producers: Writing Data to Kafka  

### ğŸ” What Are Producers?  
Producers are applications that **write data** to Kafka topics.  
âœ… Producers **automatically decide** which **broker** and **partition** to send data to.  
âœ… If a **broker fails**, producers **auto-recover** to prevent disruption.  

ğŸ“Œ Think of **producers** as **data suppliers**â€”sending messages efficiently into Kafka!  

![image](https://github.com/user-attachments/assets/7a072ff1-3d5d-49c5-9d6b-d3aed228c9b3)

---

# ğŸ”„ Producer Acknowledgements (acks)  

Kafka provides **three levels** of **data write acknowledgements**:  

âœ… **acks=0** â€“ Producer **does not wait** for confirmation (fastest, but risk of **data loss**).  
âœ… **acks=1** â€“ Producer waits for **leader acknowledgement** (**low risk** of data loss).  
âœ… **acks=all** â€“ Leader **plus all in-sync replicas** acknowledge the write (**no data loss**).  

ğŸ“Œ **Stronger acknowledgements = Higher data safety** ğŸ”¥  

![image](https://github.com/user-attachments/assets/acfecfbb-0b18-4465-9be1-c8f8417a8caa)

---

# ğŸ”‘ Producers & Message Keys  

### Why Use Keys?  
âœ… **Producers can attach a key** (e.g., **truck ID, user ID**) to each message.  
âœ… If **key=null**, messages are **sent randomly** in a **round-robin** fashion across partitions.  
âœ… If a **key is provided**, all messages with the same key **go to the same partition**.  
âœ… This is **essential for maintaining order** in case-specific fields need sequential tracking.  

ğŸ“Œ Example:  
ğŸšš **Tracking Truck GPS Data** â€“ All messages from a **specific truck ID** must go into the **same partition** for accurate ordering.  

![image](https://github.com/user-attachments/assets/61b03f6c-e897-489b-a015-a98be1df78c2)

---

# ğŸ“¥ Consumers: Reading Data from Kafka  

### ğŸ”¹ What Are Consumers?  
Consumers are applications that **read and process data** from Kafka topics.  
âœ… **Each consumer reads from a topic** identified by its **name**.  
âœ… Consumers **automatically know** which broker to fetch data from.  
âœ… If a **broker fails**, consumers **adjust** to read data from another available broker.  
âœ… **Data is read in order within each partition** â€“ ensuring **structured sequencing**.  

ğŸ“Œ **Consumers act as "data receivers," ensuring messages are properly retrieved and processed!**  

![image](https://github.com/user-attachments/assets/8964535b-df13-40c1-a275-ed2581b30841)

---

# ğŸ”„ Consumer Groups: Parallel Data Processing  

### ğŸ” What Are Consumer Groups?  
Consumers **read data efficiently** using **consumer groups**.  

âœ… A **consumer group** allows multiple consumers to **share the workload**.  
âœ… Each **consumer in the group** reads from **exclusive partitions**â€”ensuring **parallel processing**.  
âœ… If there are **more consumers than partitions**, **some consumers remain inactive**.  

ğŸ“Œ **Consumers work together to distribute the data load!**  

![image](https://github.com/user-attachments/assets/add1a59c-3ae4-41db-832d-7ab501d423b1)

---

# â“ Too Many Consumers?  

### ğŸ” What Happens If We Have More Consumers Than Partitions?  
âœ… Kafka **assigns one partition per active consumer**.  
âœ… If the **number of consumers exceeds partitions**, extra consumers **become inactive**.  
âœ… They stay **connected** but **do not receive data** until partitions **increase** or **existing consumers drop off**.  

ğŸ“Œ **Always ensure the number of consumers aligns with partitions for efficient scaling!**  

![image](https://github.com/user-attachments/assets/1b6006cc-14ec-4eae-9b62-e9a76e7d7615)

---

# ğŸ“Œ Consumer Offsets: Tracking Read Data  

### ğŸ” What Are Consumer Offsets?  
âœ… Kafka **stores offsets** tracking the last read message for **each consumer group**.  
âœ… These offsets live in a **special Kafka topic** called `__consumer_offsets`.  
âœ… When a **consumer processes data**, it commits **the offset** for future retrieval.  
âœ… If a **consumer crashes**, it can **resume reading** from the last committed offset.  

ğŸ“Œ **Offsets help ensure data continuity and prevent re-processing errors!**  

![image](https://github.com/user-attachments/assets/1248f177-0a2f-43a9-9c29-f95b49b7dbfb)

---

# ğŸ¯ Consumer Delivery Semantics: Ensuring Reliability  

Consumers decide **when to commit offsets**, impacting **data reliability**.  

### ğŸ”„ Three Delivery Semantics  

âœ… **At Most Once** â€“ Offsets are committed **as soon as the message is received**.  
   - ğŸš¨ If processing **fails**, the message is **lost forever**!  

âœ… **At Least Once** (Most Common) â€“ Offsets are committed **after processing**.  
   - âš  If processing **fails**, the message **is re-read**, causing **possible duplicates**.  
   - ğŸ›  **Solution**: Make processing **idempotent** (re-processing should not impact results).  

âœ… **Exactly Once** â€“ Ensures **no duplicate reads** (Best reliability).  
   - ğŸ”¹ Achieved **within Kafka** using **Kafka Streams API**.  
   - ğŸ”¹ For **external system workflows**, use **an idempotent consumer**.  

ğŸ“Œ **Choosing the right delivery semantics is critical to avoid data loss and duplication!**  

---

# ğŸ” Kafka Broker Discovery  

### ğŸ”¹ What Are Bootstrap Servers?  
âœ… Every Kafka **broker** is known as a **bootstrap server**.  
âœ… This means connecting to **any single broker** gives access to the **entire Kafka cluster**.  
âœ… Each broker **stores metadata**, knowing about **all other brokers, topics, and partitions**.  

ğŸ“Œ **Connect to one broker, and youâ€™re connected to them all!** ğŸš€  

![image](https://github.com/user-attachments/assets/b6056ef0-d49e-496b-b323-8f896ba1d984)

---

# ğŸ› ï¸ Zookeeper: The Backbone of Kafka  

### ğŸ” Why Does Kafka Need Zookeeper?  
Zookeeper acts as Kafkaâ€™s **management system**, ensuring coordination across brokers.  

âœ… **Tracks brokers** â€“ Maintains a list of all active Kafka brokers.  
âœ… **Handles leader election** â€“ Decides which broker will lead each partition.  
âœ… **Sends notifications** â€“ Alerts Kafka when **brokers fail**, **new brokers join**, or **topics are modified**.  
âœ… **Operates in odd-numbered clusters** â€“ Usually **3, 5, or 7 servers** for **high availability**.  
âœ… **Leader & Followers** â€“ One **Zookeeper node** acts as the **leader** (handles writes), others as **followers** (handle reads).  
âœ… **No Offset Storage** â€“ Zookeeper **stopped storing consumer offsets** in Kafka **versions > 0.10**.  

ğŸ“Œ Without Zookeeper, Kafka **cannot function properly**!  

![image](https://github.com/user-attachments/assets/b2fe444f-33db-4767-a76f-8863c1ec89a4)

---

# ğŸ”„ Kafka Guarantees  

Kafka ensures **strong data reliability and consistency** with key guarantees:  

âœ… **Ordered Message Processing** â€“  
   - Messages are **appended to a topic-partition** in **the order theyâ€™re sent**.  
   - Consumers read messages **in the same order they were stored**.  

âœ… **High Fault Tolerance** â€“  
   - With a **replication factor of N**, producers/consumers can tolerate **up to (N-1) broker failures**.  
   - A **replication factor of 3** is ideal:  
     - Allows **one broker to be taken down** for maintenance.  
     - Allows **another broker to fail unexpectedly** without losing data.  

âœ… **Consistent Message Routing** â€“  
   - As long as the **number of partitions remains constant**, messages with the same **key** always go to the **same partition**.  

ğŸ“Œ Kafka ensures **resilient and scalable** message handling while maintaining strict data ordering!  

---

# ğŸ¯ Theory Roundup  

Kafka guarantees **distributed, scalable, and fault-tolerant** messagingâ€”enabling businesses to handle **real-time data** efficiently.  

![image](https://github.com/user-attachments/assets/4a8fb287-23f8-49ea-a7d9-663d85939b55)

---

### ğŸš€ **Implementation of Apache Kafka**  

---

### ğŸ”§ **Launching Kafka & Zookeeper**  

âœ” **Start Zookeeper** ğŸ—  
```bash
bash run-kafka_zookeeper_server.sh -s start
```
âœ” **Start Kafka Server** âš™  
```bash
bash run-kafka_server.sh -s start
```
ğŸ“Œ **Checking Running Services**  
```bash
jps
```
ğŸ’¡ **Expected Output:**  
You should see `Kafka` and `QuorumPeerMain` running in the process list.  

---

### ğŸ— **Working with Kafka Topics**  

âœ” **View Documentation**  
```bash
kafka-topics.sh
```

âœ” **Create a New Topic (`first_topic`)**  
```bash
kafka-topics.sh --zookeeper 127.0.0.1:2181 --topic first_topic --create
```

ğŸ“Œ **Issue:** If an error occurs, **Kafka may not show the error explicitly** but instead display the documentation.  

âœ” **Specify Partitions (Fails without Replication Factor)**  
```bash
kafka-topics.sh --zookeeper 127.0.0.1:2181 --topic first_topic --create --partitions 3
```
âŒ **Error:** Partitions must be accompanied by a **replication factor**.  

âœ” **Set Partition & Replication Factor (Fails if brokers < Replication Factor)**  
```bash
kafka-topics.sh --zookeeper 127.0.0.1:2181 --topic first_topic --create --partitions 3 --replication-factor 2
```
âš  **Warning:**  
_"Error while executing topic command: Replication factor (2) larger than available brokers (1)."_  
ğŸ’¡ **Solution:** Ensure enough brokers are running OR lower the replication factor.

âœ” **Correct Topic Creation Command**  
```bash
kafka-topics.sh --zookeeper 127.0.0.1:2181 --topic first_topic --create --partitions 3 --replication-factor 1
```
ğŸ¯ **Topic `first_topic` Created Successfully!**  

âœ” **List Available Topics**  
```bash
kafka-topics.sh --zookeeper 127.0.0.1:2181 --list
```
âœ… **Expected Output:**  
```bash
first_topic
```

---

### âœ¨ **Key Takeaways**  
âœ” **Kafka needs Zookeeper to manage brokers** ğŸ¯  
âœ” **Partitions require replication factor** ğŸš€  
âœ” **Insufficient brokers cause replication errors** âš   
âœ” **Listing topics verifies successful creation** ğŸ“œ  

---

### ğŸ— **Working with Kafka Topics & Console Producer**  

---

### ğŸ” **Describing Kafka Topics**  

ğŸ“Œ **View details of a topic (`first_topic`)**  
```bash
kafka-topics.sh --zookeeper 127.0.0.1:2181 --topic first_topic --describe
```
âœ… **Expected Output:**  
| Topic  | Partition Count | Replication Factor | Leader | Replicas | ISR |
|--------|----------------|--------------------|--------|----------|-----|
| first_topic | 3 | 1 | 0 | 0 | 0 |

ğŸ’¡ **Key Observations:**  
âœ” **Partition Count:** `3` means the topic has three partitions.  
âœ” **Replication Factor:** `1` indicates no fault tolerance (single copy per partition).  
âœ” **ISR (In-Sync Replicas):** `0` might indicate an issue if brokers arenâ€™t correctly configured.  

---

### âŒ **Deleting a Topic**  
```bash
kafka-topics.sh --zookeeper 127.0.0.1:2181 --topic second_topic --delete
```
âœ” **Removes the topic named `second_topic`** ğŸ“œ  

âš  **Important:** Kafka **topic deletion depends on the broker configuration** (`delete.topic.enable=true` must be set in `server.properties`).  

---

### ğŸ“© **Using Kafka Console Producer**  

âœ” **Start a Producer & Send Messages**  
```bash
kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic first_topic
```
ğŸ’¬ **Example Messages:**  
```
>Hello Priyanka
>It's a good day..!!
>How are you?
>I am good here.
>^C
```
âœ… **Messages are sent asynchronously** to `first_topic`.  

âœ” **Using Acknowledgment Mode (`acks=all`)**  
```bash
kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic first_topic --producer-property acks=all
```
ğŸ’¡ **What happens?**  
- Ensures messages are **fully committed** across all in-sync replicas before acknowledging.  
- Improves **reliability** at the cost of **higher latency**.  

---

### ğŸ›‘ **Using Kafka Console Consumer**  

âœ” **Start Consumer & Read Messages**  
```bash
kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic first_topic
```
âŒ **Issue:** Nothing happens!  

ğŸ“Œ **Possible Reasons:**  
1ï¸âƒ£ **No messages available** in the topic.  
2ï¸âƒ£ **Consumer offset starts from latest messages (empty buffer)**  
3ï¸âƒ£ **Kafka server may not be running correctly**  

âœ” **Fix: Read Messages From Beginning**  
```bash
kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic first_topic --from-beginning
```

---

### ğŸ¯ **Key Takeaways**  
âœ” **Kafka topics can be described & deleted using `kafka-topics.sh`** ğŸ”  
âœ” **Console Producer sends messages asynchronously** ğŸ“©  
âœ” **`acks=all` ensures message delivery reliability** âœ…  
âœ” **Consumer needs `--from-beginning` to retrieve historical messages** ğŸ“œ  

---

Messages in Kafka **do not guarantee strict sequential order** when consumed. Hereâ€™s why:  

ğŸ”¹ **Kafka Uses Multiple Partitions** ğŸ“¦  
- Messages are **distributed** across partitions based on a partitioning key (or randomly if not specified).  
- When consuming, messages may come **from multiple partitions in parallel**, leading to seemingly **random order**.  

ğŸ”¹ **Consumer Fetching Behavior** ğŸ¯  
- The consumer **fetches from multiple partitions simultaneously**, so message retrieval **depends on partition offsets** rather than strict timestamp order.  
- This is why messages may appear out of sequence when displayed.  

ğŸ”¹ **Broker & Replication Factors** âš™  
- Messages are **stored & replicated** across brokers, and the consumer may fetch messages from different replicas.  
- This replication process does not enforce an exact sequence globally.  

âœ… **How to Read Messages in Strict Order?**  
- **Use a Single Partition** (force all messages into one partition).  
  ```bash
  kafka-topics.sh --zookeeper 127.0.0.1:2181 --topic first_topic --create --partitions 1 --replication-factor 1
  ```
- **Assign a Consumer to a Specific Partition**  
  ```bash
  kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic first_topic --partition 0 --from-beginning
  ```
---

### ğŸš€ **Kafka Consumer Group Implementation**  

---

### ğŸ— **Launching a Consumer Group**  

ğŸ“Œ **Overview:**  
âœ” A **topic** (`first_topic`) with **3 partitions**  
âœ” **Producer sends messages** to the topic  
âœ” Multiple **consumers** join the same **consumer group** (`my-first-application`)  
âœ” Kafka distributes messages **in round-robin fashion**  

---

### ğŸ¯ **Step 1: Start Consumer Group (`Consumer 1`)**  
```bash
kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic first_topic --group my-first-application
```
âœ… **Joins Consumer Group: `my-first-application`**  

âœ” Consumer listens for new messages.  
âœ” Producer sends **M1**, **M2**, **M3** messages.  
âœ” Messages are **distributed across partitions**.

---

### ğŸ›  **Step 2: Start Producer & Send Messages**  
```bash
kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic first_topic
```
ğŸ’¡ **Producer creates messages:**  
```
>M1
>M2
>M3
>^C
```
âœ” These **3 messages get distributed across 3 partitions** ğŸ¯  

---

### ğŸ”„ **Step 3: Scaling Consumer Group (`Consumer 2`)**  
```bash
kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic first_topic --group my-first-application
```
âœ” **Second consumer joins the group**  
âœ” **Workload is shared** between multiple consumers  
âœ” More messages sent by producer **are consumed by both consumers**  

---

### ğŸš€ **Step 4: Adding More Consumers**  
ğŸ“Œ Launch `Consumer 3`  
```bash
kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic first_topic --group my-first-application
```
ğŸ“Œ Launch `Consumer 4`  
```bash
kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic first_topic --group my-first-application
```
âœ” Each **consumer picks messages in a distributed fashion**  

ğŸ“Œ **Example Message Flow:**  
- **M8 â†’ Consumer 3**  
- **M9 â†’ Consumer 2**  
- **M10 â†’ Consumer 1**  
- **M11 â†’ Consumer 4**  
- **M12 â†’ Consumer 3**  

---

### âš  **Consumer Failure Handling**  
ğŸ“Œ **Crashing Consumer 4**  
âœ” **M17 â†’ Consumer 3**  
âœ” **M18 â†’ Consumer 2**  
âœ” **M19 â†’ Consumer 1**  

ğŸ’¡ **Kafka automatically redistributes partition ownership** when a consumer crashes, ensuring no message loss.  

---

### ğŸ¯ **Key Takeaways**  
âœ” **Consumer groups allow load balancing** across multiple consumers âœ…  
âœ” **Messages are distributed across partitions** in round-robin fashion ğŸ”„  
âœ” **If a consumer crashes, Kafka redistributes its partitions** ğŸš€  
âœ” **Offsets are committed automatically to track consumption state** ğŸ¯  

---
